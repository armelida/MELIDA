{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MELIDA: Model Evaluation for Life-sciences Intelligence and Decision Assistance\n",
    "\n",
    "This notebook runs evaluations on AI models using Spanish MIR exam questions.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, clone the repository and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/armelida/MELIDA.git\n",
    "# Change to the repository directory\n",
    "%cd MELIDA\n",
    "# Install required packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Keys\n",
    "\n",
    "Create a configuration file with your API keys. This file will not be committed to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create config directory if it doesn't exist\n",
    "os.makedirs('config', exist_ok=True)\n",
    "\n",
    "# Create api_config.json with your API keys\n",
    "api_config = {\n",
    "    \"openai\": {\n",
    "        \"api_key\": \"YOUR
git add .
git commit -m "Initial project setup with core files and structure"
git push origin dev
cp config/api_config_template.json config/api_config.json
# Create a Python file for testing
cat > test_evaluator.py << EOF
from src.evaluator import ModelEvaluator

# Initialize evaluator
evaluator = ModelEvaluator()

# Test loading questions and answers
questions = evaluator.load_questions('data/questions/MIR-2024-v01-t01.json')
answers = evaluator.load_answer_key('data/answers/MIR-2024-v01-t01-answers.json')

print(f"Loaded {len(questions)} questions and {len(answers)} answers")
print("Sample question:", questions[0]['question_text'])
print("Answer to first question:", answers[questions[0]['id']])
