{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOIAwEAxKoXvI7DZIiqc0Ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelida/MELIDA/blob/main/notebooks/MELIDA_Evaluator_V2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Model LLM Evaluation Notebook\n",
        "\n",
        "This notebook evaluates **multiple Large Language Models (LLMs)** on a set of standardized test questions. We will start by comparing a few models (2â€“3) and set up the code to **scale up to 10+ models** easily using an external registry file for model configuration. The evaluation will use a consistent prompt strategy and track performance metrics like accuracy, tokens used, and response time for each model. Weâ€™ll also log detailed results per question and export the outcomes to CSV files for visualization (e.g., in Tableau).\n",
        "\n",
        "**Key features of this notebook:**\n",
        "- Uses an **external JSON/YAML file** (â€œmodel registryâ€) to define which models to evaluate and how to access them, so you can easily add/remove models without changing code.\n",
        "- Evaluates each model on a set of **standardized test questions** with a chosen prompt format (e.g., multiple-choice questions) â€“ currently using a zero-shot prompt asking for the best answer.\n",
        "- Records **metrics per model**: accuracy (percentage of questions answered correctly), total score (number of correct answers), number of tokens used (if available), and average response time.\n",
        "- Stores **detailed logs per question** and model, including question ID, model name, full input prompt, modelâ€™s output, whether it was correct, and latency.\n",
        "- Exports results to **CSV files** (summary and detailed) for external analysis. Weâ€™ll include a guide on how to use these in Tableau to filter by model/prompt/question and create visualizations of accuracy and identify the hardest questions.\n",
        "- Modular code structure with clear comments and section headings, so you can identify and modify specific parts (e.g., to change the prompt strategy or add new evaluation features like chain-of-thought or hallucination detection in the future).\n",
        "\n"
      ],
      "metadata": {
        "id": "RFDg3bqrp5Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Cell:\n",
        "# Check Runtime & GPU Availability\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def check_runtime():\n",
        "    \"\"\"Check whether a GPU or TPU is available.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"âœ… GPU is enabled! Using: {gpu_name}\")\n",
        "    elif \"COLAB_TPU_ADDR\" in os.environ:\n",
        "        print(\"âœ… TPU is enabled!\")\n",
        "    else:\n",
        "        print(\"âš ï¸ WARNING: No GPU or TPU detected. Running on CPU.\")\n",
        "        print(\"ðŸ‘‰ Go to Runtime > Change runtime type > Select GPU/TPU\")\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check GPU details using nvidia-smi if available.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"âš ï¸ `nvidia-smi` not found. No GPU detected.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"âš ï¸ No GPU found.\")\n",
        "\n",
        "# Run the checks\n",
        "check_runtime()\n",
        "check_gpu()\n",
        "\n",
        "#  Clone repository and change working directory\n",
        "!rm -rf MELIDA  # Remove any existing copy (optional)\n",
        "!git clone https://github.com/armelida/MELIDA.git\n",
        "%cd MELIDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyrEgEQ-xVTk",
        "outputId": "bb2394be-cc20-4c8c-ebc2-990c250c723c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ WARNING: No GPU or TPU detected. Running on CPU.\n",
            "ðŸ‘‰ Go to Runtime > Change runtime type > Select GPU/TPU\n",
            "âš ï¸ No GPU found.\n",
            "Cloning into 'MELIDA'...\n",
            "remote: Enumerating objects: 509, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 509 (delta 11), reused 16 (delta 4), pack-reused 480 (from 1)\u001b[K\n",
            "Receiving objects: 100% (509/509), 1.37 MiB | 17.52 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n",
            "/content/MELIDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0A: Load API Keys & Save API Configuration\n",
        "\n",
        "# Install necessary libraries if not already present\n",
        "!pip install -q python-dotenv google-generativeai # Added google-generativeai\n",
        "# Keep openai pinned if needed, but google-generativeai is separate\n",
        "!pip install openai==0.28\n",
        "\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Initialize API keys dictionary - Added 'google'\n",
        "api_keys = {\"openai\": None, \"anthropic\": None, \"together\": None, \"google\": None}\n",
        "\n",
        "# Try to load from Colab secrets using userdata - Added 'google'\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_keys[\"openai\"] = userdata.get('OPENAI_API_KEY')\n",
        "    api_keys[\"anthropic\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "    api_keys[\"together\"] = userdata.get('TOGETHER_API_KEY')\n",
        "    # --- Add Google API Key ---\n",
        "    api_keys[\"google\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    # ------------------------\n",
        "    # Update check condition\n",
        "    if all(v is not None for k, v in api_keys.items() if k != 'google'): # Check others first\n",
        "         print(\"âœ“ API keys (OpenAI, Anthropic, Together) loaded from Colab secrets\")\n",
        "    if api_keys[\"google\"]:\n",
        "         print(\"âœ“ Google API key loaded from Colab secrets\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Note: Couldn't load some keys from Colab secrets - {e}\")\n",
        "\n",
        "# Fallback: load from environment variables if not loaded yet - Added 'google'\n",
        "if not all(api_keys.values()): # Check if *any* are missing\n",
        "    api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "    # --- Add Google API Key ---\n",
        "    api_keys[\"google\"] = api_keys[\"google\"] or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    # ------------------------\n",
        "    # Update log message if needed\n",
        "    loaded_from_env = [k for k, v in api_keys.items() if v and not userdata.get(k.upper()+'_API_KEY')] # Rough check\n",
        "    if loaded_from_env:\n",
        "        print(f\"âœ“ API keys loaded from environment variables for: {', '.join(loaded_from_env)}\")\n",
        "\n",
        "\n",
        "# Fallback: load from a .env file if still missing - Added 'google'\n",
        "if not all(api_keys.values()):\n",
        "    try:\n",
        "        load_dotenv() # This will load variables from a .env file\n",
        "        api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "        api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "        # --- Add Google API Key ---\n",
        "        api_keys[\"google\"] = api_keys[\"google\"] or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "        # ------------------------\n",
        "        # Update log message if needed\n",
        "        loaded_from_dotenv = [k for k, v in api_keys.items() if v and not userdata.get(k.upper()+'_API_KEY') and k not in loaded_from_env] # Rough check\n",
        "        if loaded_from_dotenv:\n",
        "             print(f\"âœ“ API keys loaded from .env file for: {', '.join(loaded_from_dotenv)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Couldn't load from .env file - {e}\")\n",
        "\n",
        "# Propagate keys to os.environ so subsequent cells can access them - Added 'google'\n",
        "if api_keys[\"openai\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_keys[\"openai\"]\n",
        "if api_keys[\"anthropic\"]:\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_keys[\"anthropic\"]\n",
        "if api_keys[\"together\"]:\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = api_keys[\"together\"]\n",
        "# --- Add Google API Key ---\n",
        "if api_keys[\"google\"]:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_keys[\"google\"]\n",
        "# ------------------------\n",
        "\n",
        "# Save API configuration to a JSON file for future reference\n",
        "os.makedirs('config', exist_ok=True)\n",
        "api_config = {\n",
        "    \"openai\": {\"api_key\": api_keys[\"openai\"] or \"YOUR_OPENAI_API_KEY_HERE\"},\n",
        "    \"anthropic\": {\"api_key\": api_keys[\"anthropic\"] or \"YOUR_ANTHROPIC_API_KEY_HERE\"},\n",
        "    \"together\": {\"api_key\": api_keys[\"together\"] or \"YOUR_TOGETHER_API_KEY_HERE\"},\n",
        "    \"google\": {\"api_key\": api_keys[\"google\"] or \"YOUR_GOOGLE_API_KEY_HERE\"}\n",
        "}\n",
        "with open('config/api_config.json', 'w') as f:\n",
        "    json.dump(api_config, f, indent=2)\n",
        "\n",
        "# Report missing keys, if any\n",
        "missing = []\n",
        "if not api_keys[\"openai\"]: missing.append(\"OpenAI\")\n",
        "if not api_keys[\"anthropic\"]: missing.append(\"Anthropic\")\n",
        "if not api_keys[\"together\"]: missing.append(\"Together\")\n",
        "if not api_keys[\"google\"]: missing.append(\"Google\")\n",
        "if missing:\n",
        "    print(f\"âš ï¸ Missing API keys: {', '.join(missing)}\")\n",
        "    print(\"ðŸ‘‰ Please set the necessary API keys using Colab secrets (recommended), environment variables, or a .env file.\")\n",
        "else:\n",
        "    print(\"âœ“ All required API configurations saved/loaded.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5_dSkv97Efr",
        "outputId": "4ff64987-c467-42e3-d1bb-48195341f2b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "âœ“ API keys (OpenAI, Anthropic, Together) loaded from Colab secrets\n",
            "âœ“ Google API key loaded from Colab secrets\n",
            "âœ“ All required API configurations saved/loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation (Cell 1)\n",
        "\n",
        "First, we install and import necessary libraries. This includes:\n",
        "- **Hugging Face Transformers** for local model inference (if using HuggingFace-hosted models).\n",
        "- **OpenAI/Anthropic API SDKs** (if using direct APIs like OpenAIâ€™s GPT or Anthropicâ€™s Claude).\n",
        "- **Together AI** client (if using the Together API for hosted models).\n",
        "- **PyYAML** (for reading YAML config) and **pandas** (for data manipulation and CSV export).\n",
        "\n",
        "We will also ensure any required API keys are set (for OpenAI, Anthropic, Together, etc.) via environment variables for security. Replace or set these environment variables before running the evaluation.\n"
      ],
      "metadata": {
        "id": "xiu37WIKrI_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fMnoEmHYo81v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715d87a0-67e1-4322-a367-8f1a4a207edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Libraries loaded.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup environment and install required packages\n",
        "\n",
        "# Install required packages - Added google-generativeai\n",
        "!pip install -q pandas PyYAML openai anthropic together transformers google-generativeai\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# Import model API clients\n",
        "import openai\n",
        "# Ensure your OpenAI API key is available (loaded in Cell 0A)\n",
        "\n",
        "import anthropic\n",
        "# Anthropic client might be initialized later in call_model or globally here if preferred\n",
        "\n",
        "import together\n",
        "# Together client might be initialized later in call_model or globally here if preferred\n",
        "\n",
        "# --- Add Google Generative AI Import ---\n",
        "import google.generativeai as genai\n",
        "# --------------------------------------\n",
        "\n",
        "# (Optional) If using Hugging Face transformers for local models:\n",
        "try:\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "except ImportError:\n",
        "    !pip install -q transformers # Ensure transformers is installed if needed\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Setup complete. Libraries loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configure Model Registry (Cell 2)\n",
        "We use an external model registry file (JSON or YAML) to list all models to evaluate and their access details. This file allows easy scaling to more models â€“ just add or remove entries without changing the notebook code. Each model entry can include:\n",
        "name: A human-readable name for the model (used in results and plots).\n",
        "provider: The method to access the model (huggingface, together, openai, anthropic, etc.).\n",
        "model_id: Identifier for the model:\n",
        "For huggingface, this is the modelâ€™s name on HuggingFace Hub (e.g., \"google/flan-t5-small\").\n",
        "For together, it might be a model ID known to the Together API (e.g., \"meta-llama/Llama-2-7b-chat-hf\").\n",
        "For openai, it could be the API model name (e.g., \"gpt-4\" or \"gpt-3.5-turbo\").\n",
        "For other providers, use the appropriate identifier.\n",
        "api_key_env (if needed): The environment variable name for the API key (e.g., \"OPENAI_API_KEY\"). This can be omitted for HuggingFace (if using local models or if no auth needed).\n",
        "Additional settings like max_tokens, temperature, etc., which define generation parameters for that model.\n",
        "Example model registry (YAML format):\n",
        "\n",
        "models:\n",
        "  - name: FlanT5 Small\n",
        "    provider: huggingface\n",
        "    model_id: google/flan-t5-small\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: GPT-4 (Jan 2025)\n",
        "    provider: openai\n",
        "    model_id: gpt-4\n",
        "    api_key_env: OPENAI_API_KEY\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: Llama2 7B Chat\n",
        "    provider: together\n",
        "    model_id: meta-llama/Llama-2-7b-chat-hf\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "\n",
        "In the code below, we load the model list from the registry file. Update model_config_path to point to your JSON/YAML file. The code will automatically detect JSON vs YAML based on file extension and parse accordingly. After loading, it prints out the model configurations to confirm."
      ],
      "metadata": {
        "id": "1_unmmk1qxdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load model registry from external JSON/YAML file\n",
        "\n",
        "# Set the path to your model registry file in the notebooks folder\n",
        "model_config_path = \"notebooks/models.yaml\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(model_config_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Model config file not found at {model_config_path}. \"\n",
        "        \"Please create it as per the example and update the path.\"\n",
        "    )\n",
        "\n",
        "# Parse the config file (supports YAML and JSON)\n",
        "if model_config_path.endswith((\".yaml\", \".yml\")):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = yaml.safe_load(f)\n",
        "elif model_config_path.endswith(\".json\"):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported config file format. Use .json or .yaml\")\n",
        "\n",
        "# The config should either be a dict with a top-level 'models' key or a list itself.\n",
        "if isinstance(config_data, dict) and \"models\" in config_data:\n",
        "    models_config = config_data[\"models\"]\n",
        "elif isinstance(config_data, list):\n",
        "    models_config = config_data\n",
        "else:\n",
        "    raise ValueError(\"Config file format error: expected a list of models or a 'models' key.\")\n",
        "\n",
        "print(f\"Loaded {len(models_config)} models from registry:\")\n",
        "for m in models_config:\n",
        "    print(f\" - {m.get('name', 'Unnamed')} ({m.get('provider', 'Unknown')}, id={m.get('model_id', 'N/A')})\")\n"
      ],
      "metadata": {
        "id": "z8Zg9195pKkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37c2c6e-f9d1-4186-ee7b-3b2d6a9cf050"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3 models from registry:\n",
            " - o3-mini-2025-01-31 (OpenAI, id=o3-mini-2025-01-31)\n",
            " - Claude (Anthropic, id=claude-3-7-sonnet-20250219)\n",
            " - Together (Together, id=deepseek-ai/DeepSeek-R1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load Standardized Test Questions (Cell 3)\n",
        "Next, we prepare the standardized test questions for evaluation. These can be hard-coded, loaded from a file, or generated. In this notebook, weâ€™ll define a list of questions in a structured format (each with an ID, question text, multiple-choice options, and the correct answer). You can replace these with any set of questions relevant to your use case. For demonstration, weâ€™ll use a few simple sample questions. In a real scenario, you might load dozens of questions from a JSON/CSV file or an existing dataset. Ensure each question has a known correct answer to compute accuracy."
      ],
      "metadata": {
        "id": "qa5t7ftJrBM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load extracted exam questions for evaluation\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Define the path to the exported questions file.\n",
        "# Adjust this path if your exported file name or location is different.\n",
        "questions_file = \"data/questions/MIR-2024-v01-t01.json\"\n",
        "\n",
        "if not os.path.exists(questions_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Questions file not found at {questions_file}. \"\n",
        "        \"Please run the extraction process to generate the questions file.\"\n",
        "    )\n",
        "\n",
        "with open(questions_file, 'r', encoding='utf-8') as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(questions)} questions for evaluation.\")\n",
        "\n",
        "# Optionally, preview the first three questions\n",
        "for q in questions[:3]:\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print(f\"ID: {q['id']}\")\n",
        "    print(f\"Question: {q['question_text']}\")\n",
        "    print(\"Options:\")\n",
        "    for key, value in q['options'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"---------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "1ljgjq5bpKqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922aa85d-5254-4568-c730-61b760a4df28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 185 questions for evaluation.\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q026\n",
            "Question: Entre los cambios metabÃ³licos que se observan en un paciente con resistencia a insulina existe:\n",
            "Options:\n",
            "  A: Incremento de la expresiÃ³n hepÃ¡tica de genes gluconeogÃ©nicos mediado por FOXO1 (forkhead box other) fosforilado.\n",
            "  B: Descenso en los niveles intracelulares de hexoquinasa 2 dependiente de insulina.\n",
            "  C: Aumento de la glucogenÃ³lisis muscular, contribuyendo al incremento de la glucemia.\n",
            "  D: Aumento en los niveles sÃ©ricos de aminoÃ¡cidos como leucina e isoleucina.\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q027\n",
            "Question: La deficiencia de acil-CoA-deshidrogenasa provoca una de las siguientes alteraciones bioquÃ­micas:\n",
            "Options:\n",
            "  A: DisminuciÃ³n de Ã¡cidos dicarboxÃ­licos.\n",
            "  B: Aumento de la gluconeogÃ©nesis.\n",
            "  C: DisminuciÃ³n de la ureagÃ©nesis.\n",
            "  D: Aumento de carnitina libre.\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q028\n",
            "Question: Respecto a la tÃ©cnica de exploraciÃ³n de la motilidad pupilar y sus reflejos indique la respuesta INCORRECTA:\n",
            "Options:\n",
            "  A: El reflejo de la visiÃ³n prÃ³xima se explora con una linterna que se enfoca sobre las pupilas de forma alternativa.\n",
            "  B: El signo pupilar de Marcus Gunn o defecto pupilar aferente relativo estÃ¡ presente en enfermedades del nervio Ã³ptico y enfermedades retinianas no extensas.\n",
            "  C: El reflejo fotomotor consensuado estÃ¡ afectado cuando el daÃ±o implica la vÃ­a eferente vehiculada por el nervio motor ocular comÃºn.\n",
            "  D: La anisocoria que aumenta en condiciones de oscuridad mostrando una pupila mÃ¡s miÃ³tica se debe a que hay un trastorno en el sistema simpÃ¡tico.\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We defined a list of dictionaries, where each dictionary represents a question. Each question has:\n",
        "id: a unique identifier,\n",
        "question: the text of the question,\n",
        "choices: a list of answer choices (as strings, each prefixed with a letter),\n",
        "answer: the correct choice (here represented by the letter of the correct option).\n",
        "Feel free to extend or replace this list. For example, you could load questions from a file or generate them. Just ensure each question has a known correct answer for scoring."
      ],
      "metadata": {
        "id": "ycbGse8qrO6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define Prompt Strategy (Cell 4)\n",
        "The prompt strategy determines how we present each question to the model. For standardized test questions, a common strategy is to provide the question and multiple-choice options and ask the model to pick the best answer. We might instruct the model to output just the option letter (to make it easier to check correctness). In future, you could experiment with different strategies (e.g., asking for an explanation, chain-of-thought prompting, etc.). For now, weâ€™ll use a straightforward prompt: the question, the options, and a final instruction like â€œAnswer with the letter of the correct option.â€ We implement this as a function format_prompt(question) that takes a question entry and returns the full prompt text (or structured prompt) to send to the model. This function can be easily modified if you want to change how prompts are constructed."
      ],
      "metadata": {
        "id": "oWi8tcY7rVM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define the prompt formatting strategy\n",
        "\n",
        "def format_prompt(q):\n",
        "    \"\"\"\n",
        "    Format a question dictionary into a prompt string for the MIR exam in Spanish.\n",
        "\n",
        "    The prompt instructs the model to answer with a single letter (A, B, C, D) if it knows the answer,\n",
        "    or with N if unsure.\n",
        "    \"\"\"\n",
        "    # Get the main question text\n",
        "    question_text = q.get(\"question_text\", \"\")\n",
        "\n",
        "    # Get each answer option from the options dictionary (default to empty string if missing)\n",
        "    options = q.get(\"options\", {})\n",
        "    option_a = options.get(\"A\", \"\")\n",
        "    option_b = options.get(\"B\", \"\")\n",
        "    option_c = options.get(\"C\", \"\")\n",
        "    option_d = options.get(\"D\", \"\")\n",
        "\n",
        "    # Build the prompt using the provided format\n",
        "    prompt = (\n",
        "        \"Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. \"\n",
        "        \"Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. \"\n",
        "        \"Si no estÃ¡s seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\n\"\n",
        "        f\"{question_text}\\n\\n\"\n",
        "        f\"A) {option_a}\\n\"\n",
        "        f\"B) {option_b}\\n\"\n",
        "        f\"C) {option_c}\\n\"\n",
        "        f\"D) {option_d}\\n\\n\"\n",
        "        \"Tu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# Test the prompt formatting on the first loaded question\n",
        "example_prompt = format_prompt(questions[0])\n",
        "print(\"Example formatted prompt:\\n\", example_prompt)\n"
      ],
      "metadata": {
        "id": "toS2gKl3pKtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838b47f9-0868-4bde-a0dc-d090952b4f3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example formatted prompt:\n",
            " Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡s seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\n",
            "\n",
            "Entre los cambios metabÃ³licos que se observan en un paciente con resistencia a insulina existe:\n",
            "\n",
            "A) Incremento de la expresiÃ³n hepÃ¡tica de genes gluconeogÃ©nicos mediado por FOXO1 (forkhead box other) fosforilado.\n",
            "B) Descenso en los niveles intracelulares de hexoquinasa 2 dependiente de insulina.\n",
            "C) Aumento de la glucogenÃ³lisis muscular, contribuyendo al incremento de la glucemia.\n",
            "D) Aumento en los niveles sÃ©ricos de aminoÃ¡cidos como leucina e isoleucina.\n",
            "\n",
            "Tu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: The format_prompt function takes a question from our list and builds a prompt. We put the question text, list all the choices (joined in one line for simplicity), and then give an explicit instruction. By asking for the letter only, we aim to have consistent outputs that are easy to check (the model hopefully will just respond with â€œBâ€, etc.). After defining the function, we preview an example prompt for the first question to verify the format. You can adjust this format as needed (for instance, if a model tends to do better with a different phrasing or if you want the model to explain its answer, etc.)."
      ],
      "metadata": {
        "id": "5-XAXQV_rgfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define function to call a model and get its response\n",
        "\n",
        "import time\n",
        "import os\n",
        "import openai\n",
        "import anthropic\n",
        "import together\n",
        "import google.generativeai as genai # Ensure import is here or in Cell 1\n",
        "from transformers import pipeline, AutoTokenizer # Keep existing imports\n",
        "\n",
        "# --- Keep your existing call_model function structure ---\n",
        "\n",
        "def call_model(model_cfg, prompt):\n",
        "    \"\"\"\n",
        "    Call a model with the given prompt and return its response and metadata.\n",
        "    Returns: output_text, tokens_used, latency\n",
        "    \"\"\"\n",
        "    provider = model_cfg.get(\"provider\", \"\").lower()\n",
        "    model_id = model_cfg.get(\"model_id\")\n",
        "    api_key_env = model_cfg.get(\"api_key_env\") # Get the env variable name\n",
        "    max_tokens = model_cfg.get(\"max_tokens\", 2000) # Use default from your setup\n",
        "    temperature = model_cfg.get(\"temperature\", 0.0) # Use default from your setup\n",
        "\n",
        "    tokens_used = None\n",
        "    output_text = \"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Existing HuggingFace Branch ---\n",
        "    if provider == \"huggingface\":\n",
        "        # (Your existing HuggingFace code...)\n",
        "        # Make sure to handle potential errors and set output_text, tokens_used\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "            # Determine pipeline type (text2text vs text-generation)\n",
        "            if \"google/flan\" in model_id.lower() or \"t5\" in model_id.lower():\n",
        "                pipe = pipeline(\"text2text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "                # Adjust generation parameters based on model_cfg\n",
        "                result = pipe(prompt, max_length=max_tokens, temperature=temperature if temperature > 0 else None, do_sample=temperature > 0)\n",
        "                output_text = result[0]['generated_text']\n",
        "            else:\n",
        "                pipe = pipeline(\"text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "                 # Adjust generation parameters based on model_cfg\n",
        "                result = pipe(prompt, max_new_tokens=max_tokens, temperature=temperature if temperature > 0 else None, do_sample=temperature > 0) # Use max_new_tokens\n",
        "                # Need to handle prompt inclusion in output for text-generation\n",
        "                output_text = result[0]['generated_text']\n",
        "                # Optional: Remove prompt from output if pipeline includes it\n",
        "                if output_text.startswith(prompt):\n",
        "                     output_text = output_text[len(prompt):]\n",
        "\n",
        "            # Calculate tokens (optional, can be slow for local models)\n",
        "            try:\n",
        "                input_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "                output_tokens = tokenizer(output_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "                tokens_used = int(len(input_tokens[0]) + len(output_tokens[0]))\n",
        "            except Exception:\n",
        "                tokens_used = None # Or handle error appropriately\n",
        "        except Exception as e:\n",
        "             # Log error, set output_text to indicate failure\n",
        "             print(f\"ERROR calling HuggingFace model {model_id}: {e}\")\n",
        "             output_text = f\"ERROR: {e}\"\n",
        "             tokens_used = None\n",
        "\n",
        "\n",
        "    # --- Existing OpenAI Branch ---\n",
        "    elif provider == \"openai\":\n",
        "        # (Your existing OpenAI code...)\n",
        "        # Ensure it checks the key using api_key_env\n",
        "        api_key = os.getenv(api_key_env or \"OPENAI_API_KEY\")\n",
        "        if api_key is None:\n",
        "             output_text = f\"ERROR: OpenAI API key ({api_key_env or 'OPENAI_API_KEY'}) not set.\"\n",
        "             # raise RuntimeError(f\"OpenAI API key not set...\") # Or raise error\n",
        "        else:\n",
        "            openai.api_key = api_key # Set key for this call (needed for older openai lib versions)\n",
        "            try:\n",
        "                # Your existing create logic (ChatCompletion)\n",
        "                # Make sure max_tokens and temperature are passed correctly\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                    # Handle other params like 'max_completion_tokens' if needed based on model_id\n",
        "                )\n",
        "                output_text = response['choices'][0]['message']['content'].strip()\n",
        "                if 'usage' in response:\n",
        "                    tokens_used = response['usage'].get('total_tokens')\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR calling OpenAI model {model_id}: {e}\")\n",
        "                output_text = f\"ERROR: {e}\"\n",
        "                tokens_used = None\n",
        "\n",
        "\n",
        "    # --- Existing Anthropic Branch ---\n",
        "    elif provider == \"anthropic\":\n",
        "        # (Your existing Anthropic code...)\n",
        "        # Ensure it checks the key using api_key_env\n",
        "        api_key = os.getenv(api_key_env or \"ANTHROPIC_API_KEY\")\n",
        "        if api_key is None:\n",
        "             output_text = f\"ERROR: Anthropic API key ({api_key_env or 'ANTHROPIC_API_KEY'}) not set.\"\n",
        "             # raise RuntimeError(...)\n",
        "        else:\n",
        "            try:\n",
        "                anthropic_client = anthropic.Client(api_key=api_key) # Initialize client here\n",
        "                response = anthropic_client.messages.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                # Extract text correctly from the Anthropic response object\n",
        "                # The structure might be response.content[0].text\n",
        "                if response.content and isinstance(response.content, list):\n",
        "                     output_text = response.content[0].text.strip()\n",
        "                else:\n",
        "                     # Fallback or check older response structures if needed\n",
        "                     output_text = str(response.completion).strip() # Adjust based on actual response object\n",
        "\n",
        "                # Anthropic API v3 response includes usage data\n",
        "                if hasattr(response, 'usage'):\n",
        "                    tokens_used = response.usage.input_tokens + response.usage.output_tokens\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"ERROR calling Anthropic model {model_id}: {e}\")\n",
        "                 output_text = f\"ERROR: {e}\"\n",
        "                 tokens_used = None\n",
        "\n",
        "\n",
        "    # --- Existing Together Branch ---\n",
        "    elif provider == \"together\":\n",
        "        # (Your existing Together code...)\n",
        "        # Ensure it checks the key using api_key_env\n",
        "        api_key = os.getenv(api_key_env or \"TOGETHER_API_KEY\")\n",
        "        if api_key is None:\n",
        "            output_text = f\"ERROR: Together API key ({api_key_env or 'TOGETHER_API_KEY'}) not set.\"\n",
        "            # raise RuntimeError(...)\n",
        "        else:\n",
        "            try:\n",
        "                together.api_key = api_key # Set the key for the together library\n",
        "                # Use together.Complete.create for non-chat models if needed\n",
        "                # Use together.Chat.create for chat models\n",
        "                # Assuming chat model based on your example:\n",
        "                response = together.Chat.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,\n",
        "                    # stream=False # Ensure streaming is off if not handled\n",
        "                )\n",
        "                # Extract text correctly\n",
        "                if response and 'choices' in response and len(response['choices']) > 0:\n",
        "                    output_text = response['choices'][0]['message']['content'].strip()\n",
        "                else:\n",
        "                    # Handle cases where response structure might differ or be empty\n",
        "                    output_text = \"ERROR: Unexpected response structure from Together AI\"\n",
        "                    print(f\"Unexpected Together AI response for {model_id}: {response}\")\n",
        "\n",
        "\n",
        "                # Together API might return usage info, check their documentation\n",
        "                # Example placeholder:\n",
        "                # if response and 'usage' in response:\n",
        "                # Â  Â  tokens_used = response['usage'].get('total_tokens')\n",
        "                tokens_used = None # Update if usage data is available\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"ERROR calling Together model {model_id}: {e}\")\n",
        "                 output_text = f\"ERROR: {e}\"\n",
        "                 tokens_used = None\n",
        "\n",
        "\n",
        "    # --- NEW Google Gemini Branch ---\n",
        "    elif provider == \"google\":\n",
        "        api_key = os.getenv(api_key_env or \"GOOGLE_API_KEY\")\n",
        "        if api_key is None:\n",
        "             output_text = f\"ERROR: Google API key ({api_key_env or 'GOOGLE_API_KEY'}) not set.\"\n",
        "             # raise RuntimeError(...) # Or raise error\n",
        "        else:\n",
        "            try:\n",
        "                # Configure the API key (safe to call multiple times)\n",
        "                genai.configure(api_key=api_key)\n",
        "\n",
        "                # Set up generation config\n",
        "                generation_config = genai.types.GenerationConfig(\n",
        "                    # candidate_count=1, # Default is 1\n",
        "                    # stop_sequences=None, # Optional stop sequences\n",
        "                    max_output_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                    # top_p=, top_k= # Add if needed\n",
        "                )\n",
        "\n",
        "                # Define safety settings (adjust as needed, e.g., BLOCK_NONE for less filtering during eval)\n",
        "                safety_settings = [\n",
        "                    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                ]\n",
        "                # Or BLOCK_NONE if you want minimal filtering for eval:\n",
        "                # safety_settings = [\n",
        "                #     {\"category\": f\"HARM_CATEGORY_{cat}\", \"threshold\": \"BLOCK_NONE\"}\n",
        "                #     for cat in [\"HARASSMENT\", \"HATE_SPEECH\", \"SEXUALLY_EXPLICIT\", \"DANGEROUS_CONTENT\"]\n",
        "                # ]\n",
        "\n",
        "\n",
        "                # Initialize the model\n",
        "                model = genai.GenerativeModel(\n",
        "                    model_name=model_id,\n",
        "                    generation_config=generation_config,\n",
        "                    safety_settings=safety_settings\n",
        "                    )\n",
        "\n",
        "                # Generate content\n",
        "                response = model.generate_content(prompt)\n",
        "\n",
        "                # Extract text and handle potential blocks\n",
        "                if response.parts:\n",
        "                     output_text = response.text # response.text joins parts automatically\n",
        "                elif response.prompt_feedback.block_reason:\n",
        "                     output_text = f\"ERROR: Blocked by safety filter - Reason: {response.prompt_feedback.block_reason}\"\n",
        "                     print(f\"WARNING: Call to {model_id} blocked. Reason: {response.prompt_feedback.block_reason}\")\n",
        "                else:\n",
        "                     # Handle other potential empty response cases\n",
        "                     output_text = \"ERROR: No content generated (unknown reason)\"\n",
        "                     print(f\"WARNING: Empty response from {model_id}, Response object: {response}\")\n",
        "\n",
        "\n",
        "                # Get token count from usage metadata\n",
        "                if hasattr(response, 'usage_metadata'):\n",
        "                     tokens_used = response.usage_metadata.prompt_token_count + response.usage_metadata.candidates_token_count\n",
        "                else:\n",
        "                     tokens_used = None # Fallback if metadata not present\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"ERROR calling Google model {model_id}: {e}\")\n",
        "                 output_text = f\"ERROR: {e}\"\n",
        "                 tokens_used = None\n",
        "\n",
        "    # --- Fallback for Unknown Provider ---\n",
        "    else:\n",
        "        output_text = f\"ERROR: Unknown provider '{provider}' for model {model_cfg.get('name', model_id)}\"\n",
        "        # raise ValueError(...) # Or raise error\n",
        "\n",
        "    latency = time.time() - start_time\n",
        "    # Ensure output_text is always a string\n",
        "    if not isinstance(output_text, str):\n",
        "         output_text = str(output_text)\n",
        "\n",
        "    return output_text.strip(), tokens_used, latency # Return stripped text"
      ],
      "metadata": {
        "id": "59_SJq6jrgDH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Interface and Evaluation Functions (Cells 5â€“6)\n",
        "In this section, we set up functions to handle model inference and evaluation:\n",
        "call_model(model_config, prompt_text) â€“ Invokes a single model (based on its provider and config) with the given prompt, and returns the modelâ€™s answer, along with metadata like token usage and latency.\n",
        "evaluate_model(model_config, questions) â€“ Uses call_model to get answers for each question from one model, checks correctness, and collects detailed results.\n",
        "We will also prepare a loop or another function to evaluate all models and aggregate the results for comparison.\n",
        "Structuring this logic into functions makes the notebook modular and easier to update. For example, if in the future we want to add a step for chain-of-thought (CoT) prompting or filter the model output for hallucinations, we could modify or wrap call_model accordingly. 5.1 call_model Implementation: This function will branch based on the provider:\n",
        "HuggingFace: use transformers pipeline or model generate. Weâ€™ll initialize a pipeline for text generation or use the modelâ€™s generate method. We also tokenize the input to count input tokens. The output tokens can be counted by the tokenizer as well.\n",
        "OpenAI: use openai.Completion or openai.ChatCompletion depending on model type. For chat models (e.g., GPT-4), we pass the prompt as a user message. We retrieve the output text and usage info (token counts).\n",
        "Anthropic: (Claude models) use anthropicâ€™s client. Typically you provide a prompt with a special format (like \"\\n\\nHuman: <question>\\n\\nAssistant:\"). We skip detailed implementation here but it can be added.\n",
        "Together: use Together API client. For example, together_client.complete or the chat completion as needed, based on their documentation. (Ensure TOGETHER_API_KEY is set.)\n",
        "Additional providers (e.g., Cohere, AI21) can be integrated similarly by adding new branches.\n",
        "We also measure the time taken for each call (latency). If token counts are not readily available from the API, we will set them to None (or you could estimate via a tokenizer). Letâ€™s implement call_model below:"
      ],
      "metadata": {
        "id": "626XG9mwrmbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In call_model:\n",
        "We take the modelâ€™s config and a prompt string.\n",
        "Based on provider, we handle the call differently.\n",
        "HuggingFace: We load the model and tokenizer (from local or HuggingFace Hub). We use pipeline for simplicity (it will handle the model loading and generation). We choose the pipeline task based on model type (a quick check for â€œt5â€ in the model name to decide between text2text-generation and text-generation). After generation, we count tokens by encoding the prompt and output with the tokenizer.\n",
        "OpenAI: We use the OpenAI API. If the model is chat-based (we guess by name containing â€œgpt-3.5â€ or â€œgpt-4â€), we use the ChatCompletion endpoint with a single user message. Otherwise, we use the older Completion endpoint. We fetch the text from the response and also get token usage if provided. (Make sure your OpenAI API key is set in the environment.)\n",
        "Anthropic: We format the prompt in the required way for Claude and call the clientâ€™s completion method. (This assumes the anthropic package is installed and imported.) Token count isnâ€™t directly captured here.\n",
        "Together: We initialize the Together client and call the chat.completions.create method with the prompt as a user message. (This assumes the model supports chat format; for pure text-generation models on Together, you might use a different method like client.completion.create.) We extract the content from the response. (Token usage may be available via Togetherâ€™s response, but for simplicity, we set it to None in this example.)\n",
        "We measure the time just before and after the call to compute latency.\n",
        "Finally, we return output_text (the modelâ€™s answer), tokens_used, and latency.\n",
        "This function abstracts away the differences in model access, giving us a unified interface for the evaluation loop.\n",
        "\n",
        "5.2 evaluate_model Implementation: This function will loop through all questions for a single model, use call_model to get the answer, check correctness, and record results. It will return a list of result records (one per question for that model) and also compute summary metrics (like number correct). Weâ€™ll implement evaluate_model next:"
      ],
      "metadata": {
        "id": "lzix9vk7rwpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Define file paths\n",
        "original_questions_file = 'data/questions/MIR-2024-v01-t01.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "\n",
        "# Load answer key (assumed to be a dictionary with question IDs as keys)\n",
        "with open(answer_key_file, 'r', encoding='utf-8') as f:\n",
        "    answer_key = json.load(f)\n",
        "\n",
        "# Load original questions (assumed to be a list)\n",
        "with open(original_questions_file, 'r', encoding='utf-8') as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "# Filter out questions that are not in the answer key or whose answer is empty\n",
        "filtered_questions = [\n",
        "    q for q in questions\n",
        "    if q.get('id') in answer_key and answer_key[q.get('id')].strip() != \"\"\n",
        "]\n",
        "\n",
        "# Save filtered questions to a new file\n",
        "with open(filtered_questions_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(filtered_questions, f, indent=2)\n",
        "\n",
        "print(f\"Filtered questions saved to {filtered_questions_file} with {len(filtered_questions)} questions out of {len(questions)} original questions.\")\n"
      ],
      "metadata": {
        "id": "bibvMSjJEYcE",
        "outputId": "c981dc83-a9b7-4ebb-a5cf-9a23089927e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered questions saved to data/questions/MIR-2024-v01-t01_filtered.json with 180 questions out of 185 original questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.1: Patch OpenAI to support legacy calls in evaluator.py and reload evaluator module\n",
        "\n",
        "!pip install openai==0.28\n",
        "\n",
        "import openai\n",
        "\n",
        "# Define a wrapper class so that openai.chat.completions.create(...) works.\n",
        "class ChatCompletionsWrapper:\n",
        "    @staticmethod\n",
        "    def create(*args, **kwargs):\n",
        "        return openai.ChatCompletion.create(*args, **kwargs)\n",
        "\n",
        "class OpenAIChatWrapper:\n",
        "    completions = ChatCompletionsWrapper\n",
        "\n",
        "# Assign our wrapper to openai.chat\n",
        "openai.chat = OpenAIChatWrapper\n",
        "\n",
        "# Verify the patch:\n",
        "print(\"openai.chat.completions.create:\", openai.chat.completions.create)\n",
        "\n",
        "# Reload evaluator so that it picks up our patched openai\n",
        "import importlib\n",
        "import src.evaluator as evaluator_module\n",
        "importlib.reload(evaluator_module)\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "print(\"OpenAI version:\", openai.__version__)\n"
      ],
      "metadata": {
        "id": "C3Uqr1dYpKxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc585777-3a3f-4aad-e706-6b0c7634b477"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "openai.chat.completions.create: <function ChatCompletionsWrapper.create at 0x7ef4b3cb5440>\n",
            "OpenAI version: 0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.2\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "def extract_answer_letter(output):\n",
        "    \"\"\"\n",
        "    Extracts the first valid answer letter (A, B, C, D, or N) from the model output.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\b([ABCDN])\\b', output.upper())\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "# Parameters for test evaluation\n",
        "models_to_test = ['o3-mini-2025-01-31', 'claude-3-7-sonnet-20250219']\n",
        "prompt_strategy = \"Prompt-001\"\n",
        "sample_size = 5  # Evaluate 5 questions per model\n",
        "\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "\n",
        "# Define a separate folder for test evaluation results\n",
        "test_results_folder = \"data/test_results\"\n",
        "os.makedirs(test_results_folder, exist_ok=True)\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "results_summary = {}\n",
        "\n",
        "print(\"\\n--- RUNNING SAMPLE TEST EVALUATION (Results saved in a separate folder) ---\")\n",
        "for model in models_to_test:\n",
        "    print(f\"\\nEvaluating Model: {model} using Prompt Strategy: {prompt_strategy}\")\n",
        "    try:\n",
        "        # Run evaluation; if evaluator.run_evaluation supports an output_folder parameter, use it.\n",
        "        # Otherwise, move the result file to the test folder after creation.\n",
        "        result_file = evaluator.run_evaluation(\n",
        "            questions_file=filtered_questions_file,\n",
        "            answer_key_file=answer_key_file,\n",
        "            prompt_strategy=prompt_strategy,\n",
        "            model=model,\n",
        "            sample_size=sample_size\n",
        "            # , output_folder=test_results_folder  # Uncomment if supported by your evaluator\n",
        "        )\n",
        "        # If output_folder parameter is not supported, move the file manually:\n",
        "        new_result_file = os.path.join(test_results_folder, os.path.basename(result_file))\n",
        "        os.rename(result_file, new_result_file)\n",
        "        result_file = new_result_file\n",
        "\n",
        "        print(f\"âœ“ Sample evaluation complete for {model}. Results saved to: {result_file}\")\n",
        "\n",
        "        with open(result_file, 'r') as f:\n",
        "            results = json.load(f)\n",
        "\n",
        "        details = results.get(\"details\", [])\n",
        "        if details:\n",
        "            print(\"\\n--- DETAILS FOR EACH QUESTION ---\")\n",
        "            for entry in details:\n",
        "                question_id = entry.get(\"question_id\", \"N/A\")\n",
        "                question_prompt = entry.get(\"prompt\", \"No prompt available\")\n",
        "                raw_output = entry.get(\"model_output\", \"No output\")\n",
        "                extracted = extract_answer_letter(raw_output)\n",
        "                entry[\"extracted_answer\"] = extracted\n",
        "                print(f\"Question ID: {question_id}\")\n",
        "                print(\"Prompt:\")\n",
        "                print(question_prompt)\n",
        "                print(\"Raw Output:\")\n",
        "                print(raw_output)\n",
        "                print(\"Extracted Answer:\", extracted)\n",
        "                print(\"-\" * 40)\n",
        "        else:\n",
        "            print(\"\\nNo per-question details found in the evaluation results.\")\n",
        "\n",
        "        summary = results.get(\"summary\", {})\n",
        "        results_summary[model] = summary\n",
        "\n",
        "        print(\"\\n--- SAMPLE EVALUATION SUMMARY ---\")\n",
        "        print(f\"Model: {summary.get('model', 'N/A')}\")\n",
        "        print(f\"Prompt Strategy: {summary.get('prompt_strategy', 'N/A')}\")\n",
        "        print(f\"Total Questions: {summary.get('total_questions', 'N/A')}\")\n",
        "        print(f\"Correct Answers: {summary.get('correct_count', 'N/A')} ({summary.get('accuracy', 0)*100:.2f}%)\")\n",
        "        print(f\"Incorrect Answers: {summary.get('incorrect_count', 'N/A')}\")\n",
        "        print(f\"Skipped Questions: {summary.get('skipped_count', 'N/A')}\")\n",
        "        print(f\"Invalid Count: {summary.get('invalid_count', 'N/A')}\")\n",
        "        print(f\"Total Score: {summary.get('total_score', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error during sample evaluation for {model}: {e}\")\n",
        "\n",
        "print(\"\\nIf the sample evaluation looks good, proceed to full evaluation in the next cell.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg_k5kIJ3RKJ",
        "outputId": "1a1c05b6-7bac-42a8-d868-887f77f06835"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RUNNING SAMPLE TEST EVALUATION (Results saved in a separate folder) ---\n",
            "\n",
            "Evaluating Model: o3-mini-2025-01-31 using Prompt Strategy: Prompt-001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  5.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Sample evaluation complete for o3-mini-2025-01-31. Results saved to: data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.json\n",
            "\n",
            "No per-question details found in the evaluation results.\n",
            "\n",
            "--- SAMPLE EVALUATION SUMMARY ---\n",
            "Model: o3-mini-2025-01-31\n",
            "Prompt Strategy: Prompt-001\n",
            "Total Questions: 5\n",
            "Correct Answers: 5 (100.00%)\n",
            "Incorrect Answers: 0\n",
            "Skipped Questions: 0\n",
            "Invalid Count: 0\n",
            "Total Score: 15\n",
            "\n",
            "Evaluating Model: claude-3-7-sonnet-20250219 using Prompt Strategy: Prompt-001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Sample evaluation complete for claude-3-7-sonnet-20250219. Results saved to: data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.json\n",
            "\n",
            "No per-question details found in the evaluation results.\n",
            "\n",
            "--- SAMPLE EVALUATION SUMMARY ---\n",
            "Model: claude-3-7-sonnet-20250219\n",
            "Prompt Strategy: Prompt-001\n",
            "Total Questions: 5\n",
            "Correct Answers: 4 (80.00%)\n",
            "Incorrect Answers: 1\n",
            "Skipped Questions: 0\n",
            "Invalid Count: 0\n",
            "Total Score: 11\n",
            "\n",
            "If the sample evaluation looks good, proceed to full evaluation in the next cell.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In evaluate_model:\n",
        "We iterate over each question, format the prompt, and call the model via call_model.\n",
        "We wrap the model call in a try-except to catch any errors (for instance, if an API call fails or a model isnâ€™t available). If thereâ€™s an error, we log it and move on, leaving output empty for that question.\n",
        "We then parse the modelâ€™s output to extract the answer. We assume the model should reply with a letter. The code checks the first character of the output: if itâ€™s one of â€œA, B, C, Dâ€, we treat that as the chosen option. (If the output is something else, you could include additional parsing logic â€“ for example, sometimes the model might output the full option text or a sentence. Here, we simplify by taking the first letter when possible. If the output is empty or doesnâ€™t start with a letter, we mark the answer as incorrect by default.)\n",
        "We compare the modelâ€™s answer letter (uppercased) to the true answer letter from the question. If they match, itâ€™s correct and we increment correct_count.\n",
        "We append a dictionary to results containing all relevant info: question ID, model name, the exact prompt used, the modelâ€™s raw output, a boolean for correctness, latency (in seconds), and tokens used.\n",
        "We also print a one-line progress update for each question, indicating what the model answered and whether it was correct. This helps to monitor the evaluation as it happens, especially if many questions are being tested.\n",
        "Finally, the function returns the list of results and the count of correct answers.\n",
        "With these functions in place, we can now evaluate all models and compile the metrics.\n",
        "6. Run Evaluation for All Models (Cell 7)\n",
        "Now weâ€™ll loop through each model in our models_config, evaluate it on all questions using evaluate_model, and collect the outcomes. We will calculate summary metrics for each model:\n",
        "Accuracy (% correct)\n",
        "Total score (number of correct answers out of total questions)\n",
        "Total tokens used (if available; this could be sum of tokens across all questions for that model)\n",
        "Average response time per question (latency)\n",
        "Weâ€™ll store summary results in a list of dictionaries (which we can later convert to a DataFrame for display or CSV export). Weâ€™ll also accumulate all per-question results into a single list for detailed logging."
      ],
      "metadata": {
        "id": "v-QBGGZIr927"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7 FULL EVALUATION\n",
        "\n",
        "import re\n",
        "import json\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "def extract_answer_letter(output):\n",
        "    match = re.search(r'\\b([ABCDN])\\b', output.upper())\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "# Parameters for full evaluation (you might want to increase sample_size or use all questions)\n",
        "models_to_evaluate = ['o3-mini-2025-01-31', 'claude-3-7-sonnet-20250219']\n",
        "prompt_strategy = \"Prompt-001\"\n",
        "# For full evaluation, you might set sample_size to None or the full count.\n",
        "sample_size = None\n",
        "\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "print(\"\\n--- RUNNING FULL EVALUATION ---\")\n",
        "for model in models_to_evaluate:\n",
        "    try:\n",
        "        result_file = evaluator.run_evaluation(\n",
        "            questions_file=filtered_questions_file,\n",
        "            answer_key_file=answer_key_file,\n",
        "            prompt_strategy=prompt_strategy,\n",
        "            model=model,\n",
        "            sample_size=sample_size  # Full evaluation: use all questions\n",
        "        )\n",
        "        print(f\"âœ“ Full evaluation complete for {model}. Results saved to: {result_file}\")\n",
        "        # The full evaluation results will later be merged and exported to CSV.\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error during full evaluation for {model}: {e}\")\n",
        "\n",
        "print(\"\\nProceed to the cell that merges and exports full evaluation results.\")\n"
      ],
      "metadata": {
        "id": "k0Pbpuf6r9dF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279d2a84-a706-4b1e-db5c-980238167f59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RUNNING FULL EVALUATION ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [17:13<00:00,  5.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Full evaluation complete for o3-mini-2025-01-31. Results saved to: data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [03:56<00:00,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Full evaluation complete for claude-3-7-sonnet-20250219. Results saved to: data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.json\n",
            "\n",
            "Proceed to the cell that merges and exports full evaluation results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In Cell 7:\n",
        "We initialize all_details to gather every questionâ€™s result and summary_records for each model.\n",
        "We loop over each model configuration:\n",
        "* Call evaluate_model for that model, which returns the detailed results and count of correct answers.\n",
        "* We extend the all_details list with the results (so in the end, this list contains an entry for each model-question pair).\n",
        "* Compute accuracy as (num_correct / total_questions) * 100. We round it to two decimal places later for neatness.\n",
        "* Compute total tokens used by summing the tokens_used for each question result, if available. If none of the results have token info (i.e., the list is empty because maybe the API didnâ€™t provide it), we leave total_tokens as None.\n",
        "* Compute average latency by summing all latencies and dividing by number of questions (we exclude any None latencies just in case).\n",
        "* Append a dictionary to summary_records with the modelâ€™s name and metrics. We include total questions for reference, and round the accuracy and average latency for readability.\n",
        "* Print a summary line for each model (e.g., â€œFinished ModelX: 8/10 correct, Accuracy 80.0%.â€).\n",
        "After this loop, we have:\n",
        "* summary_records: a list of summary info for each model.\n",
        "* all_details: a list of per-question info, which we can turn into a detailed log.\n",
        "Next, weâ€™ll convert these to pandas DataFrames for easy viewing and export."
      ],
      "metadata": {
        "id": "_-mQLra8sJBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# FINAL EXPORT CELL: Full Evaluation Results to CSV, GitHub, & Download\n",
        "# =========================\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Step 1: Gather Full Evaluation Result Files ---\n",
        "# Adjust the glob pattern if necessary to include only full evaluation files.\n",
        "# (This example assumes your full evaluation result files contain \"filtered\" in the filename.)\n",
        "result_files = glob.glob(\"data/results/EVAL-MIR-2024-v01-t01_filtered-*.json\")\n",
        "print(f\"Found {len(result_files)} full evaluation result file(s).\")\n",
        "\n",
        "# --- Step 2: Merge Detailed Evaluation Results ---\n",
        "# We assume each JSON file contains detailed results under either the \"details\" or \"results\" key.\n",
        "all_details = []\n",
        "for file in result_files:\n",
        "    try:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        details = data.get('details') or data.get('results')\n",
        "        if details:\n",
        "            all_details.extend(details)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "if not all_details:\n",
        "    print(\"No detailed evaluation results were found in the JSON files.\")\n",
        "else:\n",
        "    # Convert the merged results to a DataFrame.\n",
        "    df_full = pd.DataFrame(all_details)\n",
        "    print(\"Merged full evaluation results shape:\", df_full.shape)\n",
        "    print(\"Columns in full evaluation results:\", df_full.columns.tolist())\n",
        "\n",
        "    # --- Step 3: Export Merged Results to a Single CSV File ---\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    export_filename = f\"full_evaluation_results_{timestamp}.csv\"\n",
        "    export_dir = os.path.join(\"data\", \"exports\")\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "    export_path = os.path.join(export_dir, export_filename)\n",
        "    df_full.to_csv(export_path, index=False)\n",
        "    print(f\"âœ“ Full evaluation results exported to CSV: {export_path}\")\n",
        "\n",
        "    # --- Step 4: Download the CSV File Locally (Colab) ---\n",
        "    files.download(export_path)\n",
        "\n",
        "    # --- Step 5: Push the CSV File to GitHub ---\n",
        "    try:\n",
        "        # Retrieve GitHub token from Colab secrets\n",
        "        github_token = userdata.get('GITHUB_TOKEN')\n",
        "        if not github_token:\n",
        "            raise ValueError(\"GITHUB_TOKEN is not set in Colab secrets.\")\n",
        "\n",
        "        # Configure the remote URL with your GitHub token\n",
        "        repo_name = \"armelida/MELIDA\"\n",
        "        token_url = f\"https://{github_token}@github.com/{repo_name}.git\"\n",
        "\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", \"armelida@gmail.com\"], check=True)\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", \"Armelida\"], check=True)\n",
        "        subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", token_url], check=True)\n",
        "\n",
        "        # Stage the new CSV file for commit.\n",
        "        subprocess.run([\"git\", \"add\", export_path], check=True)\n",
        "        commit_message = f\"Export full evaluation results {timestamp}\"\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], check=True)\n",
        "\n",
        "        # Pull the latest changes and then push your commit.\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", \"main\", \"--rebase\"], check=True)\n",
        "        subprocess.run([\"git\", \"push\", \"origin\", \"main\"], check=True)\n",
        "        print(\"âœ“ CSV file successfully pushed to GitHub.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GitHub push: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "hfno6LLELaoL",
        "outputId": "7ffdb262-d919-4adc-9722-06189fce4562"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 full evaluation result file(s).\n",
            "Merged full evaluation results shape: (360, 14)\n",
            "Columns in full evaluation results: ['question_id', 'question_text', 'prompt_strategy', 'model', 'prompt', 'full_model_output', 'model_answer', 'raw_response', 'response_time', 'tokens_used', 'timestamp', 'correct_answer', 'score', 'result_id']\n",
            "âœ“ Full evaluation results exported to CSV: data/exports/full_evaluation_results_20250402_221350.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e028fd29-a8e2-4abe-bc3b-1a0a9b0f9f87\", \"full_evaluation_results_20250402_221350.csv\", 591323)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ CSV file successfully pushed to GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CELL: Export Most Failed Questions CSV\n",
        "# =========================\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# (Optional) Load the most recent full evaluation CSV from your exports folder:\n",
        "export_files = sorted(glob.glob(\"data/exports/full_evaluation_results_*.csv\"))\n",
        "if export_files:\n",
        "    latest_export = export_files[-1]\n",
        "    df_full = pd.read_csv(latest_export)\n",
        "    print(f\"Loaded merged full evaluation results from: {latest_export}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No full evaluation CSV file found in data/exports/.\")\n",
        "\n",
        "# Standardize answer columns and compute correctness.\n",
        "df_full['model_answer'] = df_full['model_answer'].astype(str).str.strip().str.upper()\n",
        "df_full['correct_answer'] = df_full['correct_answer'].astype(str).str.strip().str.upper()\n",
        "df_full['raw_response'] = df_full['raw_response'].astype(str).str.strip()\n",
        "df_full['correct'] = df_full['model_answer'] == df_full['correct_answer']\n",
        "\n",
        "# Determine which column to use for question text.\n",
        "if 'prompt' in df_full.columns:\n",
        "    question_text_col = 'prompt'\n",
        "elif 'question_text' in df_full.columns:\n",
        "    question_text_col = 'question_text'\n",
        "else:\n",
        "    # If no column exists, create one with a default value.\n",
        "    df_full['question_text'] = \"Not available\"\n",
        "    question_text_col = 'question_text'\n",
        "\n",
        "# Filter out only the failed evaluations.\n",
        "df_failures = df_full[~df_full['correct']]\n",
        "\n",
        "# Group by question_id to aggregate failure information.\n",
        "df_failures_summary = df_failures.groupby(\"question_id\").agg(\n",
        "    failure_count=(\"model\", \"count\"),\n",
        "    models_failed=(\"model\", lambda x: \", \".join(sorted(x.unique()))),\n",
        "    correct_answer=(\"correct_answer\", \"first\"),\n",
        "    raw_responses=(\"raw_response\", lambda x: \" || \".join(x.astype(str).unique())),\n",
        "    question_text=(question_text_col, \"first\")\n",
        ").reset_index()\n",
        "\n",
        "# Sort by failure_count descending (most failed questions at the top).\n",
        "df_failures_summary = df_failures_summary.sort_values(\"failure_count\", ascending=False)\n",
        "\n",
        "# Save the summary to a CSV file.\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "failed_csv = f\"most_failed_questions_{timestamp}.csv\"\n",
        "export_dir = os.path.join(\"data\", \"exports\")\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "export_path = os.path.join(export_dir, failed_csv)\n",
        "df_failures_summary.to_csv(export_path, index=False)\n",
        "print(f\"âœ“ Most failed questions exported to CSV: {export_path}\")\n",
        "\n",
        "# Optionally, display the DataFrame.\n",
        "print(\"=== Most Failed Questions ===\")\n",
        "print(df_failures_summary.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vphnlZLZOrGl",
        "outputId": "28c8ae0e-6473-4df1-f2c4-bf9e828dfe7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded merged full evaluation results from: data/exports/full_evaluation_results_20250402_221350.csv\n",
            "âœ“ Most failed questions exported to CSV: data/exports/most_failed_questions_20250402_221351.csv\n",
            "=== Most Failed Questions ===\n",
            "          question_id  failure_count                                  models_failed correct_answer raw_responses                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             question_text\n",
            "MIR-2024-v01-t01-Q084              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              D      nan || A                                                                                                                                                                                                                                                                                                                                                                                                                             Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 81 aÃ±os con antecedentes de HTA, dislipemia y enfermedad de Alzheimer leve. Avisa su familia porque lo han encontrado en su casa caÃ­do en el suelo y no es capaz de movilizar extremidades derechas. Llega al hospital trasladado por emergencias como cÃ³digo ictus. A la exploraciÃ³n fÃ­sica destaca paresia facial derecha supranuclear, afasia global, hemianopsia derecha y hemiplejia de miembros derechos. La TC craneal se informa como ASPECTS 8 sin datos de sangrado. En angioTC oclusiÃ³n de segmento M2 de divisiÃ³n anterior de ACM izquierda. Â¿CuÃ¡l de los siguientes es el mejor tratamiento inicial para este paciente?\\n\\nA) Fibrinolisis con alteplasa.\\nB) Doble antiagregaciÃ³n y estatinas.\\nC) Manejo conservador por los antecedentes del paciente.\\nD) TrombectomÃ­a mecÃ¡nica.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q068              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B      nan || A                                                                                       Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nAvisan de paritorio por parto a tÃ©rmino. Se explora a reciÃ©n nacida, sin esfuerzo de llanto, flÃ¡cida y cianÃ³tica. Se seca y estimula y ante la ausencia de inicio de llanto se traslada a cuna tÃ©rmica. Inicia llanto intenso y eficaz a los 30 segundos de vida, donde ya se podÃ­a observar recuperaciÃ³n del tono de forma completa con movimiento activo, frecuencia cardÃ­aca por encima de 100 lpm y estornudo tras aspiraciÃ³n de secreciones farÃ­ngeas espesas. Sin embargo, persiste cianosis generalizada hasta 1 minuto y 20 segundos de vida a partir del cual presenta solamente acrocianosis. A los 4 minutos de vida inicia dificultad respiratoria (esfuerzo respiratorio irregular) consistente en polipnea, tiraje subcostal e intercostal leve-moderado, leve aleteo nasal y quejido espiratorio (con frecuencia cardÃ­aca por encima de 100 lpm, movimiento activo, recuperaciÃ³n completa del color con coloraciÃ³n completamente rosada y reflejos de tos y estornudo), que se resuelve tras colocar presiÃ³n positiva durante 2 minutos. Â¿QuÃ© puntuaciÃ³n de Apgar tiene esta reciÃ©n nacida?\\n\\nA) 4/8.\\nB) 8/9.\\nC) 9/9.\\nD) 9/10.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q043              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B      nan || C                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nSeÃ±ala la opciÃ³n INCORRECTA respecto a la enfermedad meningocÃ³cica:\\n\\nA) El mecanismo de transmisiÃ³n es por gotas.\\nB) Se dispone de vacunas efectivas Ãºnicamente para la prevenciÃ³n de 4 subtipos de meningococo.\\nC) La vacuna frente a meningococo tipo B estÃ¡ incluida en el calendario sistemÃ¡tico de inmunizaciones del Sistema Nacional de Salud.\\nD) Ante un contacto de riesgo con un paciente con meningitis meningocÃ³cica, la vacunaciÃ³n del contacto no debe ser la primera estrategia de profilaxis postexposiciÃ³n.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q079              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              C      nan || D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nMujer de 35 aÃ±os consulta por cuadro de diplopia y debilidad proximal progresivas que empeora a lo largo de dÃ­a. En la exploraciÃ³n neurolÃ³gica se aprecia voz nasal, ptosis bilateral sin afectaciÃ³n pupilar, limitaciÃ³n para la abducciÃ³n de ojo derecho y debilidad proximal con reflejos miotÃ¡ticos conservados. Respecto a esta paciente seÃ±ale la opciÃ³n INCORRECTA:\\n\\nA) El nivel de anticuerpos es independiente de la gravedad de la enfermedad.\\nB) En ocasiones se asocia a otra enfermedad autoinmune.\\nC) Es frecuente la agregaciÃ³n familiar.\\nD) La respuesta a inmunosupresores es mayor en pacientes con anticuerpos antirreceptor de acetilcolina y anticuerpos antiMusk positivos.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q181              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B             D                                                                                                                                                                                                                                                                                                                                                                                                             Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nEn relaciÃ³n con la marca producida por el paso de la corriente elÃ©ctrica a travÃ©s de la piel (tambiÃ©n conocida como lesiÃ³n electroespecÃ­fica o â€œmarca de Jellinekâ€), Â¿cuÃ¡l de las siguientes afirmaciones es cierta?:\\n\\nA) La mayor parte de las lesiones producidas por la corriente elÃ©ctrica en medio domÃ©stico y laboral proceden de fuentes de alto voltaje.\\nB) Las lesiones dÃ©rmicas dependen del efecto de un aumento de la temperatura generado sobre la dermis.\\nC) La corriente de bajo voltaje produce lesiones profundas, mientras que la de alto voltaje da lugar a lesiones superficiales adyacentes al punto de contacto con la corriente elÃ©ctrica.\\nD) La marca elÃ©ctrica es diagnÃ³stica del lugar en el que se ha producido el contacto y, por tanto, el punto de entrada de la corriente en el cuerpo.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q160              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              A      nan || C                                                                                                                                                                                                                                           Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nMujer de 25 aÃ±os, sin enfermedades previas conocidas, que consulta por lesiÃ³n en labio inferior. No es la primera vez que le sucede desde la adolescencia, siempre en la misma zona, inicia dolor y aparece una lesiÃ³n vesiculosa que se autolimita en una semana aproximadamente. Respecto al abordaje de la enfermedad de esta paciente, es INCORRECTO que:\\n\\nA) Si presenta mÃ¡s de tres episodios anuales estÃ¡ indicado realizar profilaxis con un antiviral como valaciclovir 500 mg cada 12 horas o 1 g al dÃ­a, durante seis meses.\\nB) Si es una Ãºnica lesiÃ³n y es un cuadro leve puede no realizarse ningÃºn tratamiento o bien realizar tratamiento tÃ³pico con fomentos de sulfato de zinc o de cobre.\\nC) Si existe riesgo de afectaciÃ³n oftÃ¡lmica estarÃ­a indicado tratar con un antivÃ­rico como valaciclovir a razÃ³n de 1 g cada 8 horas durante 7 dÃ­as vÃ­a oral.\\nD) Es necesario recomendarle una adecuada protecciÃ³n solar pues la exposiciÃ³n solar puede motivar recurrencias.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q206              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              D             C                                                                                                                                                                                                                                                               Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nAvisan de paritorio por reciÃ©n nacido pretÃ©rmino tardÃ­o (35+3 semanas de edad gestacional) sin otros factores de riesgo. Al nacimiento no presenta esfuerzo respiratorio y se decide monitorizaciÃ³n con pulsioximetrÃ­a. No inicia esfuerzo por lo que a los 30 segundos se ventila con presiÃ³n positiva intermitente (VPPI) (presiÃ³n inspiratoria 20 cm H20 y PEEP 6 cm H20 y FiO2 0,21) durante 1 minuto. Inicia llanto y esfuerzo respiratorio pero con signos de dificultad respiratoria en torno a los 3 minutos de vida con cianosis asociada. En la monitorizaciÃ³n presenta frecuencia cardÃ­aca de 170 lpm y Sat O2 del 75%. Â¿CuÃ¡l es su actuaciÃ³n en este momento?:\\n\\nA) Retirar soporte respiratorio y aumento aporte de oxÃ­geno.\\nB) Mantener VPPI y fracciÃ³n inspirada de oxÃ­geno al 21%.\\nC) Pasar a modalidad presiÃ³n positiva continua y aumento aporte de oxÃ­geno.\\nD) Pasar a modalidad presiÃ³n positiva continua y fracciÃ³n inspirada de oxÃ­geno al 21%.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q173              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              A             D                                                                                                                                                                                                                                                                                                                                                 Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 55 aÃ±os con antecedentes de cÃ¡ncer de pulmÃ³n (estadio IV). Su madre muriÃ³ a los 65 aÃ±os por un episodio de broncoaspiraciÃ³n. Consulta por debilidad muscular en los muslos y disfagia. ExploraciÃ³n fÃ­sica: fuerza en psoas 4+/5, cuÃ¡driceps 3/5, deltoides 5/5. Presenta ptosis palpebral bilateral y no tiene diplopÃ­a. Los valores de CPK eran 900 UI/L (normal <150 UI/L) y los de aldolasa, 7 UI/L (normal <6 UI/L). Se practicÃ³ una biopsia en el mÃºsculo vasto lateral externo del muslo derecho, en la cual se objetivÃ³ variabilidad en el tamaÃ±o de fibras, aumento del tejido conectivo, imÃ¡genes de pseudodivisiÃ³n celular y vacuolas ribeteadas; no se observÃ³ infiltrado inflamatorio. Â¿QuÃ© es lo mÃ¡s probable que le suceda al paciente?:\\n\\nA) Distrofia oculofarÃ­ngea.\\nB) MiopatÃ­a mitocondrial.\\nC) Miastenia gravis.\\nD) Miositis con cuerpos de inclusiÃ³n.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q169              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B      nan || C Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 53 aÃ±os, sin antecedentes patolÃ³gicos, acude al hospital por fiebre de 10 dÃ­as de evoluciÃ³n, con intensa cefalea retroorbitaria y una importante sensaciÃ³n de fatiga. No refiere dolor musculoesquelÃ©tico, lesiones cutÃ¡neas ni ningÃºn otro sÃ­ntoma acompaÃ±ante. A lo largo de 15 dÃ­as de ingreso se objetiva fiebre persistente y la apariciÃ³n de discreta ictericia conjuntival. La exploraciÃ³n respiratoria, cardÃ­aca, abdominal y neurolÃ³gica es normal. La analÃ­tica muestra VSG 97 mm/h, PCR 24,5 mg/dL, GOT 156 U/L, GPT 148 U/L, FA 410 U/L, GGT 794 U/L, bilirrubina total 5,5 mg/dL a expensas de la bilirrubina directa, hemoglobina 11,5 g/dL, recuento leucocitario normal, plaquetas 648.000/mm3, funciÃ³n renal e ionograma normales, tiempo de protrombina normal pero TTPA ratio 1,8. Los anticuerpos antinucleares (ANA) son positivos a tÃ­tulos 1/160. Una ecografÃ­a abdominal no muestra ninguna alteraciÃ³n destacable. Desde el punto de vista clÃ­nico y a la vista de los datos analÃ­ticos, Â¿cuÃ¡l de las siguientes enfermedades es la que se ajusta mejor con nuestro paciente?:\\n\\nA) COVID-19\\nB) Fiebre Q aguda.\\nC) Leptospirosis grave (enfermedad de Weil).\\nD) Colangitis esclerosante primaria.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q027              1                     claude-3-7-sonnet-20250219              C             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nLa deficiencia de acil-CoA-deshidrogenasa provoca una de las siguientes alteraciones bioquÃ­micas:\\n\\nA) DisminuciÃ³n de Ã¡cidos dicarboxÃ­licos.\\nB) Aumento de la gluconeogÃ©nesis.\\nC) DisminuciÃ³n de la ureagÃ©nesis.\\nD) Aumento de carnitina libre.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q041              1                     claude-3-7-sonnet-20250219              D             C                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nAl finalizar el periodo de seguimiento en el estudio PREDIMET (â€œPrimary Prevention of Cardiovascular Disease with a Mediterranean Dietâ€), el 3,8% de los individuos asignados a dieta mediterrÃ¡nea presentaron algÃºn tipo de evento cardiovascular (infartos de miocardio, ictus o muertes de origen cardiovascular), frente al 4,4% de eventos ocurridos en el grupo control. Â¿CuÃ¡l es el nÃºmero necesario de pacientes a tratar (NNT) con dieta mediterrÃ¡nea para evitar un evento cardiovascular?:\\n\\nA) 26\\nB) 60\\nC) 80\\nD) 167\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q120              1                             o3-mini-2025-01-31              B             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nUn paciente con enfermedad de Crohn con antecedente de extirpaciÃ³n de un melanoma precisa iniciar un tratamiento avanzado por corticodependencia. Â¿CuÃ¡l de los siguientes fÃ¡rmacos se deberÃ­a evitar al estar descrito un aumento del riesgo de dicha neoplasia?\\n\\nA) Azatioprina.\\nB) Infliximab.\\nC) Vedolizumab.\\nD) Ustekinumab.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q091              1                             o3-mini-2025-01-31              B           nan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nMujer de 19 aÃ±os que, practicando patinaje sobre ruedas, sufre un traumatismo indirecto de la rodilla derecha, con luxaciÃ³n de rÃ³tula tratada en urgencias. Acude a revisiÃ³n 2 meses mÃ¡s tarde y refiere que nota dolor y sensaciÃ³n de inestabilidad y subluxaciÃ³n frecuente con la actividad fÃ­sica. De los siguientes elementos, seÃ±ale el que NO favorece la inestabilidad fÃ©moro-patelar:\\n\\nA) AnteversiÃ³n femoral.\\nB) TorsiÃ³n tibial interna.\\nC) Genu valgo.\\nD) Patela alta.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q045              1                     claude-3-7-sonnet-20250219              D             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nEn un metaanÃ¡lisis, el riesgo relativo estimado para la asociaciÃ³n causal entre el uso de mascarilla y la incidencia de SARS-CoV-2 fue de 0,47. Â¿CuÃ¡l es la interpretaciÃ³n correcta de este resultado?:\\n\\nA) La incidencia de SARS-CoV-2 se reduce un 47% cuando la poblaciÃ³n usa mascarilla.\\nB) No usar mascarilla aumenta un 53% el riesgo de infecciÃ³n por SARS-CoV-2.\\nC) El uso de mascarilla podrÃ­a evitar 47 de cada 100 casos de SARS-CoV-2 que se dan en personas que no la usan.\\nD) La incidencia de SARS-CoV-2 en la poblaciÃ³n que no usa mascarilla se reducirÃ­a un 53% si la utilizara.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q069              1                     claude-3-7-sonnet-20250219              B             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nRespecto a las medidas a tomar ante un niÃ±o diagnosticado de tos ferina, en relaciÃ³n a sus contactos, indique la respuesta INCORRECTA:\\n\\nA) Tratar con antibiÃ³ticos a toda la familia, independientemente de la edad, estado de inmunizaciÃ³n y sintomatologÃ­a.\\nB) Tratar con antibiÃ³ticos a todo el personal sanitario que ha atendido al niÃ±o independientemente de la edad, estado de inmunizaciÃ³n y sintomatologÃ­a.\\nC) Vacunar a los adultos convivientes, aunque hayan padecido la enfermedad ya que la inmunidad no dura toda la vida.\\nD) Aislar al paciente al menos 5 dÃ­as despuÃ©s de iniciar el tratamiento.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q037              1                     claude-3-7-sonnet-20250219              C             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nMutaciones en el gen Btk pueden causar una inmunodeficiencia primaria que cursa con la disminuciÃ³n drÃ¡stica o ausencia de inmunoglobulinas en la sangre perifÃ©rica (agammaglobulinemia). En relaciÃ³n al estudio de los pacientes con esta agammaglobulinemia, Â¿cuÃ¡l de las siguientes afirmaciones es cierta?\\n\\nA) Es una enfermedad autosÃ³mica recesiva.\\nB) El porcentaje de linfocitos que expresan CD19 o CD20 (linfocitos B) en sangre perifÃ©rica estÃ¡ dentro de los lÃ­mites normales.\\n\\nC) El nivel de IgG en suero disminuye paulatinamente desde el nacimiento hasta resultar indetectable.\\nD) La expresiÃ³n de la proteÃ­na Btk en monocitos es normal.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q149              1                             o3-mini-2025-01-31              A             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nMujer de 90 aÃ±os sin alergias medicamentosas conocidas, con HTA y gonalgia nocturna por gonartrosis derecha de intensidad leve-moderada de tres meses de evoluciÃ³n. Refiere que no toma ninguna medicaciÃ³n y se confirma que realiza las medidas higiÃ©nicas del sueÃ±o correctamente. Consulta a su mÃ©dico de atenciÃ³n primaria por insomnio de conciliaciÃ³n y mantenimiento que le interfiere con sus actividades diarias. Â¿QuÃ© fÃ¡rmaco es mÃ¡s adecuado iniciar en este caso?:\\n\\nA) Paracetamol.\\nB) Diazepam.\\nC) Mirtazapina.\\nD) Trazodona.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q126              1                             o3-mini-2025-01-31              D           nan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 77 aÃ±os que acude a urgencias por prurito, ictericia y sÃ­ndrome tÃ³xico de un mes de evoluciÃ³n. Refiere ingesta de amoxicilinaclavulÃ¡nico desde hace 10 dÃ­as por un resfriado. A su llegada se realiza una ecografÃ­a que se informa de dilataciÃ³n de la vÃ­a biliar intra y extrahepÃ¡tica con vesÃ­cula biliar distendida. Â¿CuÃ¡l es la sospecha diagnÃ³stica y la actitud a seguir?:\\n\\nA) Colecistitis aguda e indicaciÃ³n de colecistectomÃ­a urgente.\\nB) Coledocolitiasis distal y solicitud de colangiorresonancia magnÃ©tica.\\nC) Toxicidad por fÃ¡rmacos y suspensiÃ³n de la medicaciÃ³n.\\nD) TumoraciÃ³n maligna obstructiva de colÃ©doco distal y solicitar TC abdominal.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q137              1                     claude-3-7-sonnet-20250219              A             C                                                                                                                                                                                                                                                                                                                                                                                                                                                       Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 28 aÃ±os que consulta por haberse despertado por dolor en la regiÃ³n genital y tumefacciÃ³n en el pene. No recuerda con detalle lo sucedido en las Ãºltimas 12 horas. En las Ãºltimas 48 h ha consumido alto volumen de alcohol, varias drogas (mefedrona, cocaÃ­na, anfetamina), junto con sildenafilo y se ha autoinyectado en el pene 20 Âµg de alprostadil. A la exploraciÃ³n destaca, pene flÃ¡cido, glande no visible en su totalidad por edema y hematoma expansivo de partes blandas. Presenta restos hemÃ¡ticos en el meato uretral. Â¿CuÃ¡l de las siguientes opciones NO formarÃ­a parte de su plan terapÃ©utico?:\\n\\nA) GasometrÃ­a obtenida mediante punciÃ³n del cuerpo cavernoso.\\nB) RevisiÃ³n quirÃºrgica urgente.\\nC) Evitar el sondaje vesical.\\nD) EcografÃ­a de pene.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q170              1                             o3-mini-2025-01-31              B           nan                                                                                                                                                                                                                                                                                                                                                          Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 38 aÃ±os con dependencia alcohÃ³lica grave. Ha tenido varios ingresos por pancreatitis aguda en el contexto de ingesta de alcohol e hipertrigliceridemia grave. Acude a consulta 12 meses despuÃ©s del Ãºltimo episodio de pancreatitis. No presenta clÃ­nica que sugiera insuficiencia pancreÃ¡tica exocrina. Niega reiteradamente ingesta de alcohol. Sigue tratamiento con estatinas y fibratos que asegura tomÃ¡rselos. Aporta un anÃ¡lisis bÃ¡sico reciente de su empresa con los siguientes datos: colesterol 300 mg/dL, cHDL 42 mg/dL, TG 1102 mg/dL, cLDL no calculable por Friedewald. Colesterol no-HDL 258 mg/dL, ApoB 90 mg/dL, Lp(a) 38 nmol/L (normal <125 nmol/L). En dicho anÃ¡lisis hay un dato, ademÃ¡s de la hipertrigliceridemia, que sugiere que el paciente continÃºa ingiriendo alcohol. SeÃ±Ã¡lelo.\\n\\nA) ApoB.\\nB) cHDL.\\nC) Lp(a).\\nD) Colesterol total.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q161              1                             o3-mini-2025-01-31              A             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 50 aÃ±os que presenta una erupciÃ³n generalizada especialmente en el tronco. Refiere malestar general desde hace pocos dÃ­as. Hoy acude porque ha tenido fiebre la noche anterior y se ha visto una mancha negra en la zona abdominal. A la exploraciÃ³n fÃ­sica, la mancha negra corresponde a una pequeÃ±a escara. El paciente es cazador y acudiÃ³ recientemente a una batida. Sobre el abordaje del caso es cierto que:\\n\\nA) Es una enfermedad de declaraciÃ³n obligatoria.\\nB) El tratamiento de elecciÃ³n es la amoxicilina, un gramo cada 8 horas durante 7 dÃ­as.\\nC) Debe derivarse a un centro hospitalario.\\nD) En la mayorÃ­a de los casos, la fiebre precede al exantema y la escara.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q148              1                             o3-mini-2025-01-31              A             D                                                                                                                                                                                      Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nMujer de 50 aÃ±os diagnosticada de sÃ­ndrome mielodisplÃ¡sico con monosomÃ­a del cromosoma 7 (45 XX -7 20) es sometida a un trasplante hematopoyÃ©tico alogÃ©nico con sangre de cordÃ³n umbilical de un hombre compatible procedente de un banco de otro paÃ­s. A los tres meses del trasplante con recuperaciÃ³n completa de las cifras hematolÃ³gicas perifÃ©ricas se realiza una punciÃ³n de mÃ©dula Ã³sea, que no presenta alteraciones morfolÃ³gicas y un estudio cromosÃ³mico con un cariotipo 47 XXY 20. Â¿CuÃ¡l es la explicaciÃ³n de estos hallazgos?:\\n\\nA) El donante tiene un sÃ­ndrome de Klinefelter no diagnosticado.\\nB) En la mÃ©dula Ã³sea coexisten cÃ©lulas del donante y del receptor (quimerismo mixto).\\nC) Tras el trasplante ha existido un fenÃ³meno de fusiÃ³n celular entre las cÃ©lulas del donante y receptor que ha originado el nuevo cariotipo.\\nD) En el cariotipo realizado precozmente tras un trasplante hematopoyÃ©tico es habitual encontrar alteraciones cromosÃ³micas transitorias de este tipo que carecen de significaciÃ³n clÃ­nica.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q172              1                     claude-3-7-sonnet-20250219              A             B                                                                                                                                                                                                                                                                                                                                                                         Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 37 aÃ±os con infecciÃ³n VIH ha dejado de tomar el tratamiento por razones personales hace 3 meses; estaba tomando una combinaciÃ³n de dos inhibidores de transcriptasa inversa y un inhibidor de la integrasa. En la evaluaciÃ³n llevada a cabo en este momento tiene 120 linfocitos CD4/mm3 y una carga viral de VIH-1 de 80.000 copias/mL. Estamos pendientes del estudio de resistencias. Â¿QuÃ© debemos recomendarle en este momento?:\\n\\nA) Reiniciar el tratamiento con lo mismo con lo que estaba previamente.\\nB) Esperar a tener el resultado del estudio de resistencias para orientar el tipo de tratamiento en funciÃ³n de las mutaciones identificadas.\\nC) Prescribir a partir de ahora un rÃ©gimen basado en inhibidores de proteasa por su mayor barrera genÃ©tica a las resistencias.\\nD) Iniciar doble terapia con inhibidores de integrasa.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q183              1                             o3-mini-2025-01-31              B           nan                                                                                                                                                                                                                                                                                                                                                                                                               Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nLa telemonitorizaciÃ³n domiciliaria ha demostrado beneficios en pacientes con insuficiencia cardÃ­aca. Una mujer de 82 aÃ±os con insuficiencia cardÃ­aca y diabetes tratada con insulina estÃ¡ en programa de telemonitorizaciÃ³n y remite cada semana una serie de parÃ¡metros a su mÃ©dico de familia. La semana pasada presentaba presiÃ³n arterial 140/85 mmHg. Glucemia capilar basal 120 mg/dL. Glucemia postprandial 2 h tras la comida 160 mg/dL. Peso 83 kg. Frecuencia cardÃ­aca 72 lpm. Al recibir los parÃ¡metros de esta semana que se exponen a continuaciÃ³n, su mÃ©dico decide citar a la paciente en consulta. Â¿CuÃ¡l de los siguientes parÃ¡metros ha motivado dicha decisiÃ³n?:\\n\\nA) PresiÃ³n arterial 135/82 mmHg.\\nB) Peso 85 Kg.\\nC) Glucemia capilar basal 130 mg/dL.\\nD) Glucemia capilar postprandial 190 mg/dL.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q190              1                             o3-mini-2025-01-31              A           nan                                                                                                                                                                        Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nHombre de 36 aÃ±os sin antecedentes de interÃ©s que ingresa en observaciÃ³n de urgencias en espera de intervenciÃ³n quirÃºrgica por fractura de fÃ©mur a nivel diafisario por accidente de trÃ¡fico. El paciente llega algo taquicÃ¡rdico 102 lpm, con gafas nasales para mantener saturaciÃ³n de oxÃ­geno en torno al 96%, 20 respiraciones por minuto y presiÃ³n arterial 100/60 mmHg. Se procede a ajuste de analgesia y a las 20 horas de estancia en observaciÃ³n indican que el paciente puede ir a quirÃ³fano pero presenta un aumento de disnea con saturaciÃ³n del 89% con mascarilla tipo Venturi, 28 respiraciones por minuto, taquicardia a 110 lpm, fiebre de 38ÂºC y se encuentra algo obnubilado. En la analÃ­tica destaca anemia y trombocitopenia. No presenta aumento de volumen en ninguna de las 2 piernas. Â¿QuÃ© prueba complementaria tiene mayor sensibilidad y especificidad para la sospecha diagnÃ³stica de este paciente?\\n\\nA) AngioTC helicoidal con contraste.\\nB) GammagrafÃ­a pulmonar de perfusiÃ³n.\\nC) RadiografÃ­a de tÃ³rax.\\nD) EcocardiografÃ­a.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q201              1                     claude-3-7-sonnet-20250219              B             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nEn la enfermedad por araÃ±azo de gato, seÃ±ale la INCORRECTA:\\n\\nA) En los casos sin manifestaciones sistÃ©micas de la enfermedad es suficiente con tratamiento tÃ³pico.\\nB) Para el diagnÃ³stico es necesaria la prÃ¡ctica de una serologÃ­a.\\nC) Si se precisa tratamiento antibiÃ³tico, se recomienda azitromicina 500 mg. en una dosis inicial y 250 mg cada 24 horas durante 4 dÃ­as mÃ¡s.\\nD) La mayorÃ­a de los casos curan de forma espontÃ¡nea en semanas o meses sin tratamiento de ningÃºn tipo.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n",
            "MIR-2024-v01-t01-Q204              1                             o3-mini-2025-01-31              C             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Eres un MÃ©dico que estÃ¡ realizando el examen MIR, un test estandarizado en espaÃ±ol que determinarÃ¡ si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde ÃšNICAMENTE con una de las letras A, B, C o D. Si no estÃ¡ss seguro, responde con N. Cualquier texto adicional invalidarÃ¡ tu respuesta y restarÃ¡ puntos.\\n\\nGestante de 36 aÃ±os primigesta sin factores de riesgo excepto Ã­ndice de masa corporal de 32 pregestacional, que acude a urgencias a las 28 semanas y presenta una rotura prematura de membranas. En el momento del ingreso la analÃ­tica es normal, no presenta dinÃ¡mica uterina y la longitud cervical es de 38 mm. Â¿CuÃ¡l de las siguientes afirmaciones es INCORRECTA?\\n\\nA) Iniciaremos maduraciÃ³n pulmonar fetal con corticoides.\\nB) Se realizarÃ¡ profilaxis de corioamnionitis mediante antibioterapia endovenosa.\\nC) Se iniciarÃ¡ neuroprotecciÃ³n fetal.\\nD) Se iniciarÃ¡ profilaxis tromboembÃ³lica con heparina de bajo peso molecular.\\n\\nTu respuesta (ÃšNICAMENTE una letra: A, B, C, D o N si no estÃ¡s seguro):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We create two DataFrames:\n",
        "summary_df with one row per model, containing accuracy, scores, etc.\n",
        "details_df with one row per question per model, containing everything from question ID to correctness.\n",
        "We then display the summary and the first few detailed results to verify the content. (In a real Jupyter environment, display(df) will show a nice table. In a text environment or script, you might use print(df.to_string()) or df.head().) Review the summary to ensure metrics make sense, and review the details to spot-check that outputs and correctness are recorded as expected."
      ],
      "metadata": {
        "id": "3JqybaU1tKR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Export Results to CSV (Cell 8)\n",
        "Now that we have the results in DataFrames, weâ€™ll export them to CSV files, which can be used in external analysis tools like Excel or Tableau. We will create two CSV files:\n",
        "llm_eval_summary.csv â€“ containing the summary metrics per model.\n",
        "llm_eval_details.csv â€“ containing the detailed per-question results.\n",
        "These files will include headers and can be imported directly into Tableau or other tools."
      ],
      "metadata": {
        "id": "rdDMq8WKsMLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this, you should find two CSV files in your working directory:\n",
        "llm_eval_summary.csv â€“ with columns like model_name, accuracy (%), total_score, total_questions, tokens_used_total, avg_latency_sec.\n",
        "llm_eval_details.csv â€“ with columns like question_id, model_name, prompt, model_output, correct, latency, tokens_used.\n",
        "These can now be loaded into Tableau or any data analysis software for visualization."
      ],
      "metadata": {
        "id": "EW6I9m2ktTj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Using Tableau for Analysis of Results\n",
        "With the results exported, we can analyze and visualize the performance of the models. Below are step-by-step instructions to use Tableau (or a similar data visualization tool) to explore the data:\n",
        "Import the CSV files into Tableau: Open Tableau and connect to the llm_eval_summary.csv and llm_eval_details.csv files (you can import them separately or join/relate them on the model_name field if needed).\n",
        "Summary Dashboard (Model-Level): Using llm_eval_summary.csv, you can create a simple chart of model performance. For example:\n",
        "Create a bar chart with model_name on the x-axis and accuracy (%) on the y-axis to compare accuracy across models.\n",
        "Add labels to show the exact accuracy or score for each model bar.\n",
        "You could also include avg_latency_sec as a secondary metric (perhaps a separate chart or as a tooltip) to see the speed-accuracy tradeoff.\n",
        "Filter by Prompt or Question: Since all models used the same prompt strategy in this run, the summary is straightforward. If you had different prompt strategies or sets of questions, you could use filters. For instance, if prompt_strategy was a field, you could filter or color-code by it. Or using the detailed data, you could filter to a specific question to see all model answers for that one.\n",
        "Detailed Analysis (Question-Level): Using llm_eval_details.csv, you can analyze which questions were hardest:\n",
        "Create a view with question_id on one axis and perhaps the count of models that got it correct.\n",
        "For example, drag question_id to rows, and an aggregation of correct (treat correct as 0/1 values and take average or sum). Multiply by 100 to interpret as percentage of models correct. This will tell you the percent of models that answered each question correctly.\n",
        "Identify questions with low scores across models â€“ these are the hardest questions. You can highlight them or filter to the hardest 5 questions.\n",
        "You can also create a detail table showing each modelâ€™s answer (from model_output) for a given question by filtering question_id and listing model_name and model_output for context.\n",
        "Visualization Examples: You might create a dashboard with two charts â€“ one showing model accuracy comparison, and another showing a difficulty analysis of questions. Use color or annotations to highlight interesting findings (e.g., a particular model that outperforms others, or a question that stumped half the models).\n",
        "\n",
        "\n",
        "Example: A simple bar chart comparing model accuracy. In the figure above, each bar represents a modelâ€™s accuracy on the test set (e.g., GPT-4 achieved 100% on 3 questions, whereas a smaller FlanT5 model scored 66.7%). You can create similar charts in Tableau easily by dragging and dropping the accuracy (%) field for each model_name. Remember, you can use Tableauâ€™s filters to focus on specific models or questions. For instance, a filter on model_name could let you compare any subset of models (e.g., comparing only GPT-4.5-Preview vs. Gemini-2.0), and a filter on question_id could let you inspect performance on individual questions. Note: Ensure that in Tableau, boolean fields like correct are treated appropriately (Tableau might import them as text \"TRUE\"/\"FALSE\"). You may want to create a calculated field like Correct (0/1) as IF [correct] THEN 1 ELSE 0 END for easier aggregation.\n",
        "9. Future Enhancements and Conclusion\n",
        "We designed this notebook to be modular and easy to extend. Here are a few ways you could build on this framework:\n",
        "Chain-of-Thought Prompting: Modify the format_prompt function or the evaluation loop to incorporate chain-of-thought (CoT) prompts (e.g., by asking the model to \"think step by step\" before answering, and then evaluating the final answer separately). You could then evaluate not just the final answer accuracy but also analyze the reasoning steps.\n",
        "Hallucination Detection: If the questions have definitive answers, any divergence in the modelâ€™s explanation could be flagged. You might extend the detailed logs with fields for whether the modelâ€™s explanation contains factual errors (this could be manual or via another automated checker).\n",
        "Additional Metrics: We tracked token usage and latency. You could also log prompt length or output length separately, or cost if using paid APIs (by multiplying token usage by cost per token).\n",
        "More Providers: You can easily add new model providers (Cohere, AI21, etc.) in the call_model function. Just include a new elif branch and use their SDK or HTTP calls.\n",
        "Finally, a note on the evaluator.py (if you have a separate script for evaluation):\n",
        "To support token and time tracking, ensure that evaluator.py captures the start and end time around model invocations (as we did with time.time() in call_model) and returns or logs the duration.\n",
        "Modify the evaluator to also return the modelâ€™s raw output and any usage stats if available. For example, if originally it only returned correctness, have it return a dict with keys like output, correct, tokens_used, latency.\n",
        "To make it compatible with multi-model comparison, you could refactor evaluator.py to accept a model config or identifier as a parameter, so it can be called in a loop for different models (similar to how we did with evaluate_model). It could also be extended to handle a list of models internally and produce a combined report.\n",
        "By implementing these modifications, the evaluation pipeline will be more robust and informative. The modular structure of this notebook should make such changes straightforward â€“ each component (prompt formatting, model calling, result aggregation) can be adjusted independently. Conclusion: You now have a complete pipeline to evaluate multiple LLMs side-by-side on standardized questions, with results ready for analysis. Feel free to experiment with different models (just update the registry file), add more questions, or tweak the prompt strategy. Happy evaluating!\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Un_97SjRKrww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_all_files(root_dir='.'):\n",
        "    print(\"Listing all files in the current directory and subdirectories:\\n\")\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            print(os.path.join(root, file))\n",
        "\n",
        "def list_results_files(results_dir=os.path.join(\"data\", \"results\")):\n",
        "    print(\"\\nFiles in the data/results directory:\")\n",
        "    if os.path.exists(results_dir):\n",
        "        for file in os.listdir(results_dir):\n",
        "            print(file)\n",
        "    else:\n",
        "        print(\"The data/results directory does not exist.\")\n",
        "\n",
        "# Run the functions\n",
        "list_all_files()\n",
        "list_results_files()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkjH7muhT88W",
        "outputId": "4acc9334-a003-405f-becd-c4229445076d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing all files in the current directory and subdirectories:\n",
            "\n",
            "./requirements.txt\n",
            "./README.md\n",
            "./.gitignore\n",
            "./LICENSE.md\n",
            "./test_evaluator.py\n",
            "./docs/csv-wdc.html\n",
            "./src/evaluator.py\n",
            "./src/utils.py\n",
            "./src/prompt_manager.py\n",
            "./src/questions-scraper/MELIDA_PDF_Scraper_V2.ipynb\n",
            "./src/questions-scraper/MELIDA_PDF_Scraper_V1.ipynb\n",
            "./src/questions-scraper/README.md\n",
            "./src/__pycache__/evaluator.cpython-311.pyc\n",
            "./.git/packed-refs\n",
            "./.git/FETCH_HEAD\n",
            "./.git/description\n",
            "./.git/COMMIT_EDITMSG\n",
            "./.git/config\n",
            "./.git/HEAD\n",
            "./.git/index\n",
            "./.git/logs/HEAD\n",
            "./.git/logs/refs/heads/main\n",
            "./.git/logs/refs/remotes/origin/main\n",
            "./.git/logs/refs/remotes/origin/HEAD\n",
            "./.git/info/exclude\n",
            "./.git/refs/heads/main\n",
            "./.git/refs/remotes/origin/main\n",
            "./.git/refs/remotes/origin/HEAD\n",
            "./.git/hooks/update.sample\n",
            "./.git/hooks/pre-merge-commit.sample\n",
            "./.git/hooks/prepare-commit-msg.sample\n",
            "./.git/hooks/push-to-checkout.sample\n",
            "./.git/hooks/commit-msg.sample\n",
            "./.git/hooks/pre-push.sample\n",
            "./.git/hooks/pre-applypatch.sample\n",
            "./.git/hooks/pre-commit.sample\n",
            "./.git/hooks/pre-receive.sample\n",
            "./.git/hooks/fsmonitor-watchman.sample\n",
            "./.git/hooks/post-update.sample\n",
            "./.git/hooks/applypatch-msg.sample\n",
            "./.git/hooks/pre-rebase.sample\n",
            "./.git/objects/96/287bfe825a757adaf8644e0512a414e1b8106a\n",
            "./.git/objects/pack/pack-ef269f086f6d30272afdc0f8948fa519621d9ec9.idx\n",
            "./.git/objects/pack/pack-ef269f086f6d30272afdc0f8948fa519621d9ec9.pack\n",
            "./.git/objects/92/65a526a90a22032a3fd508f6bebee74d921577\n",
            "./.git/objects/92/da5e5a71b2f14219228edcd68fc752b80f0822\n",
            "./.git/objects/38/65b73ac0805b863396364fd8206d62033a0ebe\n",
            "./.git/objects/cb/807ee92b535ce700a79e290f307190c17695bc\n",
            "./notebooks/MELIDA_Evaluator_V2_0.ipynb\n",
            "./notebooks/prompting_strategy_evaluator.ipynb\n",
            "./notebooks/prod_evaluation_runner.ipynb\n",
            "./notebooks/models.yaml\n",
            "./data/exports/full_evaluation_results_20250402_221350.csv\n",
            "./data/exports/full_evaluation_results_20250401_145845.csv\n",
            "./data/exports/most_failed_questions_20250402_221351.csv\n",
            "./data/questions/MIR-2024-v01-t01.json\n",
            "./data/questions/TEST-MIR-2024-v01-t01.json\n",
            "./data/questions/MIR-2024-v01-t01_filtered.json\n",
            "./data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.json\n",
            "./data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.json\n",
            "./data/answers/MIR-2024-v01-t01-answers.json\n",
            "./data/answers/TEST-MIR-2024-v01-t01-answers.json\n",
            "./data/answers/MIR-2024-v01-t01-answers-standardized.json\n",
            "./data/answers/MIR-2024-v01-t01.json\n",
            "./data/answers/TEST-MIR-2024-v01-t01-answers-standardized.json\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.csv\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.json\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.csv\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.csv\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.json\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.csv\n",
            "./config/api_config_template.json\n",
            "./config/api_config.json\n",
            "./config/prompt_strategies.json\n",
            "\n",
            "Files in the data/results directory:\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.csv\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.json\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.csv\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.csv\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.json\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.csv\n"
          ]
        }
      ]
    }
  ]
}