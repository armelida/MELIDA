{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv9FJd8LLASxhbRWEQKeHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelida/MELIDA/blob/main/notebooks/MELIDA_Evaluator_V2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Model LLM Evaluation Notebook\n",
        "\n",
        "This notebook evaluates **multiple Large Language Models (LLMs)** on a set of standardized test questions. We will start by comparing a few models (2‚Äì3) and set up the code to **scale up to 10+ models** easily using an external registry file for model configuration. The evaluation will use a consistent prompt strategy and track performance metrics like accuracy, tokens used, and response time for each model. We‚Äôll also log detailed results per question and export the outcomes to CSV files for visualization (e.g., in Tableau).\n",
        "\n",
        "**Key features of this notebook:**\n",
        "- Uses an **external JSON/YAML file** (‚Äúmodel registry‚Äù) to define which models to evaluate and how to access them, so you can easily add/remove models without changing code.\n",
        "- Evaluates each model on a set of **standardized test questions** with a chosen prompt format (e.g., multiple-choice questions) ‚Äì currently using a zero-shot prompt asking for the best answer.\n",
        "- Records **metrics per model**: accuracy (percentage of questions answered correctly), total score (number of correct answers), number of tokens used (if available), and average response time.\n",
        "- Stores **detailed logs per question** and model, including question ID, model name, full input prompt, model‚Äôs output, whether it was correct, and latency.\n",
        "- Exports results to **CSV files** (summary and detailed) for external analysis. We‚Äôll include a guide on how to use these in Tableau to filter by model/prompt/question and create visualizations of accuracy and identify the hardest questions.\n",
        "- Modular code structure with clear comments and section headings, so you can identify and modify specific parts (e.g., to change the prompt strategy or add new evaluation features like chain-of-thought or hallucination detection in the future).\n",
        "\n"
      ],
      "metadata": {
        "id": "RFDg3bqrp5Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Check Runtime & GPU Availability\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def check_runtime():\n",
        "    \"\"\"Check whether a GPU or TPU is available.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"‚úÖ GPU is enabled! Using: {gpu_name}\")\n",
        "    elif \"COLAB_TPU_ADDR\" in os.environ:\n",
        "        print(\"‚úÖ TPU is enabled!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: No GPU or TPU detected. Running on CPU.\")\n",
        "        print(\"üëâ Go to Runtime > Change runtime type > Select GPU/TPU\")\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check GPU details using nvidia-smi if available.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è `nvidia-smi` not found. No GPU detected.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è No GPU found.\")\n",
        "\n",
        "# Run the checks\n",
        "check_runtime()\n",
        "check_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyrEgEQ-xVTk",
        "outputId": "571ea514-673e-4c67-a989-f94099a8d26b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è WARNING: No GPU or TPU detected. Running on CPU.\n",
            "üëâ Go to Runtime > Change runtime type > Select GPU/TPU\n",
            "‚ö†Ô∏è No GPU found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation (Cell 1)\n",
        "\n",
        "First, we install and import necessary libraries. This includes:\n",
        "- **Hugging Face Transformers** for local model inference (if using HuggingFace-hosted models).\n",
        "- **OpenAI/Anthropic API SDKs** (if using direct APIs like OpenAI‚Äôs GPT or Anthropic‚Äôs Claude).\n",
        "- **Together AI** client (if using the Together API for hosted models).\n",
        "- **PyYAML** (for reading YAML config) and **pandas** (for data manipulation and CSV export).\n",
        "\n",
        "We will also ensure any required API keys are set (for OpenAI, Anthropic, Together, etc.) via environment variables for security. Replace or set these environment variables before running the evaluation.\n"
      ],
      "metadata": {
        "id": "xiu37WIKrI_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fMnoEmHYo81v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf03e40-b0e3-4a54-c379-76b9f30cf653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Libraries loaded.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup environment and install required packages\n",
        "\n",
        "\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q pandas PyYAML openai anthropic together transformers\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import yaml  # For YAML parsing (PyYAML)\n",
        "\n",
        "# Import model API clients\n",
        "import openai\n",
        "# Uncomment and ensure your OpenAI API key is available via environment variables\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "import anthropic\n",
        "# Uncomment if you plan to use Anthropic API:\n",
        "# anthropic_client = anthropic.Client(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
        "\n",
        "import together\n",
        "# Uncomment if you plan to use Together API:\n",
        "# together_client = together.Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n",
        "\n",
        "# (Optional) If using Hugging Face transformers for local models:\n",
        "try:\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "except ImportError:\n",
        "    !pip install -q transformers\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Setup complete. Libraries loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configure Model Registry (Cell 2)\n",
        "We use an external model registry file (JSON or YAML) to list all models to evaluate and their access details. This file allows easy scaling to more models ‚Äì just add or remove entries without changing the notebook code. Each model entry can include:\n",
        "name: A human-readable name for the model (used in results and plots).\n",
        "provider: The method to access the model (huggingface, together, openai, anthropic, etc.).\n",
        "model_id: Identifier for the model:\n",
        "For huggingface, this is the model‚Äôs name on HuggingFace Hub (e.g., \"google/flan-t5-small\").\n",
        "For together, it might be a model ID known to the Together API (e.g., \"meta-llama/Llama-2-7b-chat-hf\").\n",
        "For openai, it could be the API model name (e.g., \"gpt-4\" or \"gpt-3.5-turbo\").\n",
        "For other providers, use the appropriate identifier.\n",
        "api_key_env (if needed): The environment variable name for the API key (e.g., \"OPENAI_API_KEY\"). This can be omitted for HuggingFace (if using local models or if no auth needed).\n",
        "Additional settings like max_tokens, temperature, etc., which define generation parameters for that model.\n",
        "Example model registry (YAML format):\n",
        "\n",
        "models:\n",
        "  - name: FlanT5 Small\n",
        "    provider: huggingface\n",
        "    model_id: google/flan-t5-small\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: GPT-4 (Jan 2025)\n",
        "    provider: openai\n",
        "    model_id: gpt-4\n",
        "    api_key_env: OPENAI_API_KEY\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: Llama2 7B Chat\n",
        "    provider: together\n",
        "    model_id: meta-llama/Llama-2-7b-chat-hf\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "\n",
        "In the code below, we load the model list from the registry file. Update model_config_path to point to your JSON/YAML file. The code will automatically detect JSON vs YAML based on file extension and parse accordingly. After loading, it prints out the model configurations to confirm."
      ],
      "metadata": {
        "id": "1_unmmk1qxdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load model registry from external JSON/YAML file\n",
        "\n",
        "# Set the path to your model registry file.\n",
        "# Since the file is placed in the same folder as your notebook, use \"models.yaml\"\n",
        "model_config_path = \"models.yaml\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(model_config_path):\n",
        "    raise FileNotFoundError(f\"Model config file not found at {model_config_path}. \"\n",
        "                            \"Please create it as per the example and update the path.\")\n",
        "\n",
        "# Parse the config file (supports YAML and JSON)\n",
        "if model_config_path.endswith((\".yaml\", \".yml\")):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = yaml.safe_load(f)\n",
        "elif model_config_path.endswith(\".json\"):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported config file format. Use .json or .yaml\")\n",
        "\n",
        "# The config is expected to either be a dict with a top-level 'models' key or a list directly.\n",
        "if isinstance(config_data, dict) and \"models\" in config_data:\n",
        "    models_config = config_data[\"models\"]\n",
        "elif isinstance(config_data, list):\n",
        "    models_config = config_data\n",
        "else:\n",
        "    raise ValueError(\"Config file format error: expected a list of models or a 'models' key.\")\n",
        "\n",
        "print(f\"Loaded {len(models_config)} models from registry:\")\n",
        "for m in models_config:\n",
        "    print(f\" - {m.get('name', 'Unnamed')} ({m.get('provider', 'Unknown')}, id={m.get('model_id', 'N/A')})\")\n"
      ],
      "metadata": {
        "id": "z8Zg9195pKkY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "a80bc9af-9375-490e-a1ef-df830bd12bdc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Model config file not found at models.yaml. Please create it as per the example and update the path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5161a1387643>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check if the file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     raise FileNotFoundError(f\"Model config file not found at {model_config_path}. \"\n\u001b[0m\u001b[1;32m     10\u001b[0m                             \"Please create it as per the example and update the path.\")\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Model config file not found at models.yaml. Please create it as per the example and update the path."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load Standardized Test Questions (Cell 3)\n",
        "Next, we prepare the standardized test questions for evaluation. These can be hard-coded, loaded from a file, or generated. In this notebook, we‚Äôll define a list of questions in a structured format (each with an ID, question text, multiple-choice options, and the correct answer). You can replace these with any set of questions relevant to your use case. For demonstration, we‚Äôll use a few simple sample questions. In a real scenario, you might load dozens of questions from a JSON/CSV file or an existing dataset. Ensure each question has a known correct answer to compute accuracy."
      ],
      "metadata": {
        "id": "qa5t7ftJrBM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Define standardized test questions\n",
        "questions = [\n",
        "    {\n",
        "        \"id\": 1,\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"choices\": [\"A) Berlin\", \"B) Paris\", \"C) Rome\", \"D) Madrid\"],\n",
        "        \"answer\": \"B\"  # Correct answer option (Paris)\n",
        "    },\n",
        "    {\n",
        "        \"id\": 2,\n",
        "        \"question\": \"Calculate 2 + 2 * 2 = ?\",\n",
        "        \"choices\": [\"A) 6\", \"B) 8\", \"C) 4\", \"D) 2\"],\n",
        "        \"answer\": \"A\"  # Correct answer is 6 (remember order of operations)\n",
        "    },\n",
        "    {\n",
        "        \"id\": 3,\n",
        "        \"question\": \"Which planet is known as the Red Planet?\",\n",
        "        \"choices\": [\"A) Earth\", \"B) Jupiter\", \"C) Mars\", \"D) Venus\"],\n",
        "        \"answer\": \"C\"  # Correct answer is Mars\n",
        "    }\n",
        "]\n",
        "print(f\"Loaded {len(questions)} questions for evaluation.\")\n"
      ],
      "metadata": {
        "id": "1ljgjq5bpKqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We defined a list of dictionaries, where each dictionary represents a question. Each question has:\n",
        "id: a unique identifier,\n",
        "question: the text of the question,\n",
        "choices: a list of answer choices (as strings, each prefixed with a letter),\n",
        "answer: the correct choice (here represented by the letter of the correct option).\n",
        "Feel free to extend or replace this list. For example, you could load questions from a file or generate them. Just ensure each question has a known correct answer for scoring."
      ],
      "metadata": {
        "id": "ycbGse8qrO6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define Prompt Strategy (Cell 4)\n",
        "The prompt strategy determines how we present each question to the model. For standardized test questions, a common strategy is to provide the question and multiple-choice options and ask the model to pick the best answer. We might instruct the model to output just the option letter (to make it easier to check correctness). In future, you could experiment with different strategies (e.g., asking for an explanation, chain-of-thought prompting, etc.). For now, we‚Äôll use a straightforward prompt: the question, the options, and a final instruction like ‚ÄúAnswer with the letter of the correct option.‚Äù We implement this as a function format_prompt(question) that takes a question entry and returns the full prompt text (or structured prompt) to send to the model. This function can be easily modified if you want to change how prompts are constructed."
      ],
      "metadata": {
        "id": "oWi8tcY7rVM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define the prompt formatting strategy\n",
        "def format_prompt(q):\n",
        "    \"\"\"\n",
        "    Format a question dictionary into a prompt string (or message) for the model.\n",
        "    \"\"\"\n",
        "    # Combine question and choices into one string\n",
        "    question_text = q[\"question\"]\n",
        "    choices_text = \" \".join(q[\"choices\"])\n",
        "    # Instruction for the model to answer with a single choice\n",
        "    prompt = f\"{question_text}\\n{choices_text}\\nAnswer with the letter of the correct option.\"\n",
        "    return prompt\n",
        "\n",
        "# Test the prompt formatting on the first question\n",
        "example_prompt = format_prompt(questions[0])\n",
        "print(\"Example formatted prompt:\\n\", example_prompt)\n"
      ],
      "metadata": {
        "id": "toS2gKl3pKtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: The format_prompt function takes a question from our list and builds a prompt. We put the question text, list all the choices (joined in one line for simplicity), and then give an explicit instruction. By asking for the letter only, we aim to have consistent outputs that are easy to check (the model hopefully will just respond with ‚ÄúB‚Äù, etc.). After defining the function, we preview an example prompt for the first question to verify the format. You can adjust this format as needed (for instance, if a model tends to do better with a different phrasing or if you want the model to explain its answer, etc.)."
      ],
      "metadata": {
        "id": "5-XAXQV_rgfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Interface and Evaluation Functions (Cells 5‚Äì6)\n",
        "In this section, we set up functions to handle model inference and evaluation:\n",
        "call_model(model_config, prompt_text) ‚Äì Invokes a single model (based on its provider and config) with the given prompt, and returns the model‚Äôs answer, along with metadata like token usage and latency.\n",
        "evaluate_model(model_config, questions) ‚Äì Uses call_model to get answers for each question from one model, checks correctness, and collects detailed results.\n",
        "We will also prepare a loop or another function to evaluate all models and aggregate the results for comparison.\n",
        "Structuring this logic into functions makes the notebook modular and easier to update. For example, if in the future we want to add a step for chain-of-thought (CoT) prompting or filter the model output for hallucinations, we could modify or wrap call_model accordingly. 5.1 call_model Implementation: This function will branch based on the provider:\n",
        "HuggingFace: use transformers pipeline or model generate. We‚Äôll initialize a pipeline for text generation or use the model‚Äôs generate method. We also tokenize the input to count input tokens. The output tokens can be counted by the tokenizer as well.\n",
        "OpenAI: use openai.Completion or openai.ChatCompletion depending on model type. For chat models (e.g., GPT-4), we pass the prompt as a user message. We retrieve the output text and usage info (token counts).\n",
        "Anthropic: (Claude models) use anthropic‚Äôs client. Typically you provide a prompt with a special format (like \"\\n\\nHuman: <question>\\n\\nAssistant:\"). We skip detailed implementation here but it can be added.\n",
        "Together: use Together API client. For example, together_client.complete or the chat completion as needed, based on their documentation. (Ensure TOGETHER_API_KEY is set.)\n",
        "Additional providers (e.g., Cohere, AI21) can be integrated similarly by adding new branches.\n",
        "We also measure the time taken for each call (latency). If token counts are not readily available from the API, we will set them to None (or you could estimate via a tokenizer). Let‚Äôs implement call_model below:"
      ],
      "metadata": {
        "id": "626XG9mwrmbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define function to call a model and get its response\n",
        "def call_model(model_cfg, prompt):\n",
        "    \"\"\"\n",
        "    Call a model with the given prompt and return its response and metadata.\n",
        "    Returns: output_text, tokens_used, latency\n",
        "    \"\"\"\n",
        "    provider = model_cfg.get(\"provider\", \"\").lower()\n",
        "    model_id = model_cfg.get(\"model_id\")\n",
        "    max_tokens = model_cfg.get(\"max_tokens\", 100)\n",
        "    temperature = model_cfg.get(\"temperature\", 0.0)\n",
        "    tokens_used = None\n",
        "    output_text = \"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    if provider == \"huggingface\":\n",
        "        # Use a Hugging Face model (local or through pipeline)\n",
        "        # Load tokenizer and model if not already loaded for this model_id\n",
        "        # (For efficiency, you could cache pipelines per model_id globally)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if \"google/flan\" in model_id or \"t5\" in model_id:\n",
        "            # If it's a seq2seq model like T5/Flan-T5, use text2text-generation pipeline\n",
        "            pipe = pipeline(\"text2text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "            result = pipe(prompt, max_length=max_tokens, temperature=temperature)\n",
        "            output_text = result[0]['generated_text']\n",
        "        else:\n",
        "            # For causal LM (e.g., GPT, Bloom, etc.), use text-generation\n",
        "            pipe = pipeline(\"text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "            result = pipe(prompt, max_length=max_tokens, temperature=temperature, do_sample=False)\n",
        "            output_text = result[0]['generated_text']\n",
        "        # Count tokens (input + output) if possible\n",
        "        try:\n",
        "            input_tokens = tokenizer(prompt, return_tensors=None)[\"input_ids\"]\n",
        "            output_tokens = tokenizer(output_text, return_tensors=None)[\"input_ids\"]\n",
        "            tokens_used = len(input_tokens) + len(output_tokens)\n",
        "        except Exception as e:\n",
        "            tokens_used = None\n",
        "\n",
        "    elif provider == \"openai\":\n",
        "        # Ensure OpenAI API key is set\n",
        "        if os.getenv(model_cfg.get(\"api_key_env\", \"OPENAI_API_KEY\")) is None:\n",
        "            raise RuntimeError(f\"OpenAI API key not set for model {model_cfg['name']}\")\n",
        "        # Determine if this is a chat or completion model\n",
        "        # Here we'll assume GPT-3.5/4 use ChatCompletion and others use Completion\n",
        "        if \"gpt-3.5\" in model_id or \"gpt-4\" in model_id or model_cfg.get(\"chat\", True):\n",
        "            # ChatCompletion API\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            response = openai.ChatCompletion.create(model=model_id, messages=messages,\n",
        "                                                    max_tokens=max_tokens, temperature=temperature)\n",
        "            output_text = response['choices'][0]['message']['content'].strip()\n",
        "            # Token usage (OpenAI provides this in the response)\n",
        "            if 'usage' in response:\n",
        "                tokens_used = response['usage'].get('total_tokens')\n",
        "        else:\n",
        "            # Completion API for models like \"text-davinci-003\"\n",
        "            response = openai.Completion.create(model=model_id, prompt=prompt,\n",
        "                                               max_tokens=max_tokens, temperature=temperature)\n",
        "            output_text = response['choices'][0]['text'].strip()\n",
        "            if 'usage' in response:\n",
        "                tokens_used = response['usage'].get('total_tokens')\n",
        "\n",
        "    elif provider == \"anthropic\":\n",
        "        # Using anthropic‚Äôs Claude API\n",
        "        if os.getenv(model_cfg.get(\"api_key_env\", \"ANTHROPIC_API_KEY\")) is None:\n",
        "            raise RuntimeError(f\"Anthropic API key not set for model {model_cfg['name']}\")\n",
        "        # Anthropic API expects the prompt in the format \"<Human>: ... <Assistant>:\"\n",
        "        # We'll craft a prompt accordingly:\n",
        "        human_prompt = f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\"\n",
        "        anthropic_client = anthropic.Client(api_key=os.getenv(model_cfg.get(\"api_key_env\", \"ANTHROPIC_API_KEY\")))\n",
        "        response = anthropic_client.completion(prompt=human_prompt, stop_sequences=[\"\\n\\nHuman:\"],\n",
        "                                              model=model_id, max_tokens_to_sample=max_tokens, temperature=temperature)\n",
        "        output_text = response.get('completion', \"\").strip()\n",
        "        # Note: Anthropic API may not directly give token count in response\n",
        "        tokens_used = None  # Could estimate if needed\n",
        "\n",
        "    elif provider == \"together\":\n",
        "        # Using Together API to call the model\n",
        "        if os.getenv(model_cfg.get(\"api_key_env\", \"TOGETHER_API_KEY\")) is None:\n",
        "            raise RuntimeError(f\"Together API key not set for model {model_cfg['name']}\")\n",
        "        # We'll use Together's Python SDK for chat or completion depending on model type\n",
        "        # For simplicity, treat all as chat here with one user message.\n",
        "        together_client = together.Together(api_key=os.getenv(model_cfg.get(\"api_key_env\", \"TOGETHER_API_KEY\")))\n",
        "        response = together_client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            stream=False  # we‚Äôll get the complete response at once\n",
        "        )\n",
        "        # The Together response might return an object; assume similar to OpenAI's structure:\n",
        "        try:\n",
        "            output_text = response.choices[0].message.content.strip()\n",
        "        except:\n",
        "            # If the Together client returns a dict similar to REST API\n",
        "            output_text = response['choices'][0]['message']['content'].strip() if isinstance(response, dict) else str(response)\n",
        "        # (Token usage might be available in response usage, but skipping for now)\n",
        "        tokens_used = None\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider} for model {model_cfg['name']}\")\n",
        "\n",
        "    latency = time.time() - start_time\n",
        "    return output_text, tokens_used, latency\n",
        "\n",
        "# For clarity, no immediate test call here because actual model calls may require connectivity or large models.\n"
      ],
      "metadata": {
        "id": "59_SJq6jrgDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In call_model:\n",
        "We take the model‚Äôs config and a prompt string.\n",
        "Based on provider, we handle the call differently.\n",
        "HuggingFace: We load the model and tokenizer (from local or HuggingFace Hub). We use pipeline for simplicity (it will handle the model loading and generation). We choose the pipeline task based on model type (a quick check for ‚Äút5‚Äù in the model name to decide between text2text-generation and text-generation). After generation, we count tokens by encoding the prompt and output with the tokenizer.\n",
        "OpenAI: We use the OpenAI API. If the model is chat-based (we guess by name containing ‚Äúgpt-3.5‚Äù or ‚Äúgpt-4‚Äù), we use the ChatCompletion endpoint with a single user message. Otherwise, we use the older Completion endpoint. We fetch the text from the response and also get token usage if provided. (Make sure your OpenAI API key is set in the environment.)\n",
        "Anthropic: We format the prompt in the required way for Claude and call the client‚Äôs completion method. (This assumes the anthropic package is installed and imported.) Token count isn‚Äôt directly captured here.\n",
        "Together: We initialize the Together client and call the chat.completions.create method with the prompt as a user message. (This assumes the model supports chat format; for pure text-generation models on Together, you might use a different method like client.completion.create.) We extract the content from the response. (Token usage may be available via Together‚Äôs response, but for simplicity, we set it to None in this example.)\n",
        "We measure the time just before and after the call to compute latency.\n",
        "Finally, we return output_text (the model‚Äôs answer), tokens_used, and latency.\n",
        "This function abstracts away the differences in model access, giving us a unified interface for the evaluation loop.\n",
        "\n",
        "5.2 evaluate_model Implementation: This function will loop through all questions for a single model, use call_model to get the answer, check correctness, and record results. It will return a list of result records (one per question for that model) and also compute summary metrics (like number correct). We‚Äôll implement evaluate_model next:"
      ],
      "metadata": {
        "id": "lzix9vk7rwpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Define function to evaluate one model on all questions\n",
        "def evaluate_model(model_cfg, questions):\n",
        "    \"\"\"\n",
        "    Evaluate a single model on all questions.\n",
        "    Returns: (results_list, num_correct)\n",
        "      - results_list is a list of dicts with detailed info per question\n",
        "      - num_correct is the count of correct answers for this model\n",
        "    \"\"\"\n",
        "    model_name = model_cfg[\"name\"]\n",
        "    results = []\n",
        "    correct_count = 0\n",
        "    print(f\"Evaluating model: {model_name}\")\n",
        "    for q in questions:\n",
        "        prompt = format_prompt(q)\n",
        "        try:\n",
        "            output, tokens, latency = call_model(model_cfg, prompt)\n",
        "        except Exception as e:\n",
        "            # Handle exceptions (e.g., API failure) gracefully\n",
        "            print(f\"  Error calling model {model_name} on question {q['id']}: {e}\")\n",
        "            output = \"\"\n",
        "            tokens = None\n",
        "            latency = None\n",
        "        # Determine if the model's answer is correct\n",
        "        # We expect the model to output a letter (like \"B\"). Sometimes it might output the word or sentence.\n",
        "        model_answer = output.strip()\n",
        "        # If the model gave a full answer like \"B) Paris\", just take the first character\n",
        "        if len(model_answer) > 0 and model_answer[0] in [\"A\",\"B\",\"C\",\"D\"]:\n",
        "            model_answer_letter = model_answer[0]\n",
        "        else:\n",
        "            # If we can't easily get a letter, mark as incorrect (or you could add more parsing logic)\n",
        "            model_answer_letter = model_answer\n",
        "        is_correct = (model_answer_letter.upper() == q[\"answer\"])\n",
        "        if is_correct:\n",
        "            correct_count += 1\n",
        "        # Record the detailed result\n",
        "        results.append({\n",
        "            \"question_id\": q[\"id\"],\n",
        "            \"model_name\": model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"model_output\": output,\n",
        "            \"correct\": is_correct,\n",
        "            \"latency\": latency,\n",
        "            \"tokens_used\": tokens\n",
        "        })\n",
        "        # Print progress for the user\n",
        "        print(f\"  Q{q['id']} -> Model answered: '{model_answer}' | Correct: {is_correct}\")\n",
        "    return results, correct_count\n"
      ],
      "metadata": {
        "id": "C3Uqr1dYpKxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In evaluate_model:\n",
        "We iterate over each question, format the prompt, and call the model via call_model.\n",
        "We wrap the model call in a try-except to catch any errors (for instance, if an API call fails or a model isn‚Äôt available). If there‚Äôs an error, we log it and move on, leaving output empty for that question.\n",
        "We then parse the model‚Äôs output to extract the answer. We assume the model should reply with a letter. The code checks the first character of the output: if it‚Äôs one of ‚ÄúA, B, C, D‚Äù, we treat that as the chosen option. (If the output is something else, you could include additional parsing logic ‚Äì for example, sometimes the model might output the full option text or a sentence. Here, we simplify by taking the first letter when possible. If the output is empty or doesn‚Äôt start with a letter, we mark the answer as incorrect by default.)\n",
        "We compare the model‚Äôs answer letter (uppercased) to the true answer letter from the question. If they match, it‚Äôs correct and we increment correct_count.\n",
        "We append a dictionary to results containing all relevant info: question ID, model name, the exact prompt used, the model‚Äôs raw output, a boolean for correctness, latency (in seconds), and tokens used.\n",
        "We also print a one-line progress update for each question, indicating what the model answered and whether it was correct. This helps to monitor the evaluation as it happens, especially if many questions are being tested.\n",
        "Finally, the function returns the list of results and the count of correct answers.\n",
        "With these functions in place, we can now evaluate all models and compile the metrics.\n",
        "6. Run Evaluation for All Models (Cell 7)\n",
        "Now we‚Äôll loop through each model in our models_config, evaluate it on all questions using evaluate_model, and collect the outcomes. We will calculate summary metrics for each model:\n",
        "Accuracy (% correct)\n",
        "Total score (number of correct answers out of total questions)\n",
        "Total tokens used (if available; this could be sum of tokens across all questions for that model)\n",
        "Average response time per question (latency)\n",
        "We‚Äôll store summary results in a list of dictionaries (which we can later convert to a DataFrame for display or CSV export). We‚Äôll also accumulate all per-question results into a single list for detailed logging."
      ],
      "metadata": {
        "id": "v-QBGGZIr927"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Evaluate all models and gather results\n",
        "all_details = []   # list to collect detailed results for every model and question\n",
        "summary_records = []  # list to collect summary metrics for each model\n",
        "\n",
        "total_questions = len(questions)\n",
        "for model_cfg in models_config:\n",
        "    model_name = model_cfg[\"name\"]\n",
        "    results, num_correct = evaluate_model(model_cfg, questions)\n",
        "    all_details.extend(results)\n",
        "    # Calculate summary metrics\n",
        "    accuracy = (num_correct / total_questions) * 100  # percentage\n",
        "    # Sum tokens and average latency for this model\n",
        "    tokens_list = [r[\"tokens_used\"] for r in results if r[\"tokens_used\"] is not None]\n",
        "    total_tokens = sum(tokens_list) if tokens_list else None\n",
        "    latency_list = [r[\"latency\"] for r in results if r[\"latency\"] is not None]\n",
        "    avg_latency = (sum(latency_list) / len(latency_list)) if latency_list else None\n",
        "    summary_records.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"accuracy (%)\": round(accuracy, 2),\n",
        "        \"total_score\": num_correct,\n",
        "        \"total_questions\": total_questions,\n",
        "        \"tokens_used_total\": total_tokens,\n",
        "        \"avg_latency_sec\": round(avg_latency, 2) if avg_latency is not None else None\n",
        "    })\n",
        "    print(f\"Finished {model_name}: {num_correct}/{total_questions} correct, Accuracy {accuracy:.1f}%.\")\n"
      ],
      "metadata": {
        "id": "k0Pbpuf6r9dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In Cell 7:\n",
        "We initialize all_details to gather every question‚Äôs result and summary_records for each model.\n",
        "We loop over each model configuration:\n",
        "* Call evaluate_model for that model, which returns the detailed results and count of correct answers.\n",
        "* We extend the all_details list with the results (so in the end, this list contains an entry for each model-question pair).\n",
        "* Compute accuracy as (num_correct / total_questions) * 100. We round it to two decimal places later for neatness.\n",
        "* Compute total tokens used by summing the tokens_used for each question result, if available. If none of the results have token info (i.e., the list is empty because maybe the API didn‚Äôt provide it), we leave total_tokens as None.\n",
        "* Compute average latency by summing all latencies and dividing by number of questions (we exclude any None latencies just in case).\n",
        "* Append a dictionary to summary_records with the model‚Äôs name and metrics. We include total questions for reference, and round the accuracy and average latency for readability.\n",
        "* Print a summary line for each model (e.g., ‚ÄúFinished ModelX: 8/10 correct, Accuracy 80.0%.‚Äù).\n",
        "After this loop, we have:\n",
        "* summary_records: a list of summary info for each model.\n",
        "* all_details: a list of per-question info, which we can turn into a detailed log.\n",
        "Next, we‚Äôll convert these to pandas DataFrames for easy viewing and export."
      ],
      "metadata": {
        "id": "_-mQLra8sJBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert summary and details to pandas DataFrames for display and export\n",
        "summary_df = pd.DataFrame(summary_records)\n",
        "details_df = pd.DataFrame(all_details)\n",
        "\n",
        "print(\"\\nSummary of results for each model:\")\n",
        "display(summary_df)\n",
        "\n",
        "print(\"\\nDetailed results (first few rows):\")\n",
        "display(details_df.head(10))\n"
      ],
      "metadata": {
        "id": "vfFmrWdbsHsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We create two DataFrames:\n",
        "summary_df with one row per model, containing accuracy, scores, etc.\n",
        "details_df with one row per question per model, containing everything from question ID to correctness.\n",
        "We then display the summary and the first few detailed results to verify the content. (In a real Jupyter environment, display(df) will show a nice table. In a text environment or script, you might use print(df.to_string()) or df.head().) Review the summary to ensure metrics make sense, and review the details to spot-check that outputs and correctness are recorded as expected."
      ],
      "metadata": {
        "id": "3JqybaU1tKR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Export Results to CSV (Cell 8)\n",
        "Now that we have the results in DataFrames, we‚Äôll export them to CSV files, which can be used in external analysis tools like Excel or Tableau. We will create two CSV files:\n",
        "llm_eval_summary.csv ‚Äì containing the summary metrics per model.\n",
        "llm_eval_details.csv ‚Äì containing the detailed per-question results.\n",
        "These files will include headers and can be imported directly into Tableau or other tools."
      ],
      "metadata": {
        "id": "rdDMq8WKsMLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Export the summary and detailed results to CSV files\n",
        "summary_csv_path = \"llm_eval_summary.csv\"\n",
        "details_csv_path = \"llm_eval_details.csv\"\n",
        "summary_df.to_csv(summary_csv_path, index=False)\n",
        "details_df.to_csv(details_csv_path, index=False)\n",
        "print(f\"Exported summary results to {summary_csv_path}\")\n",
        "print(f\"Exported detailed results to {details_csv_path}\")\n"
      ],
      "metadata": {
        "id": "djYouS4otOoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this, you should find two CSV files in your working directory:\n",
        "llm_eval_summary.csv ‚Äì with columns like model_name, accuracy (%), total_score, total_questions, tokens_used_total, avg_latency_sec.\n",
        "llm_eval_details.csv ‚Äì with columns like question_id, model_name, prompt, model_output, correct, latency, tokens_used.\n",
        "These can now be loaded into Tableau or any data analysis software for visualization."
      ],
      "metadata": {
        "id": "EW6I9m2ktTj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "8. Using Tableau for Analysis of Results\n",
        "With the results exported, we can analyze and visualize the performance of the models. Below are step-by-step instructions to use Tableau (or a similar data visualization tool) to explore the data:\n",
        "Import the CSV files into Tableau: Open Tableau and connect to the llm_eval_summary.csv and llm_eval_details.csv files (you can import them separately or join/relate them on the model_name field if needed).\n",
        "Summary Dashboard (Model-Level): Using llm_eval_summary.csv, you can create a simple chart of model performance. For example:\n",
        "Create a bar chart with model_name on the x-axis and accuracy (%) on the y-axis to compare accuracy across models.\n",
        "Add labels to show the exact accuracy or score for each model bar.\n",
        "You could also include avg_latency_sec as a secondary metric (perhaps a separate chart or as a tooltip) to see the speed-accuracy tradeoff.\n",
        "Filter by Prompt or Question: Since all models used the same prompt strategy in this run, the summary is straightforward. If you had different prompt strategies or sets of questions, you could use filters. For instance, if prompt_strategy was a field, you could filter or color-code by it. Or using the detailed data, you could filter to a specific question to see all model answers for that one.\n",
        "Detailed Analysis (Question-Level): Using llm_eval_details.csv, you can analyze which questions were hardest:\n",
        "Create a view with question_id on one axis and perhaps the count of models that got it correct.\n",
        "For example, drag question_id to rows, and an aggregation of correct (treat correct as 0/1 values and take average or sum). Multiply by 100 to interpret as percentage of models correct. This will tell you the percent of models that answered each question correctly.\n",
        "Identify questions with low scores across models ‚Äì these are the hardest questions. You can highlight them or filter to the hardest 5 questions.\n",
        "You can also create a detail table showing each model‚Äôs answer (from model_output) for a given question by filtering question_id and listing model_name and model_output for context.\n",
        "Visualization Examples: You might create a dashboard with two charts ‚Äì one showing model accuracy comparison, and another showing a difficulty analysis of questions. Use color or annotations to highlight interesting findings (e.g., a particular model that outperforms others, or a question that stumped half the models).\n",
        "\n",
        "\n",
        "Example: A simple bar chart comparing model accuracy. In the figure above, each bar represents a model‚Äôs accuracy on the test set (e.g., GPT-4 achieved 100% on 3 questions, whereas a smaller FlanT5 model scored 66.7%). You can create similar charts in Tableau easily by dragging and dropping the accuracy (%) field for each model_name. Remember, you can use Tableau‚Äôs filters to focus on specific models or questions. For instance, a filter on model_name could let you compare any subset of models (e.g., comparing only GPT-4.5-Preview vs. Gemini-2.0), and a filter on question_id could let you inspect performance on individual questions. Note: Ensure that in Tableau, boolean fields like correct are treated appropriately (Tableau might import them as text \"TRUE\"/\"FALSE\"). You may want to create a calculated field like Correct (0/1) as IF [correct] THEN 1 ELSE 0 END for easier aggregation.\n",
        "9. Future Enhancements and Conclusion\n",
        "We designed this notebook to be modular and easy to extend. Here are a few ways you could build on this framework:\n",
        "Chain-of-Thought Prompting: Modify the format_prompt function or the evaluation loop to incorporate chain-of-thought (CoT) prompts (e.g., by asking the model to \"think step by step\" before answering, and then evaluating the final answer separately). You could then evaluate not just the final answer accuracy but also analyze the reasoning steps.\n",
        "Hallucination Detection: If the questions have definitive answers, any divergence in the model‚Äôs explanation could be flagged. You might extend the detailed logs with fields for whether the model‚Äôs explanation contains factual errors (this could be manual or via another automated checker).\n",
        "Additional Metrics: We tracked token usage and latency. You could also log prompt length or output length separately, or cost if using paid APIs (by multiplying token usage by cost per token).\n",
        "More Providers: You can easily add new model providers (Cohere, AI21, etc.) in the call_model function. Just include a new elif branch and use their SDK or HTTP calls.\n",
        "Finally, a note on the evaluator.py (if you have a separate script for evaluation):\n",
        "To support token and time tracking, ensure that evaluator.py captures the start and end time around model invocations (as we did with time.time() in call_model) and returns or logs the duration.\n",
        "Modify the evaluator to also return the model‚Äôs raw output and any usage stats if available. For example, if originally it only returned correctness, have it return a dict with keys like output, correct, tokens_used, latency.\n",
        "To make it compatible with multi-model comparison, you could refactor evaluator.py to accept a model config or identifier as a parameter, so it can be called in a loop for different models (similar to how we did with evaluate_model). It could also be extended to handle a list of models internally and produce a combined report.\n",
        "By implementing these modifications, the evaluation pipeline will be more robust and informative. The modular structure of this notebook should make such changes straightforward ‚Äì each component (prompt formatting, model calling, result aggregation) can be adjusted independently. Conclusion: You now have a complete pipeline to evaluate multiple LLMs side-by-side on standardized questions, with results ready for analysis. Feel free to experiment with different models (just update the registry file), add more questions, or tweak the prompt strategy. Happy evaluating!\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "You said:\n",
        "Can you review this? and suggest improvements? http://github.com/armelida/MELIDA/\n",
        "ChatGPT said:\n",
        "‚ÄãI have reviewed the MELIDA (Model Evaluation for Life-sciences Intelligence and Decision Assistance) repository. This project aims to evaluate AI language models using standardized Spanish medical exam (MIR) questions to assess their medical knowledge and reasoning capabilities. The repository is well-structured, with directories for configuration (config), data (data), documentation (docs), notebooks (notebooks), and source code (src). The README.md provides a concise overview of the project's purpose and features.‚Äã\n",
        "\n",
        "Suggestions for Improvement:\n",
        "\n",
        "Detailed Documentation:\n",
        "\n",
        "Expand the README: Include sections on installation, setup, and usage instructions. Providing examples of how to run evaluations and interpret results would be beneficial for new users.‚Äã\n",
        "Docstrings and Comments: Ensure that all functions and classes in the source code have comprehensive docstrings explaining their purpose, parameters, and return values. Inline comments can further clarify complex code segments.‚Äã\n",
        "Configuration Management:\n",
        "\n",
        "Centralized Configuration: Utilize configuration files (e.g., YAML or JSON) to manage parameters like model names, API keys, and evaluation settings. This approach enhances flexibility and simplifies adjustments without modifying the codebase.‚Äã\n",
        "Modularization:\n",
        "\n",
        "Refactor Monolithic Scripts: Break down large scripts into smaller, reusable modules. For instance, separate data loading, preprocessing, evaluation, and analysis into distinct functions or classes. This modularity improves code maintainability and readability.‚Äã\n",
        "Logging and Error Handling:\n",
        "\n",
        "Implement Logging: Incorporate Python's logging module to record events, errors, and warnings. This practice aids in debugging and provides insights during the evaluation process.‚Äã\n",
        "Robust Error Handling: Add try-except blocks where appropriate to handle potential exceptions gracefully, ensuring that the program can recover or provide informative error messages.‚Äã\n",
        "Testing:\n",
        "\n",
        "Unit and Integration Tests: Develop a comprehensive test suite using frameworks like unittest or pytest to validate the functionality of individual components and their integration. Automated tests ensure code reliability and facilitate future modifications.‚Äã\n",
        "Continuous Integration (CI):\n",
        "\n",
        "Set Up CI Pipeline: Integrate CI tools (e.g., GitHub Actions, Travis CI) to automatically run tests, linting, and other checks on code commits and pull requests. CI enhances code quality and streamlines collaboration.‚Äã\n",
        "Performance Optimization:\n",
        "\n",
        "Efficient Data Handling: Optimize data loading and processing to handle large datasets effectively. Consider using libraries like pandas for data manipulation and numpy for numerical operations.‚Äã\n",
        "Parallel Processing: If evaluations are time-consuming, implement parallel processing or asynchronous calls to speed up the workflow. Libraries such as multiprocessing or asyncio can be useful in this context.‚Äã\n",
        "Visualization and Reporting:\n",
        "\n",
        "Enhanced Visualizations: Utilize libraries like matplotlib or seaborn to create insightful plots that convey model performance metrics effectively.‚Äã\n",
        "Automated Reports: Generate automated reports (e.g., HTML or PDF) summarizing evaluation results, including tables and visualizations, to facilitate analysis and sharing.‚Äã\n",
        "User Interaction:\n",
        "\n",
        "Command-Line Interface (CLI): Develop a CLI using libraries like argparse or click to allow users to run evaluations with different parameters easily.‚Äã\n",
        "Web Interface: Consider building a simple web interface using frameworks like Flask or Django to make the tool more accessible to non-technical users.‚Äã\n",
        "Community Engagement:\n",
        "\n",
        "Contribution Guidelines: Provide guidelines on how others can contribute to the project, including coding standards, branch management, and the process for submitting pull requests.\n",
        "Issue Tracking: Utilize GitHub's issue tracker to document bugs, feature requests, and enhancements. Engaging with the community can lead to valuable feedback and collaborative improvements.\n",
        "By implementing these suggestions, the MELIDA project can enhance its functionality, usability, and maintainability, making it a more robust tool for evaluating AI language models in the medical domain."
      ],
      "metadata": {
        "id": "vDNmxetdtS3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}