{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOIAwEAxKoXvI7DZIiqc0Ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelida/MELIDA/blob/main/notebooks/MELIDA_Evaluator_V2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Model LLM Evaluation Notebook\n",
        "\n",
        "This notebook evaluates **multiple Large Language Models (LLMs)** on a set of standardized test questions. We will start by comparing a few models (2‚Äì3) and set up the code to **scale up to 10+ models** easily using an external registry file for model configuration. The evaluation will use a consistent prompt strategy and track performance metrics like accuracy, tokens used, and response time for each model. We‚Äôll also log detailed results per question and export the outcomes to CSV files for visualization (e.g., in Tableau).\n",
        "\n",
        "**Key features of this notebook:**\n",
        "- Uses an **external JSON/YAML file** (‚Äúmodel registry‚Äù) to define which models to evaluate and how to access them, so you can easily add/remove models without changing code.\n",
        "- Evaluates each model on a set of **standardized test questions** with a chosen prompt format (e.g., multiple-choice questions) ‚Äì currently using a zero-shot prompt asking for the best answer.\n",
        "- Records **metrics per model**: accuracy (percentage of questions answered correctly), total score (number of correct answers), number of tokens used (if available), and average response time.\n",
        "- Stores **detailed logs per question** and model, including question ID, model name, full input prompt, model‚Äôs output, whether it was correct, and latency.\n",
        "- Exports results to **CSV files** (summary and detailed) for external analysis. We‚Äôll include a guide on how to use these in Tableau to filter by model/prompt/question and create visualizations of accuracy and identify the hardest questions.\n",
        "- Modular code structure with clear comments and section headings, so you can identify and modify specific parts (e.g., to change the prompt strategy or add new evaluation features like chain-of-thought or hallucination detection in the future).\n",
        "\n"
      ],
      "metadata": {
        "id": "RFDg3bqrp5Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Cell:\n",
        "# Check Runtime & GPU Availability\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def check_runtime():\n",
        "    \"\"\"Check whether a GPU or TPU is available.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"‚úÖ GPU is enabled! Using: {gpu_name}\")\n",
        "    elif \"COLAB_TPU_ADDR\" in os.environ:\n",
        "        print(\"‚úÖ TPU is enabled!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: No GPU or TPU detected. Running on CPU.\")\n",
        "        print(\"üëâ Go to Runtime > Change runtime type > Select GPU/TPU\")\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check GPU details using nvidia-smi if available.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è `nvidia-smi` not found. No GPU detected.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è No GPU found.\")\n",
        "\n",
        "# Run the checks\n",
        "check_runtime()\n",
        "check_gpu()\n",
        "\n",
        "#  Clone repository and change working directory\n",
        "!rm -rf MELIDA  # Remove any existing copy (optional)\n",
        "!git clone https://github.com/armelida/MELIDA.git\n",
        "%cd MELIDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyrEgEQ-xVTk",
        "outputId": "bb2394be-cc20-4c8c-ebc2-990c250c723c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è WARNING: No GPU or TPU detected. Running on CPU.\n",
            "üëâ Go to Runtime > Change runtime type > Select GPU/TPU\n",
            "‚ö†Ô∏è No GPU found.\n",
            "Cloning into 'MELIDA'...\n",
            "remote: Enumerating objects: 509, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 509 (delta 11), reused 16 (delta 4), pack-reused 480 (from 1)\u001b[K\n",
            "Receiving objects: 100% (509/509), 1.37 MiB | 17.52 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n",
            "/content/MELIDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0A: Load API Keys & Save API Configuration\n",
        "\n",
        "# Install necessary libraries if not already present\n",
        "!pip install -q python-dotenv google-generativeai # Added google-generativeai\n",
        "# Keep openai pinned if needed, but google-generativeai is separate\n",
        "!pip install openai==0.28\n",
        "\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Initialize API keys dictionary - Added 'google'\n",
        "api_keys = {\"openai\": None, \"anthropic\": None, \"together\": None, \"google\": None}\n",
        "\n",
        "# Try to load from Colab secrets using userdata - Added 'google'\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_keys[\"openai\"] = userdata.get('OPENAI_API_KEY')\n",
        "    api_keys[\"anthropic\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "    api_keys[\"together\"] = userdata.get('TOGETHER_API_KEY')\n",
        "    # --- Add Google API Key ---\n",
        "    api_keys[\"google\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    # ------------------------\n",
        "    # Update check condition\n",
        "    if all(v is not None for k, v in api_keys.items() if k != 'google'): # Check others first\n",
        "         print(\"‚úì API keys (OpenAI, Anthropic, Together) loaded from Colab secrets\")\n",
        "    if api_keys[\"google\"]:\n",
        "         print(\"‚úì Google API key loaded from Colab secrets\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Note: Couldn't load some keys from Colab secrets - {e}\")\n",
        "\n",
        "# Fallback: load from environment variables if not loaded yet - Added 'google'\n",
        "if not all(api_keys.values()): # Check if *any* are missing\n",
        "    api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "    # --- Add Google API Key ---\n",
        "    api_keys[\"google\"] = api_keys[\"google\"] or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    # ------------------------\n",
        "    # Update log message if needed\n",
        "    loaded_from_env = [k for k, v in api_keys.items() if v and not userdata.get(k.upper()+'_API_KEY')] # Rough check\n",
        "    if loaded_from_env:\n",
        "        print(f\"‚úì API keys loaded from environment variables for: {', '.join(loaded_from_env)}\")\n",
        "\n",
        "\n",
        "# Fallback: load from a .env file if still missing - Added 'google'\n",
        "if not all(api_keys.values()):\n",
        "    try:\n",
        "        load_dotenv() # This will load variables from a .env file\n",
        "        api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "        api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "        # --- Add Google API Key ---\n",
        "        api_keys[\"google\"] = api_keys[\"google\"] or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "        # ------------------------\n",
        "        # Update log message if needed\n",
        "        loaded_from_dotenv = [k for k, v in api_keys.items() if v and not userdata.get(k.upper()+'_API_KEY') and k not in loaded_from_env] # Rough check\n",
        "        if loaded_from_dotenv:\n",
        "             print(f\"‚úì API keys loaded from .env file for: {', '.join(loaded_from_dotenv)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Couldn't load from .env file - {e}\")\n",
        "\n",
        "# Propagate keys to os.environ so subsequent cells can access them - Added 'google'\n",
        "if api_keys[\"openai\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_keys[\"openai\"]\n",
        "if api_keys[\"anthropic\"]:\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_keys[\"anthropic\"]\n",
        "if api_keys[\"together\"]:\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = api_keys[\"together\"]\n",
        "# --- Add Google API Key ---\n",
        "if api_keys[\"google\"]:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_keys[\"google\"]\n",
        "# ------------------------\n",
        "\n",
        "# Save API configuration to a JSON file for future reference\n",
        "os.makedirs('config', exist_ok=True)\n",
        "api_config = {\n",
        "    \"openai\": {\"api_key\": api_keys[\"openai\"] or \"YOUR_OPENAI_API_KEY_HERE\"},\n",
        "    \"anthropic\": {\"api_key\": api_keys[\"anthropic\"] or \"YOUR_ANTHROPIC_API_KEY_HERE\"},\n",
        "    \"together\": {\"api_key\": api_keys[\"together\"] or \"YOUR_TOGETHER_API_KEY_HERE\"},\n",
        "    \"google\": {\"api_key\": api_keys[\"google\"] or \"YOUR_GOOGLE_API_KEY_HERE\"}\n",
        "}\n",
        "with open('config/api_config.json', 'w') as f:\n",
        "    json.dump(api_config, f, indent=2)\n",
        "\n",
        "# Report missing keys, if any\n",
        "missing = []\n",
        "if not api_keys[\"openai\"]: missing.append(\"OpenAI\")\n",
        "if not api_keys[\"anthropic\"]: missing.append(\"Anthropic\")\n",
        "if not api_keys[\"together\"]: missing.append(\"Together\")\n",
        "if not api_keys[\"google\"]: missing.append(\"Google\")\n",
        "if missing:\n",
        "    print(f\"‚ö†Ô∏è Missing API keys: {', '.join(missing)}\")\n",
        "    print(\"üëâ Please set the necessary API keys using Colab secrets (recommended), environment variables, or a .env file.\")\n",
        "else:\n",
        "    print(\"‚úì All required API configurations saved/loaded.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5_dSkv97Efr",
        "outputId": "4ff64987-c467-42e3-d1bb-48195341f2b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "‚úì API keys (OpenAI, Anthropic, Together) loaded from Colab secrets\n",
            "‚úì Google API key loaded from Colab secrets\n",
            "‚úì All required API configurations saved/loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation (Cell 1)\n",
        "\n",
        "First, we install and import necessary libraries. This includes:\n",
        "- **Hugging Face Transformers** for local model inference (if using HuggingFace-hosted models).\n",
        "- **OpenAI/Anthropic API SDKs** (if using direct APIs like OpenAI‚Äôs GPT or Anthropic‚Äôs Claude).\n",
        "- **Together AI** client (if using the Together API for hosted models).\n",
        "- **PyYAML** (for reading YAML config) and **pandas** (for data manipulation and CSV export).\n",
        "\n",
        "We will also ensure any required API keys are set (for OpenAI, Anthropic, Together, etc.) via environment variables for security. Replace or set these environment variables before running the evaluation.\n"
      ],
      "metadata": {
        "id": "xiu37WIKrI_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fMnoEmHYo81v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715d87a0-67e1-4322-a367-8f1a4a207edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Libraries loaded.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup environment and install required packages\n",
        "\n",
        "# Install required packages - Added google-generativeai\n",
        "!pip install -q pandas PyYAML openai anthropic together transformers google-generativeai\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# Import model API clients\n",
        "import openai\n",
        "# Ensure your OpenAI API key is available (loaded in Cell 0A)\n",
        "\n",
        "import anthropic\n",
        "# Anthropic client might be initialized later in call_model or globally here if preferred\n",
        "\n",
        "import together\n",
        "# Together client might be initialized later in call_model or globally here if preferred\n",
        "\n",
        "# --- Add Google Generative AI Import ---\n",
        "import google.generativeai as genai\n",
        "# --------------------------------------\n",
        "\n",
        "# (Optional) If using Hugging Face transformers for local models:\n",
        "try:\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "except ImportError:\n",
        "    !pip install -q transformers # Ensure transformers is installed if needed\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Setup complete. Libraries loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configure Model Registry (Cell 2)\n",
        "We use an external model registry file (JSON or YAML) to list all models to evaluate and their access details. This file allows easy scaling to more models ‚Äì just add or remove entries without changing the notebook code. Each model entry can include:\n",
        "name: A human-readable name for the model (used in results and plots).\n",
        "provider: The method to access the model (huggingface, together, openai, anthropic, etc.).\n",
        "model_id: Identifier for the model:\n",
        "For huggingface, this is the model‚Äôs name on HuggingFace Hub (e.g., \"google/flan-t5-small\").\n",
        "For together, it might be a model ID known to the Together API (e.g., \"meta-llama/Llama-2-7b-chat-hf\").\n",
        "For openai, it could be the API model name (e.g., \"gpt-4\" or \"gpt-3.5-turbo\").\n",
        "For other providers, use the appropriate identifier.\n",
        "api_key_env (if needed): The environment variable name for the API key (e.g., \"OPENAI_API_KEY\"). This can be omitted for HuggingFace (if using local models or if no auth needed).\n",
        "Additional settings like max_tokens, temperature, etc., which define generation parameters for that model.\n",
        "Example model registry (YAML format):\n",
        "\n",
        "models:\n",
        "  - name: FlanT5 Small\n",
        "    provider: huggingface\n",
        "    model_id: google/flan-t5-small\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: GPT-4 (Jan 2025)\n",
        "    provider: openai\n",
        "    model_id: gpt-4\n",
        "    api_key_env: OPENAI_API_KEY\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: Llama2 7B Chat\n",
        "    provider: together\n",
        "    model_id: meta-llama/Llama-2-7b-chat-hf\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "\n",
        "In the code below, we load the model list from the registry file. Update model_config_path to point to your JSON/YAML file. The code will automatically detect JSON vs YAML based on file extension and parse accordingly. After loading, it prints out the model configurations to confirm."
      ],
      "metadata": {
        "id": "1_unmmk1qxdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load model registry from external JSON/YAML file\n",
        "\n",
        "# Set the path to your model registry file in the notebooks folder\n",
        "model_config_path = \"notebooks/models.yaml\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(model_config_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Model config file not found at {model_config_path}. \"\n",
        "        \"Please create it as per the example and update the path.\"\n",
        "    )\n",
        "\n",
        "# Parse the config file (supports YAML and JSON)\n",
        "if model_config_path.endswith((\".yaml\", \".yml\")):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = yaml.safe_load(f)\n",
        "elif model_config_path.endswith(\".json\"):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported config file format. Use .json or .yaml\")\n",
        "\n",
        "# The config should either be a dict with a top-level 'models' key or a list itself.\n",
        "if isinstance(config_data, dict) and \"models\" in config_data:\n",
        "    models_config = config_data[\"models\"]\n",
        "elif isinstance(config_data, list):\n",
        "    models_config = config_data\n",
        "else:\n",
        "    raise ValueError(\"Config file format error: expected a list of models or a 'models' key.\")\n",
        "\n",
        "print(f\"Loaded {len(models_config)} models from registry:\")\n",
        "for m in models_config:\n",
        "    print(f\" - {m.get('name', 'Unnamed')} ({m.get('provider', 'Unknown')}, id={m.get('model_id', 'N/A')})\")\n"
      ],
      "metadata": {
        "id": "z8Zg9195pKkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37c2c6e-f9d1-4186-ee7b-3b2d6a9cf050"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3 models from registry:\n",
            " - o3-mini-2025-01-31 (OpenAI, id=o3-mini-2025-01-31)\n",
            " - Claude (Anthropic, id=claude-3-7-sonnet-20250219)\n",
            " - Together (Together, id=deepseek-ai/DeepSeek-R1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load Standardized Test Questions (Cell 3)\n",
        "Next, we prepare the standardized test questions for evaluation. These can be hard-coded, loaded from a file, or generated. In this notebook, we‚Äôll define a list of questions in a structured format (each with an ID, question text, multiple-choice options, and the correct answer). You can replace these with any set of questions relevant to your use case. For demonstration, we‚Äôll use a few simple sample questions. In a real scenario, you might load dozens of questions from a JSON/CSV file or an existing dataset. Ensure each question has a known correct answer to compute accuracy."
      ],
      "metadata": {
        "id": "qa5t7ftJrBM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load extracted exam questions for evaluation\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Define the path to the exported questions file.\n",
        "# Adjust this path if your exported file name or location is different.\n",
        "questions_file = \"data/questions/MIR-2024-v01-t01.json\"\n",
        "\n",
        "if not os.path.exists(questions_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Questions file not found at {questions_file}. \"\n",
        "        \"Please run the extraction process to generate the questions file.\"\n",
        "    )\n",
        "\n",
        "with open(questions_file, 'r', encoding='utf-8') as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(questions)} questions for evaluation.\")\n",
        "\n",
        "# Optionally, preview the first three questions\n",
        "for q in questions[:3]:\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print(f\"ID: {q['id']}\")\n",
        "    print(f\"Question: {q['question_text']}\")\n",
        "    print(\"Options:\")\n",
        "    for key, value in q['options'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"---------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "1ljgjq5bpKqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922aa85d-5254-4568-c730-61b760a4df28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 185 questions for evaluation.\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q026\n",
            "Question: Entre los cambios metab√≥licos que se observan en un paciente con resistencia a insulina existe:\n",
            "Options:\n",
            "  A: Incremento de la expresi√≥n hep√°tica de genes gluconeog√©nicos mediado por FOXO1 (forkhead box other) fosforilado.\n",
            "  B: Descenso en los niveles intracelulares de hexoquinasa 2 dependiente de insulina.\n",
            "  C: Aumento de la glucogen√≥lisis muscular, contribuyendo al incremento de la glucemia.\n",
            "  D: Aumento en los niveles s√©ricos de amino√°cidos como leucina e isoleucina.\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q027\n",
            "Question: La deficiencia de acil-CoA-deshidrogenasa provoca una de las siguientes alteraciones bioqu√≠micas:\n",
            "Options:\n",
            "  A: Disminuci√≥n de √°cidos dicarbox√≠licos.\n",
            "  B: Aumento de la gluconeog√©nesis.\n",
            "  C: Disminuci√≥n de la ureag√©nesis.\n",
            "  D: Aumento de carnitina libre.\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q028\n",
            "Question: Respecto a la t√©cnica de exploraci√≥n de la motilidad pupilar y sus reflejos indique la respuesta INCORRECTA:\n",
            "Options:\n",
            "  A: El reflejo de la visi√≥n pr√≥xima se explora con una linterna que se enfoca sobre las pupilas de forma alternativa.\n",
            "  B: El signo pupilar de Marcus Gunn o defecto pupilar aferente relativo est√° presente en enfermedades del nervio √≥ptico y enfermedades retinianas no extensas.\n",
            "  C: El reflejo fotomotor consensuado est√° afectado cuando el da√±o implica la v√≠a eferente vehiculada por el nervio motor ocular com√∫n.\n",
            "  D: La anisocoria que aumenta en condiciones de oscuridad mostrando una pupila m√°s mi√≥tica se debe a que hay un trastorno en el sistema simp√°tico.\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We defined a list of dictionaries, where each dictionary represents a question. Each question has:\n",
        "id: a unique identifier,\n",
        "question: the text of the question,\n",
        "choices: a list of answer choices (as strings, each prefixed with a letter),\n",
        "answer: the correct choice (here represented by the letter of the correct option).\n",
        "Feel free to extend or replace this list. For example, you could load questions from a file or generate them. Just ensure each question has a known correct answer for scoring."
      ],
      "metadata": {
        "id": "ycbGse8qrO6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define Prompt Strategy (Cell 4)\n",
        "The prompt strategy determines how we present each question to the model. For standardized test questions, a common strategy is to provide the question and multiple-choice options and ask the model to pick the best answer. We might instruct the model to output just the option letter (to make it easier to check correctness). In future, you could experiment with different strategies (e.g., asking for an explanation, chain-of-thought prompting, etc.). For now, we‚Äôll use a straightforward prompt: the question, the options, and a final instruction like ‚ÄúAnswer with the letter of the correct option.‚Äù We implement this as a function format_prompt(question) that takes a question entry and returns the full prompt text (or structured prompt) to send to the model. This function can be easily modified if you want to change how prompts are constructed."
      ],
      "metadata": {
        "id": "oWi8tcY7rVM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define the prompt formatting strategy\n",
        "\n",
        "def format_prompt(q):\n",
        "    \"\"\"\n",
        "    Format a question dictionary into a prompt string for the MIR exam in Spanish.\n",
        "\n",
        "    The prompt instructs the model to answer with a single letter (A, B, C, D) if it knows the answer,\n",
        "    or with N if unsure.\n",
        "    \"\"\"\n",
        "    # Get the main question text\n",
        "    question_text = q.get(\"question_text\", \"\")\n",
        "\n",
        "    # Get each answer option from the options dictionary (default to empty string if missing)\n",
        "    options = q.get(\"options\", {})\n",
        "    option_a = options.get(\"A\", \"\")\n",
        "    option_b = options.get(\"B\", \"\")\n",
        "    option_c = options.get(\"C\", \"\")\n",
        "    option_d = options.get(\"D\", \"\")\n",
        "\n",
        "    # Build the prompt using the provided format\n",
        "    prompt = (\n",
        "        \"Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. \"\n",
        "        \"Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. \"\n",
        "        \"Si no est√°s seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\n\"\n",
        "        f\"{question_text}\\n\\n\"\n",
        "        f\"A) {option_a}\\n\"\n",
        "        f\"B) {option_b}\\n\"\n",
        "        f\"C) {option_c}\\n\"\n",
        "        f\"D) {option_d}\\n\\n\"\n",
        "        \"Tu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# Test the prompt formatting on the first loaded question\n",
        "example_prompt = format_prompt(questions[0])\n",
        "print(\"Example formatted prompt:\\n\", example_prompt)\n"
      ],
      "metadata": {
        "id": "toS2gKl3pKtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838b47f9-0868-4bde-a0dc-d090952b4f3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example formatted prompt:\n",
            " Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°s seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\n",
            "\n",
            "Entre los cambios metab√≥licos que se observan en un paciente con resistencia a insulina existe:\n",
            "\n",
            "A) Incremento de la expresi√≥n hep√°tica de genes gluconeog√©nicos mediado por FOXO1 (forkhead box other) fosforilado.\n",
            "B) Descenso en los niveles intracelulares de hexoquinasa 2 dependiente de insulina.\n",
            "C) Aumento de la glucogen√≥lisis muscular, contribuyendo al incremento de la glucemia.\n",
            "D) Aumento en los niveles s√©ricos de amino√°cidos como leucina e isoleucina.\n",
            "\n",
            "Tu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: The format_prompt function takes a question from our list and builds a prompt. We put the question text, list all the choices (joined in one line for simplicity), and then give an explicit instruction. By asking for the letter only, we aim to have consistent outputs that are easy to check (the model hopefully will just respond with ‚ÄúB‚Äù, etc.). After defining the function, we preview an example prompt for the first question to verify the format. You can adjust this format as needed (for instance, if a model tends to do better with a different phrasing or if you want the model to explain its answer, etc.)."
      ],
      "metadata": {
        "id": "5-XAXQV_rgfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define function to call a model and get its response\n",
        "\n",
        "import time\n",
        "import os\n",
        "import openai\n",
        "import anthropic\n",
        "import together\n",
        "import google.generativeai as genai # Ensure import is here or in Cell 1\n",
        "from transformers import pipeline, AutoTokenizer # Keep existing imports\n",
        "\n",
        "# --- Keep your existing call_model function structure ---\n",
        "\n",
        "def call_model(model_cfg, prompt):\n",
        "    \"\"\"\n",
        "    Call a model with the given prompt and return its response and metadata.\n",
        "    Returns: output_text, tokens_used, latency\n",
        "    \"\"\"\n",
        "    provider = model_cfg.get(\"provider\", \"\").lower()\n",
        "    model_id = model_cfg.get(\"model_id\")\n",
        "    api_key_env = model_cfg.get(\"api_key_env\") # Get the env variable name\n",
        "    max_tokens = model_cfg.get(\"max_tokens\", 2000) # Use default from your setup\n",
        "    temperature = model_cfg.get(\"temperature\", 0.0) # Use default from your setup\n",
        "\n",
        "    tokens_used = None\n",
        "    output_text = \"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Existing HuggingFace Branch ---\n",
        "    if provider == \"huggingface\":\n",
        "        # (Your existing HuggingFace code...)\n",
        "        # Make sure to handle potential errors and set output_text, tokens_used\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "            # Determine pipeline type (text2text vs text-generation)\n",
        "            if \"google/flan\" in model_id.lower() or \"t5\" in model_id.lower():\n",
        "                pipe = pipeline(\"text2text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "                # Adjust generation parameters based on model_cfg\n",
        "                result = pipe(prompt, max_length=max_tokens, temperature=temperature if temperature > 0 else None, do_sample=temperature > 0)\n",
        "                output_text = result[0]['generated_text']\n",
        "            else:\n",
        "                pipe = pipeline(\"text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "                 # Adjust generation parameters based on model_cfg\n",
        "                result = pipe(prompt, max_new_tokens=max_tokens, temperature=temperature if temperature > 0 else None, do_sample=temperature > 0) # Use max_new_tokens\n",
        "                # Need to handle prompt inclusion in output for text-generation\n",
        "                output_text = result[0]['generated_text']\n",
        "                # Optional: Remove prompt from output if pipeline includes it\n",
        "                if output_text.startswith(prompt):\n",
        "                     output_text = output_text[len(prompt):]\n",
        "\n",
        "            # Calculate tokens (optional, can be slow for local models)\n",
        "            try:\n",
        "                input_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "                output_tokens = tokenizer(output_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "                tokens_used = int(len(input_tokens[0]) + len(output_tokens[0]))\n",
        "            except Exception:\n",
        "                tokens_used = None # Or handle error appropriately\n",
        "        except Exception as e:\n",
        "             # Log error, set output_text to indicate failure\n",
        "             print(f\"ERROR calling HuggingFace model {model_id}: {e}\")\n",
        "             output_text = f\"ERROR: {e}\"\n",
        "             tokens_used = None\n",
        "\n",
        "\n",
        "    # --- Existing OpenAI Branch ---\n",
        "    elif provider == \"openai\":\n",
        "        # (Your existing OpenAI code...)\n",
        "        # Ensure it checks the key using api_key_env\n",
        "        api_key = os.getenv(api_key_env or \"OPENAI_API_KEY\")\n",
        "        if api_key is None:\n",
        "             output_text = f\"ERROR: OpenAI API key ({api_key_env or 'OPENAI_API_KEY'}) not set.\"\n",
        "             # raise RuntimeError(f\"OpenAI API key not set...\") # Or raise error\n",
        "        else:\n",
        "            openai.api_key = api_key # Set key for this call (needed for older openai lib versions)\n",
        "            try:\n",
        "                # Your existing create logic (ChatCompletion)\n",
        "                # Make sure max_tokens and temperature are passed correctly\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                    # Handle other params like 'max_completion_tokens' if needed based on model_id\n",
        "                )\n",
        "                output_text = response['choices'][0]['message']['content'].strip()\n",
        "                if 'usage' in response:\n",
        "                    tokens_used = response['usage'].get('total_tokens')\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR calling OpenAI model {model_id}: {e}\")\n",
        "                output_text = f\"ERROR: {e}\"\n",
        "                tokens_used = None\n",
        "\n",
        "\n",
        "    # --- Existing Anthropic Branch ---\n",
        "    elif provider == \"anthropic\":\n",
        "        # (Your existing Anthropic code...)\n",
        "        # Ensure it checks the key using api_key_env\n",
        "        api_key = os.getenv(api_key_env or \"ANTHROPIC_API_KEY\")\n",
        "        if api_key is None:\n",
        "             output_text = f\"ERROR: Anthropic API key ({api_key_env or 'ANTHROPIC_API_KEY'}) not set.\"\n",
        "             # raise RuntimeError(...)\n",
        "        else:\n",
        "            try:\n",
        "                anthropic_client = anthropic.Client(api_key=api_key) # Initialize client here\n",
        "                response = anthropic_client.messages.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                # Extract text correctly from the Anthropic response object\n",
        "                # The structure might be response.content[0].text\n",
        "                if response.content and isinstance(response.content, list):\n",
        "                     output_text = response.content[0].text.strip()\n",
        "                else:\n",
        "                     # Fallback or check older response structures if needed\n",
        "                     output_text = str(response.completion).strip() # Adjust based on actual response object\n",
        "\n",
        "                # Anthropic API v3 response includes usage data\n",
        "                if hasattr(response, 'usage'):\n",
        "                    tokens_used = response.usage.input_tokens + response.usage.output_tokens\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"ERROR calling Anthropic model {model_id}: {e}\")\n",
        "                 output_text = f\"ERROR: {e}\"\n",
        "                 tokens_used = None\n",
        "\n",
        "\n",
        "    # --- Existing Together Branch ---\n",
        "    elif provider == \"together\":\n",
        "        # (Your existing Together code...)\n",
        "        # Ensure it checks the key using api_key_env\n",
        "        api_key = os.getenv(api_key_env or \"TOGETHER_API_KEY\")\n",
        "        if api_key is None:\n",
        "            output_text = f\"ERROR: Together API key ({api_key_env or 'TOGETHER_API_KEY'}) not set.\"\n",
        "            # raise RuntimeError(...)\n",
        "        else:\n",
        "            try:\n",
        "                together.api_key = api_key # Set the key for the together library\n",
        "                # Use together.Complete.create for non-chat models if needed\n",
        "                # Use together.Chat.create for chat models\n",
        "                # Assuming chat model based on your example:\n",
        "                response = together.Chat.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,\n",
        "                    # stream=False # Ensure streaming is off if not handled\n",
        "                )\n",
        "                # Extract text correctly\n",
        "                if response and 'choices' in response and len(response['choices']) > 0:\n",
        "                    output_text = response['choices'][0]['message']['content'].strip()\n",
        "                else:\n",
        "                    # Handle cases where response structure might differ or be empty\n",
        "                    output_text = \"ERROR: Unexpected response structure from Together AI\"\n",
        "                    print(f\"Unexpected Together AI response for {model_id}: {response}\")\n",
        "\n",
        "\n",
        "                # Together API might return usage info, check their documentation\n",
        "                # Example placeholder:\n",
        "                # if response and 'usage' in response:\n",
        "                # ¬† ¬† tokens_used = response['usage'].get('total_tokens')\n",
        "                tokens_used = None # Update if usage data is available\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"ERROR calling Together model {model_id}: {e}\")\n",
        "                 output_text = f\"ERROR: {e}\"\n",
        "                 tokens_used = None\n",
        "\n",
        "\n",
        "    # --- NEW Google Gemini Branch ---\n",
        "    elif provider == \"google\":\n",
        "        api_key = os.getenv(api_key_env or \"GOOGLE_API_KEY\")\n",
        "        if api_key is None:\n",
        "             output_text = f\"ERROR: Google API key ({api_key_env or 'GOOGLE_API_KEY'}) not set.\"\n",
        "             # raise RuntimeError(...) # Or raise error\n",
        "        else:\n",
        "            try:\n",
        "                # Configure the API key (safe to call multiple times)\n",
        "                genai.configure(api_key=api_key)\n",
        "\n",
        "                # Set up generation config\n",
        "                generation_config = genai.types.GenerationConfig(\n",
        "                    # candidate_count=1, # Default is 1\n",
        "                    # stop_sequences=None, # Optional stop sequences\n",
        "                    max_output_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                    # top_p=, top_k= # Add if needed\n",
        "                )\n",
        "\n",
        "                # Define safety settings (adjust as needed, e.g., BLOCK_NONE for less filtering during eval)\n",
        "                safety_settings = [\n",
        "                    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                ]\n",
        "                # Or BLOCK_NONE if you want minimal filtering for eval:\n",
        "                # safety_settings = [\n",
        "                #     {\"category\": f\"HARM_CATEGORY_{cat}\", \"threshold\": \"BLOCK_NONE\"}\n",
        "                #     for cat in [\"HARASSMENT\", \"HATE_SPEECH\", \"SEXUALLY_EXPLICIT\", \"DANGEROUS_CONTENT\"]\n",
        "                # ]\n",
        "\n",
        "\n",
        "                # Initialize the model\n",
        "                model = genai.GenerativeModel(\n",
        "                    model_name=model_id,\n",
        "                    generation_config=generation_config,\n",
        "                    safety_settings=safety_settings\n",
        "                    )\n",
        "\n",
        "                # Generate content\n",
        "                response = model.generate_content(prompt)\n",
        "\n",
        "                # Extract text and handle potential blocks\n",
        "                if response.parts:\n",
        "                     output_text = response.text # response.text joins parts automatically\n",
        "                elif response.prompt_feedback.block_reason:\n",
        "                     output_text = f\"ERROR: Blocked by safety filter - Reason: {response.prompt_feedback.block_reason}\"\n",
        "                     print(f\"WARNING: Call to {model_id} blocked. Reason: {response.prompt_feedback.block_reason}\")\n",
        "                else:\n",
        "                     # Handle other potential empty response cases\n",
        "                     output_text = \"ERROR: No content generated (unknown reason)\"\n",
        "                     print(f\"WARNING: Empty response from {model_id}, Response object: {response}\")\n",
        "\n",
        "\n",
        "                # Get token count from usage metadata\n",
        "                if hasattr(response, 'usage_metadata'):\n",
        "                     tokens_used = response.usage_metadata.prompt_token_count + response.usage_metadata.candidates_token_count\n",
        "                else:\n",
        "                     tokens_used = None # Fallback if metadata not present\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"ERROR calling Google model {model_id}: {e}\")\n",
        "                 output_text = f\"ERROR: {e}\"\n",
        "                 tokens_used = None\n",
        "\n",
        "    # --- Fallback for Unknown Provider ---\n",
        "    else:\n",
        "        output_text = f\"ERROR: Unknown provider '{provider}' for model {model_cfg.get('name', model_id)}\"\n",
        "        # raise ValueError(...) # Or raise error\n",
        "\n",
        "    latency = time.time() - start_time\n",
        "    # Ensure output_text is always a string\n",
        "    if not isinstance(output_text, str):\n",
        "         output_text = str(output_text)\n",
        "\n",
        "    return output_text.strip(), tokens_used, latency # Return stripped text"
      ],
      "metadata": {
        "id": "59_SJq6jrgDH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Interface and Evaluation Functions (Cells 5‚Äì6)\n",
        "In this section, we set up functions to handle model inference and evaluation:\n",
        "call_model(model_config, prompt_text) ‚Äì Invokes a single model (based on its provider and config) with the given prompt, and returns the model‚Äôs answer, along with metadata like token usage and latency.\n",
        "evaluate_model(model_config, questions) ‚Äì Uses call_model to get answers for each question from one model, checks correctness, and collects detailed results.\n",
        "We will also prepare a loop or another function to evaluate all models and aggregate the results for comparison.\n",
        "Structuring this logic into functions makes the notebook modular and easier to update. For example, if in the future we want to add a step for chain-of-thought (CoT) prompting or filter the model output for hallucinations, we could modify or wrap call_model accordingly. 5.1 call_model Implementation: This function will branch based on the provider:\n",
        "HuggingFace: use transformers pipeline or model generate. We‚Äôll initialize a pipeline for text generation or use the model‚Äôs generate method. We also tokenize the input to count input tokens. The output tokens can be counted by the tokenizer as well.\n",
        "OpenAI: use openai.Completion or openai.ChatCompletion depending on model type. For chat models (e.g., GPT-4), we pass the prompt as a user message. We retrieve the output text and usage info (token counts).\n",
        "Anthropic: (Claude models) use anthropic‚Äôs client. Typically you provide a prompt with a special format (like \"\\n\\nHuman: <question>\\n\\nAssistant:\"). We skip detailed implementation here but it can be added.\n",
        "Together: use Together API client. For example, together_client.complete or the chat completion as needed, based on their documentation. (Ensure TOGETHER_API_KEY is set.)\n",
        "Additional providers (e.g., Cohere, AI21) can be integrated similarly by adding new branches.\n",
        "We also measure the time taken for each call (latency). If token counts are not readily available from the API, we will set them to None (or you could estimate via a tokenizer). Let‚Äôs implement call_model below:"
      ],
      "metadata": {
        "id": "626XG9mwrmbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In call_model:\n",
        "We take the model‚Äôs config and a prompt string.\n",
        "Based on provider, we handle the call differently.\n",
        "HuggingFace: We load the model and tokenizer (from local or HuggingFace Hub). We use pipeline for simplicity (it will handle the model loading and generation). We choose the pipeline task based on model type (a quick check for ‚Äút5‚Äù in the model name to decide between text2text-generation and text-generation). After generation, we count tokens by encoding the prompt and output with the tokenizer.\n",
        "OpenAI: We use the OpenAI API. If the model is chat-based (we guess by name containing ‚Äúgpt-3.5‚Äù or ‚Äúgpt-4‚Äù), we use the ChatCompletion endpoint with a single user message. Otherwise, we use the older Completion endpoint. We fetch the text from the response and also get token usage if provided. (Make sure your OpenAI API key is set in the environment.)\n",
        "Anthropic: We format the prompt in the required way for Claude and call the client‚Äôs completion method. (This assumes the anthropic package is installed and imported.) Token count isn‚Äôt directly captured here.\n",
        "Together: We initialize the Together client and call the chat.completions.create method with the prompt as a user message. (This assumes the model supports chat format; for pure text-generation models on Together, you might use a different method like client.completion.create.) We extract the content from the response. (Token usage may be available via Together‚Äôs response, but for simplicity, we set it to None in this example.)\n",
        "We measure the time just before and after the call to compute latency.\n",
        "Finally, we return output_text (the model‚Äôs answer), tokens_used, and latency.\n",
        "This function abstracts away the differences in model access, giving us a unified interface for the evaluation loop.\n",
        "\n",
        "5.2 evaluate_model Implementation: This function will loop through all questions for a single model, use call_model to get the answer, check correctness, and record results. It will return a list of result records (one per question for that model) and also compute summary metrics (like number correct). We‚Äôll implement evaluate_model next:"
      ],
      "metadata": {
        "id": "lzix9vk7rwpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Define file paths\n",
        "original_questions_file = 'data/questions/MIR-2024-v01-t01.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "\n",
        "# Load answer key (assumed to be a dictionary with question IDs as keys)\n",
        "with open(answer_key_file, 'r', encoding='utf-8') as f:\n",
        "    answer_key = json.load(f)\n",
        "\n",
        "# Load original questions (assumed to be a list)\n",
        "with open(original_questions_file, 'r', encoding='utf-8') as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "# Filter out questions that are not in the answer key or whose answer is empty\n",
        "filtered_questions = [\n",
        "    q for q in questions\n",
        "    if q.get('id') in answer_key and answer_key[q.get('id')].strip() != \"\"\n",
        "]\n",
        "\n",
        "# Save filtered questions to a new file\n",
        "with open(filtered_questions_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(filtered_questions, f, indent=2)\n",
        "\n",
        "print(f\"Filtered questions saved to {filtered_questions_file} with {len(filtered_questions)} questions out of {len(questions)} original questions.\")\n"
      ],
      "metadata": {
        "id": "bibvMSjJEYcE",
        "outputId": "c981dc83-a9b7-4ebb-a5cf-9a23089927e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered questions saved to data/questions/MIR-2024-v01-t01_filtered.json with 180 questions out of 185 original questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.1: Patch OpenAI to support legacy calls in evaluator.py and reload evaluator module\n",
        "\n",
        "!pip install openai==0.28\n",
        "\n",
        "import openai\n",
        "\n",
        "# Define a wrapper class so that openai.chat.completions.create(...) works.\n",
        "class ChatCompletionsWrapper:\n",
        "    @staticmethod\n",
        "    def create(*args, **kwargs):\n",
        "        return openai.ChatCompletion.create(*args, **kwargs)\n",
        "\n",
        "class OpenAIChatWrapper:\n",
        "    completions = ChatCompletionsWrapper\n",
        "\n",
        "# Assign our wrapper to openai.chat\n",
        "openai.chat = OpenAIChatWrapper\n",
        "\n",
        "# Verify the patch:\n",
        "print(\"openai.chat.completions.create:\", openai.chat.completions.create)\n",
        "\n",
        "# Reload evaluator so that it picks up our patched openai\n",
        "import importlib\n",
        "import src.evaluator as evaluator_module\n",
        "importlib.reload(evaluator_module)\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "print(\"OpenAI version:\", openai.__version__)\n"
      ],
      "metadata": {
        "id": "C3Uqr1dYpKxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc585777-3a3f-4aad-e706-6b0c7634b477"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "openai.chat.completions.create: <function ChatCompletionsWrapper.create at 0x7ef4b3cb5440>\n",
            "OpenAI version: 0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.2\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "def extract_answer_letter(output):\n",
        "    \"\"\"\n",
        "    Extracts the first valid answer letter (A, B, C, D, or N) from the model output.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\b([ABCDN])\\b', output.upper())\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "# Parameters for test evaluation\n",
        "models_to_test = ['o3-mini-2025-01-31', 'claude-3-7-sonnet-20250219']\n",
        "prompt_strategy = \"Prompt-001\"\n",
        "sample_size = 5  # Evaluate 5 questions per model\n",
        "\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "\n",
        "# Define a separate folder for test evaluation results\n",
        "test_results_folder = \"data/test_results\"\n",
        "os.makedirs(test_results_folder, exist_ok=True)\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "results_summary = {}\n",
        "\n",
        "print(\"\\n--- RUNNING SAMPLE TEST EVALUATION (Results saved in a separate folder) ---\")\n",
        "for model in models_to_test:\n",
        "    print(f\"\\nEvaluating Model: {model} using Prompt Strategy: {prompt_strategy}\")\n",
        "    try:\n",
        "        # Run evaluation; if evaluator.run_evaluation supports an output_folder parameter, use it.\n",
        "        # Otherwise, move the result file to the test folder after creation.\n",
        "        result_file = evaluator.run_evaluation(\n",
        "            questions_file=filtered_questions_file,\n",
        "            answer_key_file=answer_key_file,\n",
        "            prompt_strategy=prompt_strategy,\n",
        "            model=model,\n",
        "            sample_size=sample_size\n",
        "            # , output_folder=test_results_folder  # Uncomment if supported by your evaluator\n",
        "        )\n",
        "        # If output_folder parameter is not supported, move the file manually:\n",
        "        new_result_file = os.path.join(test_results_folder, os.path.basename(result_file))\n",
        "        os.rename(result_file, new_result_file)\n",
        "        result_file = new_result_file\n",
        "\n",
        "        print(f\"‚úì Sample evaluation complete for {model}. Results saved to: {result_file}\")\n",
        "\n",
        "        with open(result_file, 'r') as f:\n",
        "            results = json.load(f)\n",
        "\n",
        "        details = results.get(\"details\", [])\n",
        "        if details:\n",
        "            print(\"\\n--- DETAILS FOR EACH QUESTION ---\")\n",
        "            for entry in details:\n",
        "                question_id = entry.get(\"question_id\", \"N/A\")\n",
        "                question_prompt = entry.get(\"prompt\", \"No prompt available\")\n",
        "                raw_output = entry.get(\"model_output\", \"No output\")\n",
        "                extracted = extract_answer_letter(raw_output)\n",
        "                entry[\"extracted_answer\"] = extracted\n",
        "                print(f\"Question ID: {question_id}\")\n",
        "                print(\"Prompt:\")\n",
        "                print(question_prompt)\n",
        "                print(\"Raw Output:\")\n",
        "                print(raw_output)\n",
        "                print(\"Extracted Answer:\", extracted)\n",
        "                print(\"-\" * 40)\n",
        "        else:\n",
        "            print(\"\\nNo per-question details found in the evaluation results.\")\n",
        "\n",
        "        summary = results.get(\"summary\", {})\n",
        "        results_summary[model] = summary\n",
        "\n",
        "        print(\"\\n--- SAMPLE EVALUATION SUMMARY ---\")\n",
        "        print(f\"Model: {summary.get('model', 'N/A')}\")\n",
        "        print(f\"Prompt Strategy: {summary.get('prompt_strategy', 'N/A')}\")\n",
        "        print(f\"Total Questions: {summary.get('total_questions', 'N/A')}\")\n",
        "        print(f\"Correct Answers: {summary.get('correct_count', 'N/A')} ({summary.get('accuracy', 0)*100:.2f}%)\")\n",
        "        print(f\"Incorrect Answers: {summary.get('incorrect_count', 'N/A')}\")\n",
        "        print(f\"Skipped Questions: {summary.get('skipped_count', 'N/A')}\")\n",
        "        print(f\"Invalid Count: {summary.get('invalid_count', 'N/A')}\")\n",
        "        print(f\"Total Score: {summary.get('total_score', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error during sample evaluation for {model}: {e}\")\n",
        "\n",
        "print(\"\\nIf the sample evaluation looks good, proceed to full evaluation in the next cell.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg_k5kIJ3RKJ",
        "outputId": "1a1c05b6-7bac-42a8-d868-887f77f06835"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RUNNING SAMPLE TEST EVALUATION (Results saved in a separate folder) ---\n",
            "\n",
            "Evaluating Model: o3-mini-2025-01-31 using Prompt Strategy: Prompt-001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:28<00:00,  5.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Sample evaluation complete for o3-mini-2025-01-31. Results saved to: data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.json\n",
            "\n",
            "No per-question details found in the evaluation results.\n",
            "\n",
            "--- SAMPLE EVALUATION SUMMARY ---\n",
            "Model: o3-mini-2025-01-31\n",
            "Prompt Strategy: Prompt-001\n",
            "Total Questions: 5\n",
            "Correct Answers: 5 (100.00%)\n",
            "Incorrect Answers: 0\n",
            "Skipped Questions: 0\n",
            "Invalid Count: 0\n",
            "Total Score: 15\n",
            "\n",
            "Evaluating Model: claude-3-7-sonnet-20250219 using Prompt Strategy: Prompt-001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Sample evaluation complete for claude-3-7-sonnet-20250219. Results saved to: data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.json\n",
            "\n",
            "No per-question details found in the evaluation results.\n",
            "\n",
            "--- SAMPLE EVALUATION SUMMARY ---\n",
            "Model: claude-3-7-sonnet-20250219\n",
            "Prompt Strategy: Prompt-001\n",
            "Total Questions: 5\n",
            "Correct Answers: 4 (80.00%)\n",
            "Incorrect Answers: 1\n",
            "Skipped Questions: 0\n",
            "Invalid Count: 0\n",
            "Total Score: 11\n",
            "\n",
            "If the sample evaluation looks good, proceed to full evaluation in the next cell.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In evaluate_model:\n",
        "We iterate over each question, format the prompt, and call the model via call_model.\n",
        "We wrap the model call in a try-except to catch any errors (for instance, if an API call fails or a model isn‚Äôt available). If there‚Äôs an error, we log it and move on, leaving output empty for that question.\n",
        "We then parse the model‚Äôs output to extract the answer. We assume the model should reply with a letter. The code checks the first character of the output: if it‚Äôs one of ‚ÄúA, B, C, D‚Äù, we treat that as the chosen option. (If the output is something else, you could include additional parsing logic ‚Äì for example, sometimes the model might output the full option text or a sentence. Here, we simplify by taking the first letter when possible. If the output is empty or doesn‚Äôt start with a letter, we mark the answer as incorrect by default.)\n",
        "We compare the model‚Äôs answer letter (uppercased) to the true answer letter from the question. If they match, it‚Äôs correct and we increment correct_count.\n",
        "We append a dictionary to results containing all relevant info: question ID, model name, the exact prompt used, the model‚Äôs raw output, a boolean for correctness, latency (in seconds), and tokens used.\n",
        "We also print a one-line progress update for each question, indicating what the model answered and whether it was correct. This helps to monitor the evaluation as it happens, especially if many questions are being tested.\n",
        "Finally, the function returns the list of results and the count of correct answers.\n",
        "With these functions in place, we can now evaluate all models and compile the metrics.\n",
        "6. Run Evaluation for All Models (Cell 7)\n",
        "Now we‚Äôll loop through each model in our models_config, evaluate it on all questions using evaluate_model, and collect the outcomes. We will calculate summary metrics for each model:\n",
        "Accuracy (% correct)\n",
        "Total score (number of correct answers out of total questions)\n",
        "Total tokens used (if available; this could be sum of tokens across all questions for that model)\n",
        "Average response time per question (latency)\n",
        "We‚Äôll store summary results in a list of dictionaries (which we can later convert to a DataFrame for display or CSV export). We‚Äôll also accumulate all per-question results into a single list for detailed logging."
      ],
      "metadata": {
        "id": "v-QBGGZIr927"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7 FULL EVALUATION\n",
        "\n",
        "import re\n",
        "import json\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "def extract_answer_letter(output):\n",
        "    match = re.search(r'\\b([ABCDN])\\b', output.upper())\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "# Parameters for full evaluation (you might want to increase sample_size or use all questions)\n",
        "models_to_evaluate = ['o3-mini-2025-01-31', 'claude-3-7-sonnet-20250219']\n",
        "prompt_strategy = \"Prompt-001\"\n",
        "# For full evaluation, you might set sample_size to None or the full count.\n",
        "sample_size = None\n",
        "\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "print(\"\\n--- RUNNING FULL EVALUATION ---\")\n",
        "for model in models_to_evaluate:\n",
        "    try:\n",
        "        result_file = evaluator.run_evaluation(\n",
        "            questions_file=filtered_questions_file,\n",
        "            answer_key_file=answer_key_file,\n",
        "            prompt_strategy=prompt_strategy,\n",
        "            model=model,\n",
        "            sample_size=sample_size  # Full evaluation: use all questions\n",
        "        )\n",
        "        print(f\"‚úì Full evaluation complete for {model}. Results saved to: {result_file}\")\n",
        "        # The full evaluation results will later be merged and exported to CSV.\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error during full evaluation for {model}: {e}\")\n",
        "\n",
        "print(\"\\nProceed to the cell that merges and exports full evaluation results.\")\n"
      ],
      "metadata": {
        "id": "k0Pbpuf6r9dF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279d2a84-a706-4b1e-db5c-980238167f59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RUNNING FULL EVALUATION ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [17:13<00:00,  5.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Full evaluation complete for o3-mini-2025-01-31. Results saved to: data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [03:56<00:00,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Full evaluation complete for claude-3-7-sonnet-20250219. Results saved to: data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.json\n",
            "\n",
            "Proceed to the cell that merges and exports full evaluation results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In Cell 7:\n",
        "We initialize all_details to gather every question‚Äôs result and summary_records for each model.\n",
        "We loop over each model configuration:\n",
        "* Call evaluate_model for that model, which returns the detailed results and count of correct answers.\n",
        "* We extend the all_details list with the results (so in the end, this list contains an entry for each model-question pair).\n",
        "* Compute accuracy as (num_correct / total_questions) * 100. We round it to two decimal places later for neatness.\n",
        "* Compute total tokens used by summing the tokens_used for each question result, if available. If none of the results have token info (i.e., the list is empty because maybe the API didn‚Äôt provide it), we leave total_tokens as None.\n",
        "* Compute average latency by summing all latencies and dividing by number of questions (we exclude any None latencies just in case).\n",
        "* Append a dictionary to summary_records with the model‚Äôs name and metrics. We include total questions for reference, and round the accuracy and average latency for readability.\n",
        "* Print a summary line for each model (e.g., ‚ÄúFinished ModelX: 8/10 correct, Accuracy 80.0%.‚Äù).\n",
        "After this loop, we have:\n",
        "* summary_records: a list of summary info for each model.\n",
        "* all_details: a list of per-question info, which we can turn into a detailed log.\n",
        "Next, we‚Äôll convert these to pandas DataFrames for easy viewing and export."
      ],
      "metadata": {
        "id": "_-mQLra8sJBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# FINAL EXPORT CELL: Full Evaluation Results to CSV, GitHub, & Download\n",
        "# =========================\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Step 1: Gather Full Evaluation Result Files ---\n",
        "# Adjust the glob pattern if necessary to include only full evaluation files.\n",
        "# (This example assumes your full evaluation result files contain \"filtered\" in the filename.)\n",
        "result_files = glob.glob(\"data/results/EVAL-MIR-2024-v01-t01_filtered-*.json\")\n",
        "print(f\"Found {len(result_files)} full evaluation result file(s).\")\n",
        "\n",
        "# --- Step 2: Merge Detailed Evaluation Results ---\n",
        "# We assume each JSON file contains detailed results under either the \"details\" or \"results\" key.\n",
        "all_details = []\n",
        "for file in result_files:\n",
        "    try:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        details = data.get('details') or data.get('results')\n",
        "        if details:\n",
        "            all_details.extend(details)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "if not all_details:\n",
        "    print(\"No detailed evaluation results were found in the JSON files.\")\n",
        "else:\n",
        "    # Convert the merged results to a DataFrame.\n",
        "    df_full = pd.DataFrame(all_details)\n",
        "    print(\"Merged full evaluation results shape:\", df_full.shape)\n",
        "    print(\"Columns in full evaluation results:\", df_full.columns.tolist())\n",
        "\n",
        "    # --- Step 3: Export Merged Results to a Single CSV File ---\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    export_filename = f\"full_evaluation_results_{timestamp}.csv\"\n",
        "    export_dir = os.path.join(\"data\", \"exports\")\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "    export_path = os.path.join(export_dir, export_filename)\n",
        "    df_full.to_csv(export_path, index=False)\n",
        "    print(f\"‚úì Full evaluation results exported to CSV: {export_path}\")\n",
        "\n",
        "    # --- Step 4: Download the CSV File Locally (Colab) ---\n",
        "    files.download(export_path)\n",
        "\n",
        "    # --- Step 5: Push the CSV File to GitHub ---\n",
        "    try:\n",
        "        # Retrieve GitHub token from Colab secrets\n",
        "        github_token = userdata.get('GITHUB_TOKEN')\n",
        "        if not github_token:\n",
        "            raise ValueError(\"GITHUB_TOKEN is not set in Colab secrets.\")\n",
        "\n",
        "        # Configure the remote URL with your GitHub token\n",
        "        repo_name = \"armelida/MELIDA\"\n",
        "        token_url = f\"https://{github_token}@github.com/{repo_name}.git\"\n",
        "\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", \"armelida@gmail.com\"], check=True)\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", \"Armelida\"], check=True)\n",
        "        subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", token_url], check=True)\n",
        "\n",
        "        # Stage the new CSV file for commit.\n",
        "        subprocess.run([\"git\", \"add\", export_path], check=True)\n",
        "        commit_message = f\"Export full evaluation results {timestamp}\"\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], check=True)\n",
        "\n",
        "        # Pull the latest changes and then push your commit.\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", \"main\", \"--rebase\"], check=True)\n",
        "        subprocess.run([\"git\", \"push\", \"origin\", \"main\"], check=True)\n",
        "        print(\"‚úì CSV file successfully pushed to GitHub.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GitHub push: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "hfno6LLELaoL",
        "outputId": "7ffdb262-d919-4adc-9722-06189fce4562"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 full evaluation result file(s).\n",
            "Merged full evaluation results shape: (360, 14)\n",
            "Columns in full evaluation results: ['question_id', 'question_text', 'prompt_strategy', 'model', 'prompt', 'full_model_output', 'model_answer', 'raw_response', 'response_time', 'tokens_used', 'timestamp', 'correct_answer', 'score', 'result_id']\n",
            "‚úì Full evaluation results exported to CSV: data/exports/full_evaluation_results_20250402_221350.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e028fd29-a8e2-4abe-bc3b-1a0a9b0f9f87\", \"full_evaluation_results_20250402_221350.csv\", 591323)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì CSV file successfully pushed to GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CELL: Export Most Failed Questions CSV\n",
        "# =========================\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# (Optional) Load the most recent full evaluation CSV from your exports folder:\n",
        "export_files = sorted(glob.glob(\"data/exports/full_evaluation_results_*.csv\"))\n",
        "if export_files:\n",
        "    latest_export = export_files[-1]\n",
        "    df_full = pd.read_csv(latest_export)\n",
        "    print(f\"Loaded merged full evaluation results from: {latest_export}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No full evaluation CSV file found in data/exports/.\")\n",
        "\n",
        "# Standardize answer columns and compute correctness.\n",
        "df_full['model_answer'] = df_full['model_answer'].astype(str).str.strip().str.upper()\n",
        "df_full['correct_answer'] = df_full['correct_answer'].astype(str).str.strip().str.upper()\n",
        "df_full['raw_response'] = df_full['raw_response'].astype(str).str.strip()\n",
        "df_full['correct'] = df_full['model_answer'] == df_full['correct_answer']\n",
        "\n",
        "# Determine which column to use for question text.\n",
        "if 'prompt' in df_full.columns:\n",
        "    question_text_col = 'prompt'\n",
        "elif 'question_text' in df_full.columns:\n",
        "    question_text_col = 'question_text'\n",
        "else:\n",
        "    # If no column exists, create one with a default value.\n",
        "    df_full['question_text'] = \"Not available\"\n",
        "    question_text_col = 'question_text'\n",
        "\n",
        "# Filter out only the failed evaluations.\n",
        "df_failures = df_full[~df_full['correct']]\n",
        "\n",
        "# Group by question_id to aggregate failure information.\n",
        "df_failures_summary = df_failures.groupby(\"question_id\").agg(\n",
        "    failure_count=(\"model\", \"count\"),\n",
        "    models_failed=(\"model\", lambda x: \", \".join(sorted(x.unique()))),\n",
        "    correct_answer=(\"correct_answer\", \"first\"),\n",
        "    raw_responses=(\"raw_response\", lambda x: \" || \".join(x.astype(str).unique())),\n",
        "    question_text=(question_text_col, \"first\")\n",
        ").reset_index()\n",
        "\n",
        "# Sort by failure_count descending (most failed questions at the top).\n",
        "df_failures_summary = df_failures_summary.sort_values(\"failure_count\", ascending=False)\n",
        "\n",
        "# Save the summary to a CSV file.\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "failed_csv = f\"most_failed_questions_{timestamp}.csv\"\n",
        "export_dir = os.path.join(\"data\", \"exports\")\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "export_path = os.path.join(export_dir, failed_csv)\n",
        "df_failures_summary.to_csv(export_path, index=False)\n",
        "print(f\"‚úì Most failed questions exported to CSV: {export_path}\")\n",
        "\n",
        "# Optionally, display the DataFrame.\n",
        "print(\"=== Most Failed Questions ===\")\n",
        "print(df_failures_summary.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vphnlZLZOrGl",
        "outputId": "28c8ae0e-6473-4df1-f2c4-bf9e828dfe7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded merged full evaluation results from: data/exports/full_evaluation_results_20250402_221350.csv\n",
            "‚úì Most failed questions exported to CSV: data/exports/most_failed_questions_20250402_221351.csv\n",
            "=== Most Failed Questions ===\n",
            "          question_id  failure_count                                  models_failed correct_answer raw_responses                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             question_text\n",
            "MIR-2024-v01-t01-Q084              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              D      nan || A                                                                                                                                                                                                                                                                                                                                                                                                                             Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 81 a√±os con antecedentes de HTA, dislipemia y enfermedad de Alzheimer leve. Avisa su familia porque lo han encontrado en su casa ca√≠do en el suelo y no es capaz de movilizar extremidades derechas. Llega al hospital trasladado por emergencias como c√≥digo ictus. A la exploraci√≥n f√≠sica destaca paresia facial derecha supranuclear, afasia global, hemianopsia derecha y hemiplejia de miembros derechos. La TC craneal se informa como ASPECTS 8 sin datos de sangrado. En angioTC oclusi√≥n de segmento M2 de divisi√≥n anterior de ACM izquierda. ¬øCu√°l de los siguientes es el mejor tratamiento inicial para este paciente?\\n\\nA) Fibrinolisis con alteplasa.\\nB) Doble antiagregaci√≥n y estatinas.\\nC) Manejo conservador por los antecedentes del paciente.\\nD) Trombectom√≠a mec√°nica.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q068              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B      nan || A                                                                                       Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nAvisan de paritorio por parto a t√©rmino. Se explora a reci√©n nacida, sin esfuerzo de llanto, fl√°cida y cian√≥tica. Se seca y estimula y ante la ausencia de inicio de llanto se traslada a cuna t√©rmica. Inicia llanto intenso y eficaz a los 30 segundos de vida, donde ya se pod√≠a observar recuperaci√≥n del tono de forma completa con movimiento activo, frecuencia card√≠aca por encima de 100 lpm y estornudo tras aspiraci√≥n de secreciones far√≠ngeas espesas. Sin embargo, persiste cianosis generalizada hasta 1 minuto y 20 segundos de vida a partir del cual presenta solamente acrocianosis. A los 4 minutos de vida inicia dificultad respiratoria (esfuerzo respiratorio irregular) consistente en polipnea, tiraje subcostal e intercostal leve-moderado, leve aleteo nasal y quejido espiratorio (con frecuencia card√≠aca por encima de 100 lpm, movimiento activo, recuperaci√≥n completa del color con coloraci√≥n completamente rosada y reflejos de tos y estornudo), que se resuelve tras colocar presi√≥n positiva durante 2 minutos. ¬øQu√© puntuaci√≥n de Apgar tiene esta reci√©n nacida?\\n\\nA) 4/8.\\nB) 8/9.\\nC) 9/9.\\nD) 9/10.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q043              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B      nan || C                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nSe√±ala la opci√≥n INCORRECTA respecto a la enfermedad meningoc√≥cica:\\n\\nA) El mecanismo de transmisi√≥n es por gotas.\\nB) Se dispone de vacunas efectivas √∫nicamente para la prevenci√≥n de 4 subtipos de meningococo.\\nC) La vacuna frente a meningococo tipo B est√° incluida en el calendario sistem√°tico de inmunizaciones del Sistema Nacional de Salud.\\nD) Ante un contacto de riesgo con un paciente con meningitis meningoc√≥cica, la vacunaci√≥n del contacto no debe ser la primera estrategia de profilaxis postexposici√≥n.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q079              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              C      nan || D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nMujer de 35 a√±os consulta por cuadro de diplopia y debilidad proximal progresivas que empeora a lo largo de d√≠a. En la exploraci√≥n neurol√≥gica se aprecia voz nasal, ptosis bilateral sin afectaci√≥n pupilar, limitaci√≥n para la abducci√≥n de ojo derecho y debilidad proximal con reflejos miot√°ticos conservados. Respecto a esta paciente se√±ale la opci√≥n INCORRECTA:\\n\\nA) El nivel de anticuerpos es independiente de la gravedad de la enfermedad.\\nB) En ocasiones se asocia a otra enfermedad autoinmune.\\nC) Es frecuente la agregaci√≥n familiar.\\nD) La respuesta a inmunosupresores es mayor en pacientes con anticuerpos antirreceptor de acetilcolina y anticuerpos antiMusk positivos.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q181              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B             D                                                                                                                                                                                                                                                                                                                                                                                                             Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nEn relaci√≥n con la marca producida por el paso de la corriente el√©ctrica a trav√©s de la piel (tambi√©n conocida como lesi√≥n electroespec√≠fica o ‚Äúmarca de Jellinek‚Äù), ¬øcu√°l de las siguientes afirmaciones es cierta?:\\n\\nA) La mayor parte de las lesiones producidas por la corriente el√©ctrica en medio dom√©stico y laboral proceden de fuentes de alto voltaje.\\nB) Las lesiones d√©rmicas dependen del efecto de un aumento de la temperatura generado sobre la dermis.\\nC) La corriente de bajo voltaje produce lesiones profundas, mientras que la de alto voltaje da lugar a lesiones superficiales adyacentes al punto de contacto con la corriente el√©ctrica.\\nD) La marca el√©ctrica es diagn√≥stica del lugar en el que se ha producido el contacto y, por tanto, el punto de entrada de la corriente en el cuerpo.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q160              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              A      nan || C                                                                                                                                                                                                                                           Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nMujer de 25 a√±os, sin enfermedades previas conocidas, que consulta por lesi√≥n en labio inferior. No es la primera vez que le sucede desde la adolescencia, siempre en la misma zona, inicia dolor y aparece una lesi√≥n vesiculosa que se autolimita en una semana aproximadamente. Respecto al abordaje de la enfermedad de esta paciente, es INCORRECTO que:\\n\\nA) Si presenta m√°s de tres episodios anuales est√° indicado realizar profilaxis con un antiviral como valaciclovir 500 mg cada 12 horas o 1 g al d√≠a, durante seis meses.\\nB) Si es una √∫nica lesi√≥n y es un cuadro leve puede no realizarse ning√∫n tratamiento o bien realizar tratamiento t√≥pico con fomentos de sulfato de zinc o de cobre.\\nC) Si existe riesgo de afectaci√≥n oft√°lmica estar√≠a indicado tratar con un antiv√≠rico como valaciclovir a raz√≥n de 1 g cada 8 horas durante 7 d√≠as v√≠a oral.\\nD) Es necesario recomendarle una adecuada protecci√≥n solar pues la exposici√≥n solar puede motivar recurrencias.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q206              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              D             C                                                                                                                                                                                                                                                               Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nAvisan de paritorio por reci√©n nacido pret√©rmino tard√≠o (35+3 semanas de edad gestacional) sin otros factores de riesgo. Al nacimiento no presenta esfuerzo respiratorio y se decide monitorizaci√≥n con pulsioximetr√≠a. No inicia esfuerzo por lo que a los 30 segundos se ventila con presi√≥n positiva intermitente (VPPI) (presi√≥n inspiratoria 20 cm H20 y PEEP 6 cm H20 y FiO2 0,21) durante 1 minuto. Inicia llanto y esfuerzo respiratorio pero con signos de dificultad respiratoria en torno a los 3 minutos de vida con cianosis asociada. En la monitorizaci√≥n presenta frecuencia card√≠aca de 170 lpm y Sat O2 del 75%. ¬øCu√°l es su actuaci√≥n en este momento?:\\n\\nA) Retirar soporte respiratorio y aumento aporte de ox√≠geno.\\nB) Mantener VPPI y fracci√≥n inspirada de ox√≠geno al 21%.\\nC) Pasar a modalidad presi√≥n positiva continua y aumento aporte de ox√≠geno.\\nD) Pasar a modalidad presi√≥n positiva continua y fracci√≥n inspirada de ox√≠geno al 21%.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q173              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              A             D                                                                                                                                                                                                                                                                                                                                                 Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 55 a√±os con antecedentes de c√°ncer de pulm√≥n (estadio IV). Su madre muri√≥ a los 65 a√±os por un episodio de broncoaspiraci√≥n. Consulta por debilidad muscular en los muslos y disfagia. Exploraci√≥n f√≠sica: fuerza en psoas 4+/5, cu√°driceps 3/5, deltoides 5/5. Presenta ptosis palpebral bilateral y no tiene diplop√≠a. Los valores de CPK eran 900 UI/L (normal <150 UI/L) y los de aldolasa, 7 UI/L (normal <6 UI/L). Se practic√≥ una biopsia en el m√∫sculo vasto lateral externo del muslo derecho, en la cual se objetiv√≥ variabilidad en el tama√±o de fibras, aumento del tejido conectivo, im√°genes de pseudodivisi√≥n celular y vacuolas ribeteadas; no se observ√≥ infiltrado inflamatorio. ¬øQu√© es lo m√°s probable que le suceda al paciente?:\\n\\nA) Distrofia oculofar√≠ngea.\\nB) Miopat√≠a mitocondrial.\\nC) Miastenia gravis.\\nD) Miositis con cuerpos de inclusi√≥n.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q169              2 claude-3-7-sonnet-20250219, o3-mini-2025-01-31              B      nan || C Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 53 a√±os, sin antecedentes patol√≥gicos, acude al hospital por fiebre de 10 d√≠as de evoluci√≥n, con intensa cefalea retroorbitaria y una importante sensaci√≥n de fatiga. No refiere dolor musculoesquel√©tico, lesiones cut√°neas ni ning√∫n otro s√≠ntoma acompa√±ante. A lo largo de 15 d√≠as de ingreso se objetiva fiebre persistente y la aparici√≥n de discreta ictericia conjuntival. La exploraci√≥n respiratoria, card√≠aca, abdominal y neurol√≥gica es normal. La anal√≠tica muestra VSG 97 mm/h, PCR 24,5 mg/dL, GOT 156 U/L, GPT 148 U/L, FA 410 U/L, GGT 794 U/L, bilirrubina total 5,5 mg/dL a expensas de la bilirrubina directa, hemoglobina 11,5 g/dL, recuento leucocitario normal, plaquetas 648.000/mm3, funci√≥n renal e ionograma normales, tiempo de protrombina normal pero TTPA ratio 1,8. Los anticuerpos antinucleares (ANA) son positivos a t√≠tulos 1/160. Una ecograf√≠a abdominal no muestra ninguna alteraci√≥n destacable. Desde el punto de vista cl√≠nico y a la vista de los datos anal√≠ticos, ¬øcu√°l de las siguientes enfermedades es la que se ajusta mejor con nuestro paciente?:\\n\\nA) COVID-19\\nB) Fiebre Q aguda.\\nC) Leptospirosis grave (enfermedad de Weil).\\nD) Colangitis esclerosante primaria.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q027              1                     claude-3-7-sonnet-20250219              C             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nLa deficiencia de acil-CoA-deshidrogenasa provoca una de las siguientes alteraciones bioqu√≠micas:\\n\\nA) Disminuci√≥n de √°cidos dicarbox√≠licos.\\nB) Aumento de la gluconeog√©nesis.\\nC) Disminuci√≥n de la ureag√©nesis.\\nD) Aumento de carnitina libre.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q041              1                     claude-3-7-sonnet-20250219              D             C                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nAl finalizar el periodo de seguimiento en el estudio PREDIMET (‚ÄúPrimary Prevention of Cardiovascular Disease with a Mediterranean Diet‚Äù), el 3,8% de los individuos asignados a dieta mediterr√°nea presentaron alg√∫n tipo de evento cardiovascular (infartos de miocardio, ictus o muertes de origen cardiovascular), frente al 4,4% de eventos ocurridos en el grupo control. ¬øCu√°l es el n√∫mero necesario de pacientes a tratar (NNT) con dieta mediterr√°nea para evitar un evento cardiovascular?:\\n\\nA) 26\\nB) 60\\nC) 80\\nD) 167\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q120              1                             o3-mini-2025-01-31              B             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nUn paciente con enfermedad de Crohn con antecedente de extirpaci√≥n de un melanoma precisa iniciar un tratamiento avanzado por corticodependencia. ¬øCu√°l de los siguientes f√°rmacos se deber√≠a evitar al estar descrito un aumento del riesgo de dicha neoplasia?\\n\\nA) Azatioprina.\\nB) Infliximab.\\nC) Vedolizumab.\\nD) Ustekinumab.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q091              1                             o3-mini-2025-01-31              B           nan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nMujer de 19 a√±os que, practicando patinaje sobre ruedas, sufre un traumatismo indirecto de la rodilla derecha, con luxaci√≥n de r√≥tula tratada en urgencias. Acude a revisi√≥n 2 meses m√°s tarde y refiere que nota dolor y sensaci√≥n de inestabilidad y subluxaci√≥n frecuente con la actividad f√≠sica. De los siguientes elementos, se√±ale el que NO favorece la inestabilidad f√©moro-patelar:\\n\\nA) Anteversi√≥n femoral.\\nB) Torsi√≥n tibial interna.\\nC) Genu valgo.\\nD) Patela alta.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q045              1                     claude-3-7-sonnet-20250219              D             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nEn un metaan√°lisis, el riesgo relativo estimado para la asociaci√≥n causal entre el uso de mascarilla y la incidencia de SARS-CoV-2 fue de 0,47. ¬øCu√°l es la interpretaci√≥n correcta de este resultado?:\\n\\nA) La incidencia de SARS-CoV-2 se reduce un 47% cuando la poblaci√≥n usa mascarilla.\\nB) No usar mascarilla aumenta un 53% el riesgo de infecci√≥n por SARS-CoV-2.\\nC) El uso de mascarilla podr√≠a evitar 47 de cada 100 casos de SARS-CoV-2 que se dan en personas que no la usan.\\nD) La incidencia de SARS-CoV-2 en la poblaci√≥n que no usa mascarilla se reducir√≠a un 53% si la utilizara.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q069              1                     claude-3-7-sonnet-20250219              B             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nRespecto a las medidas a tomar ante un ni√±o diagnosticado de tos ferina, en relaci√≥n a sus contactos, indique la respuesta INCORRECTA:\\n\\nA) Tratar con antibi√≥ticos a toda la familia, independientemente de la edad, estado de inmunizaci√≥n y sintomatolog√≠a.\\nB) Tratar con antibi√≥ticos a todo el personal sanitario que ha atendido al ni√±o independientemente de la edad, estado de inmunizaci√≥n y sintomatolog√≠a.\\nC) Vacunar a los adultos convivientes, aunque hayan padecido la enfermedad ya que la inmunidad no dura toda la vida.\\nD) Aislar al paciente al menos 5 d√≠as despu√©s de iniciar el tratamiento.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q037              1                     claude-3-7-sonnet-20250219              C             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nMutaciones en el gen Btk pueden causar una inmunodeficiencia primaria que cursa con la disminuci√≥n dr√°stica o ausencia de inmunoglobulinas en la sangre perif√©rica (agammaglobulinemia). En relaci√≥n al estudio de los pacientes con esta agammaglobulinemia, ¬øcu√°l de las siguientes afirmaciones es cierta?\\n\\nA) Es una enfermedad autos√≥mica recesiva.\\nB) El porcentaje de linfocitos que expresan CD19 o CD20 (linfocitos B) en sangre perif√©rica est√° dentro de los l√≠mites normales.\\n\\nC) El nivel de IgG en suero disminuye paulatinamente desde el nacimiento hasta resultar indetectable.\\nD) La expresi√≥n de la prote√≠na Btk en monocitos es normal.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q149              1                             o3-mini-2025-01-31              A             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nMujer de 90 a√±os sin alergias medicamentosas conocidas, con HTA y gonalgia nocturna por gonartrosis derecha de intensidad leve-moderada de tres meses de evoluci√≥n. Refiere que no toma ninguna medicaci√≥n y se confirma que realiza las medidas higi√©nicas del sue√±o correctamente. Consulta a su m√©dico de atenci√≥n primaria por insomnio de conciliaci√≥n y mantenimiento que le interfiere con sus actividades diarias. ¬øQu√© f√°rmaco es m√°s adecuado iniciar en este caso?:\\n\\nA) Paracetamol.\\nB) Diazepam.\\nC) Mirtazapina.\\nD) Trazodona.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q126              1                             o3-mini-2025-01-31              D           nan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 77 a√±os que acude a urgencias por prurito, ictericia y s√≠ndrome t√≥xico de un mes de evoluci√≥n. Refiere ingesta de amoxicilinaclavul√°nico desde hace 10 d√≠as por un resfriado. A su llegada se realiza una ecograf√≠a que se informa de dilataci√≥n de la v√≠a biliar intra y extrahep√°tica con ves√≠cula biliar distendida. ¬øCu√°l es la sospecha diagn√≥stica y la actitud a seguir?:\\n\\nA) Colecistitis aguda e indicaci√≥n de colecistectom√≠a urgente.\\nB) Coledocolitiasis distal y solicitud de colangiorresonancia magn√©tica.\\nC) Toxicidad por f√°rmacos y suspensi√≥n de la medicaci√≥n.\\nD) Tumoraci√≥n maligna obstructiva de col√©doco distal y solicitar TC abdominal.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q137              1                     claude-3-7-sonnet-20250219              A             C                                                                                                                                                                                                                                                                                                                                                                                                                                                       Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 28 a√±os que consulta por haberse despertado por dolor en la regi√≥n genital y tumefacci√≥n en el pene. No recuerda con detalle lo sucedido en las √∫ltimas 12 horas. En las √∫ltimas 48 h ha consumido alto volumen de alcohol, varias drogas (mefedrona, coca√≠na, anfetamina), junto con sildenafilo y se ha autoinyectado en el pene 20 ¬µg de alprostadil. A la exploraci√≥n destaca, pene fl√°cido, glande no visible en su totalidad por edema y hematoma expansivo de partes blandas. Presenta restos hem√°ticos en el meato uretral. ¬øCu√°l de las siguientes opciones NO formar√≠a parte de su plan terap√©utico?:\\n\\nA) Gasometr√≠a obtenida mediante punci√≥n del cuerpo cavernoso.\\nB) Revisi√≥n quir√∫rgica urgente.\\nC) Evitar el sondaje vesical.\\nD) Ecograf√≠a de pene.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q170              1                             o3-mini-2025-01-31              B           nan                                                                                                                                                                                                                                                                                                                                                          Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 38 a√±os con dependencia alcoh√≥lica grave. Ha tenido varios ingresos por pancreatitis aguda en el contexto de ingesta de alcohol e hipertrigliceridemia grave. Acude a consulta 12 meses despu√©s del √∫ltimo episodio de pancreatitis. No presenta cl√≠nica que sugiera insuficiencia pancre√°tica exocrina. Niega reiteradamente ingesta de alcohol. Sigue tratamiento con estatinas y fibratos que asegura tom√°rselos. Aporta un an√°lisis b√°sico reciente de su empresa con los siguientes datos: colesterol 300 mg/dL, cHDL 42 mg/dL, TG 1102 mg/dL, cLDL no calculable por Friedewald. Colesterol no-HDL 258 mg/dL, ApoB 90 mg/dL, Lp(a) 38 nmol/L (normal <125 nmol/L). En dicho an√°lisis hay un dato, adem√°s de la hipertrigliceridemia, que sugiere que el paciente contin√∫a ingiriendo alcohol. Se√±√°lelo.\\n\\nA) ApoB.\\nB) cHDL.\\nC) Lp(a).\\nD) Colesterol total.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q161              1                             o3-mini-2025-01-31              A             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 50 a√±os que presenta una erupci√≥n generalizada especialmente en el tronco. Refiere malestar general desde hace pocos d√≠as. Hoy acude porque ha tenido fiebre la noche anterior y se ha visto una mancha negra en la zona abdominal. A la exploraci√≥n f√≠sica, la mancha negra corresponde a una peque√±a escara. El paciente es cazador y acudi√≥ recientemente a una batida. Sobre el abordaje del caso es cierto que:\\n\\nA) Es una enfermedad de declaraci√≥n obligatoria.\\nB) El tratamiento de elecci√≥n es la amoxicilina, un gramo cada 8 horas durante 7 d√≠as.\\nC) Debe derivarse a un centro hospitalario.\\nD) En la mayor√≠a de los casos, la fiebre precede al exantema y la escara.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q148              1                             o3-mini-2025-01-31              A             D                                                                                                                                                                                      Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nMujer de 50 a√±os diagnosticada de s√≠ndrome mielodispl√°sico con monosom√≠a del cromosoma 7 (45 XX -7 20) es sometida a un trasplante hematopoy√©tico alog√©nico con sangre de cord√≥n umbilical de un hombre compatible procedente de un banco de otro pa√≠s. A los tres meses del trasplante con recuperaci√≥n completa de las cifras hematol√≥gicas perif√©ricas se realiza una punci√≥n de m√©dula √≥sea, que no presenta alteraciones morfol√≥gicas y un estudio cromos√≥mico con un cariotipo 47 XXY 20. ¬øCu√°l es la explicaci√≥n de estos hallazgos?:\\n\\nA) El donante tiene un s√≠ndrome de Klinefelter no diagnosticado.\\nB) En la m√©dula √≥sea coexisten c√©lulas del donante y del receptor (quimerismo mixto).\\nC) Tras el trasplante ha existido un fen√≥meno de fusi√≥n celular entre las c√©lulas del donante y receptor que ha originado el nuevo cariotipo.\\nD) En el cariotipo realizado precozmente tras un trasplante hematopoy√©tico es habitual encontrar alteraciones cromos√≥micas transitorias de este tipo que carecen de significaci√≥n cl√≠nica.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q172              1                     claude-3-7-sonnet-20250219              A             B                                                                                                                                                                                                                                                                                                                                                                         Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 37 a√±os con infecci√≥n VIH ha dejado de tomar el tratamiento por razones personales hace 3 meses; estaba tomando una combinaci√≥n de dos inhibidores de transcriptasa inversa y un inhibidor de la integrasa. En la evaluaci√≥n llevada a cabo en este momento tiene 120 linfocitos CD4/mm3 y una carga viral de VIH-1 de 80.000 copias/mL. Estamos pendientes del estudio de resistencias. ¬øQu√© debemos recomendarle en este momento?:\\n\\nA) Reiniciar el tratamiento con lo mismo con lo que estaba previamente.\\nB) Esperar a tener el resultado del estudio de resistencias para orientar el tipo de tratamiento en funci√≥n de las mutaciones identificadas.\\nC) Prescribir a partir de ahora un r√©gimen basado en inhibidores de proteasa por su mayor barrera gen√©tica a las resistencias.\\nD) Iniciar doble terapia con inhibidores de integrasa.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q183              1                             o3-mini-2025-01-31              B           nan                                                                                                                                                                                                                                                                                                                                                                                                               Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nLa telemonitorizaci√≥n domiciliaria ha demostrado beneficios en pacientes con insuficiencia card√≠aca. Una mujer de 82 a√±os con insuficiencia card√≠aca y diabetes tratada con insulina est√° en programa de telemonitorizaci√≥n y remite cada semana una serie de par√°metros a su m√©dico de familia. La semana pasada presentaba presi√≥n arterial 140/85 mmHg. Glucemia capilar basal 120 mg/dL. Glucemia postprandial 2 h tras la comida 160 mg/dL. Peso 83 kg. Frecuencia card√≠aca 72 lpm. Al recibir los par√°metros de esta semana que se exponen a continuaci√≥n, su m√©dico decide citar a la paciente en consulta. ¬øCu√°l de los siguientes par√°metros ha motivado dicha decisi√≥n?:\\n\\nA) Presi√≥n arterial 135/82 mmHg.\\nB) Peso 85 Kg.\\nC) Glucemia capilar basal 130 mg/dL.\\nD) Glucemia capilar postprandial 190 mg/dL.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q190              1                             o3-mini-2025-01-31              A           nan                                                                                                                                                                        Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nHombre de 36 a√±os sin antecedentes de inter√©s que ingresa en observaci√≥n de urgencias en espera de intervenci√≥n quir√∫rgica por fractura de f√©mur a nivel diafisario por accidente de tr√°fico. El paciente llega algo taquic√°rdico 102 lpm, con gafas nasales para mantener saturaci√≥n de ox√≠geno en torno al 96%, 20 respiraciones por minuto y presi√≥n arterial 100/60 mmHg. Se procede a ajuste de analgesia y a las 20 horas de estancia en observaci√≥n indican que el paciente puede ir a quir√≥fano pero presenta un aumento de disnea con saturaci√≥n del 89% con mascarilla tipo Venturi, 28 respiraciones por minuto, taquicardia a 110 lpm, fiebre de 38¬∫C y se encuentra algo obnubilado. En la anal√≠tica destaca anemia y trombocitopenia. No presenta aumento de volumen en ninguna de las 2 piernas. ¬øQu√© prueba complementaria tiene mayor sensibilidad y especificidad para la sospecha diagn√≥stica de este paciente?\\n\\nA) AngioTC helicoidal con contraste.\\nB) Gammagraf√≠a pulmonar de perfusi√≥n.\\nC) Radiograf√≠a de t√≥rax.\\nD) Ecocardiograf√≠a.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q201              1                     claude-3-7-sonnet-20250219              B             A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nEn la enfermedad por ara√±azo de gato, se√±ale la INCORRECTA:\\n\\nA) En los casos sin manifestaciones sist√©micas de la enfermedad es suficiente con tratamiento t√≥pico.\\nB) Para el diagn√≥stico es necesaria la pr√°ctica de una serolog√≠a.\\nC) Si se precisa tratamiento antibi√≥tico, se recomienda azitromicina 500 mg. en una dosis inicial y 250 mg cada 24 horas durante 4 d√≠as m√°s.\\nD) La mayor√≠a de los casos curan de forma espont√°nea en semanas o meses sin tratamiento de ning√∫n tipo.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "MIR-2024-v01-t01-Q204              1                             o3-mini-2025-01-31              C             D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°ss seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\nGestante de 36 a√±os primigesta sin factores de riesgo excepto √≠ndice de masa corporal de 32 pregestacional, que acude a urgencias a las 28 semanas y presenta una rotura prematura de membranas. En el momento del ingreso la anal√≠tica es normal, no presenta din√°mica uterina y la longitud cervical es de 38 mm. ¬øCu√°l de las siguientes afirmaciones es INCORRECTA?\\n\\nA) Iniciaremos maduraci√≥n pulmonar fetal con corticoides.\\nB) Se realizar√° profilaxis de corioamnionitis mediante antibioterapia endovenosa.\\nC) Se iniciar√° neuroprotecci√≥n fetal.\\nD) Se iniciar√° profilaxis tromboemb√≥lica con heparina de bajo peso molecular.\\n\\nTu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We create two DataFrames:\n",
        "summary_df with one row per model, containing accuracy, scores, etc.\n",
        "details_df with one row per question per model, containing everything from question ID to correctness.\n",
        "We then display the summary and the first few detailed results to verify the content. (In a real Jupyter environment, display(df) will show a nice table. In a text environment or script, you might use print(df.to_string()) or df.head().) Review the summary to ensure metrics make sense, and review the details to spot-check that outputs and correctness are recorded as expected."
      ],
      "metadata": {
        "id": "3JqybaU1tKR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Export Results to CSV (Cell 8)\n",
        "Now that we have the results in DataFrames, we‚Äôll export them to CSV files, which can be used in external analysis tools like Excel or Tableau. We will create two CSV files:\n",
        "llm_eval_summary.csv ‚Äì containing the summary metrics per model.\n",
        "llm_eval_details.csv ‚Äì containing the detailed per-question results.\n",
        "These files will include headers and can be imported directly into Tableau or other tools."
      ],
      "metadata": {
        "id": "rdDMq8WKsMLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this, you should find two CSV files in your working directory:\n",
        "llm_eval_summary.csv ‚Äì with columns like model_name, accuracy (%), total_score, total_questions, tokens_used_total, avg_latency_sec.\n",
        "llm_eval_details.csv ‚Äì with columns like question_id, model_name, prompt, model_output, correct, latency, tokens_used.\n",
        "These can now be loaded into Tableau or any data analysis software for visualization."
      ],
      "metadata": {
        "id": "EW6I9m2ktTj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Using Tableau for Analysis of Results\n",
        "With the results exported, we can analyze and visualize the performance of the models. Below are step-by-step instructions to use Tableau (or a similar data visualization tool) to explore the data:\n",
        "Import the CSV files into Tableau: Open Tableau and connect to the llm_eval_summary.csv and llm_eval_details.csv files (you can import them separately or join/relate them on the model_name field if needed).\n",
        "Summary Dashboard (Model-Level): Using llm_eval_summary.csv, you can create a simple chart of model performance. For example:\n",
        "Create a bar chart with model_name on the x-axis and accuracy (%) on the y-axis to compare accuracy across models.\n",
        "Add labels to show the exact accuracy or score for each model bar.\n",
        "You could also include avg_latency_sec as a secondary metric (perhaps a separate chart or as a tooltip) to see the speed-accuracy tradeoff.\n",
        "Filter by Prompt or Question: Since all models used the same prompt strategy in this run, the summary is straightforward. If you had different prompt strategies or sets of questions, you could use filters. For instance, if prompt_strategy was a field, you could filter or color-code by it. Or using the detailed data, you could filter to a specific question to see all model answers for that one.\n",
        "Detailed Analysis (Question-Level): Using llm_eval_details.csv, you can analyze which questions were hardest:\n",
        "Create a view with question_id on one axis and perhaps the count of models that got it correct.\n",
        "For example, drag question_id to rows, and an aggregation of correct (treat correct as 0/1 values and take average or sum). Multiply by 100 to interpret as percentage of models correct. This will tell you the percent of models that answered each question correctly.\n",
        "Identify questions with low scores across models ‚Äì these are the hardest questions. You can highlight them or filter to the hardest 5 questions.\n",
        "You can also create a detail table showing each model‚Äôs answer (from model_output) for a given question by filtering question_id and listing model_name and model_output for context.\n",
        "Visualization Examples: You might create a dashboard with two charts ‚Äì one showing model accuracy comparison, and another showing a difficulty analysis of questions. Use color or annotations to highlight interesting findings (e.g., a particular model that outperforms others, or a question that stumped half the models).\n",
        "\n",
        "\n",
        "Example: A simple bar chart comparing model accuracy. In the figure above, each bar represents a model‚Äôs accuracy on the test set (e.g., GPT-4 achieved 100% on 3 questions, whereas a smaller FlanT5 model scored 66.7%). You can create similar charts in Tableau easily by dragging and dropping the accuracy (%) field for each model_name. Remember, you can use Tableau‚Äôs filters to focus on specific models or questions. For instance, a filter on model_name could let you compare any subset of models (e.g., comparing only GPT-4.5-Preview vs. Gemini-2.0), and a filter on question_id could let you inspect performance on individual questions. Note: Ensure that in Tableau, boolean fields like correct are treated appropriately (Tableau might import them as text \"TRUE\"/\"FALSE\"). You may want to create a calculated field like Correct (0/1) as IF [correct] THEN 1 ELSE 0 END for easier aggregation.\n",
        "9. Future Enhancements and Conclusion\n",
        "We designed this notebook to be modular and easy to extend. Here are a few ways you could build on this framework:\n",
        "Chain-of-Thought Prompting: Modify the format_prompt function or the evaluation loop to incorporate chain-of-thought (CoT) prompts (e.g., by asking the model to \"think step by step\" before answering, and then evaluating the final answer separately). You could then evaluate not just the final answer accuracy but also analyze the reasoning steps.\n",
        "Hallucination Detection: If the questions have definitive answers, any divergence in the model‚Äôs explanation could be flagged. You might extend the detailed logs with fields for whether the model‚Äôs explanation contains factual errors (this could be manual or via another automated checker).\n",
        "Additional Metrics: We tracked token usage and latency. You could also log prompt length or output length separately, or cost if using paid APIs (by multiplying token usage by cost per token).\n",
        "More Providers: You can easily add new model providers (Cohere, AI21, etc.) in the call_model function. Just include a new elif branch and use their SDK or HTTP calls.\n",
        "Finally, a note on the evaluator.py (if you have a separate script for evaluation):\n",
        "To support token and time tracking, ensure that evaluator.py captures the start and end time around model invocations (as we did with time.time() in call_model) and returns or logs the duration.\n",
        "Modify the evaluator to also return the model‚Äôs raw output and any usage stats if available. For example, if originally it only returned correctness, have it return a dict with keys like output, correct, tokens_used, latency.\n",
        "To make it compatible with multi-model comparison, you could refactor evaluator.py to accept a model config or identifier as a parameter, so it can be called in a loop for different models (similar to how we did with evaluate_model). It could also be extended to handle a list of models internally and produce a combined report.\n",
        "By implementing these modifications, the evaluation pipeline will be more robust and informative. The modular structure of this notebook should make such changes straightforward ‚Äì each component (prompt formatting, model calling, result aggregation) can be adjusted independently. Conclusion: You now have a complete pipeline to evaluate multiple LLMs side-by-side on standardized questions, with results ready for analysis. Feel free to experiment with different models (just update the registry file), add more questions, or tweak the prompt strategy. Happy evaluating!\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Un_97SjRKrww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_all_files(root_dir='.'):\n",
        "    print(\"Listing all files in the current directory and subdirectories:\\n\")\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            print(os.path.join(root, file))\n",
        "\n",
        "def list_results_files(results_dir=os.path.join(\"data\", \"results\")):\n",
        "    print(\"\\nFiles in the data/results directory:\")\n",
        "    if os.path.exists(results_dir):\n",
        "        for file in os.listdir(results_dir):\n",
        "            print(file)\n",
        "    else:\n",
        "        print(\"The data/results directory does not exist.\")\n",
        "\n",
        "# Run the functions\n",
        "list_all_files()\n",
        "list_results_files()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkjH7muhT88W",
        "outputId": "4acc9334-a003-405f-becd-c4229445076d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing all files in the current directory and subdirectories:\n",
            "\n",
            "./requirements.txt\n",
            "./README.md\n",
            "./.gitignore\n",
            "./LICENSE.md\n",
            "./test_evaluator.py\n",
            "./docs/csv-wdc.html\n",
            "./src/evaluator.py\n",
            "./src/utils.py\n",
            "./src/prompt_manager.py\n",
            "./src/questions-scraper/MELIDA_PDF_Scraper_V2.ipynb\n",
            "./src/questions-scraper/MELIDA_PDF_Scraper_V1.ipynb\n",
            "./src/questions-scraper/README.md\n",
            "./src/__pycache__/evaluator.cpython-311.pyc\n",
            "./.git/packed-refs\n",
            "./.git/FETCH_HEAD\n",
            "./.git/description\n",
            "./.git/COMMIT_EDITMSG\n",
            "./.git/config\n",
            "./.git/HEAD\n",
            "./.git/index\n",
            "./.git/logs/HEAD\n",
            "./.git/logs/refs/heads/main\n",
            "./.git/logs/refs/remotes/origin/main\n",
            "./.git/logs/refs/remotes/origin/HEAD\n",
            "./.git/info/exclude\n",
            "./.git/refs/heads/main\n",
            "./.git/refs/remotes/origin/main\n",
            "./.git/refs/remotes/origin/HEAD\n",
            "./.git/hooks/update.sample\n",
            "./.git/hooks/pre-merge-commit.sample\n",
            "./.git/hooks/prepare-commit-msg.sample\n",
            "./.git/hooks/push-to-checkout.sample\n",
            "./.git/hooks/commit-msg.sample\n",
            "./.git/hooks/pre-push.sample\n",
            "./.git/hooks/pre-applypatch.sample\n",
            "./.git/hooks/pre-commit.sample\n",
            "./.git/hooks/pre-receive.sample\n",
            "./.git/hooks/fsmonitor-watchman.sample\n",
            "./.git/hooks/post-update.sample\n",
            "./.git/hooks/applypatch-msg.sample\n",
            "./.git/hooks/pre-rebase.sample\n",
            "./.git/objects/96/287bfe825a757adaf8644e0512a414e1b8106a\n",
            "./.git/objects/pack/pack-ef269f086f6d30272afdc0f8948fa519621d9ec9.idx\n",
            "./.git/objects/pack/pack-ef269f086f6d30272afdc0f8948fa519621d9ec9.pack\n",
            "./.git/objects/92/65a526a90a22032a3fd508f6bebee74d921577\n",
            "./.git/objects/92/da5e5a71b2f14219228edcd68fc752b80f0822\n",
            "./.git/objects/38/65b73ac0805b863396364fd8206d62033a0ebe\n",
            "./.git/objects/cb/807ee92b535ce700a79e290f307190c17695bc\n",
            "./notebooks/MELIDA_Evaluator_V2_0.ipynb\n",
            "./notebooks/prompting_strategy_evaluator.ipynb\n",
            "./notebooks/prod_evaluation_runner.ipynb\n",
            "./notebooks/models.yaml\n",
            "./data/exports/full_evaluation_results_20250402_221350.csv\n",
            "./data/exports/full_evaluation_results_20250401_145845.csv\n",
            "./data/exports/most_failed_questions_20250402_221351.csv\n",
            "./data/questions/MIR-2024-v01-t01.json\n",
            "./data/questions/TEST-MIR-2024-v01-t01.json\n",
            "./data/questions/MIR-2024-v01-t01_filtered.json\n",
            "./data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.json\n",
            "./data/test_results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.json\n",
            "./data/answers/MIR-2024-v01-t01-answers.json\n",
            "./data/answers/TEST-MIR-2024-v01-t01-answers.json\n",
            "./data/answers/MIR-2024-v01-t01-answers-standardized.json\n",
            "./data/answers/MIR-2024-v01-t01.json\n",
            "./data/answers/TEST-MIR-2024-v01-t01-answers-standardized.json\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.csv\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.json\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.csv\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.csv\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.json\n",
            "./data/results/EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.csv\n",
            "./config/api_config_template.json\n",
            "./config/api_config.json\n",
            "./config/prompt_strategies.json\n",
            "\n",
            "Files in the data/results directory:\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-215233.csv\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.json\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-215240.csv\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-o3-mini-2025-01-31-20250402-220953.csv\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.json\n",
            "EVAL-MIR-2024-v01-t01_filtered-Prompt-001-claude-3-7-sonnet-20250219-20250402-221350.csv\n"
          ]
        }
      ]
    }
  ]
}