{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZEjuONdfog+d2AC/rBEBp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelida/MELIDA/blob/main/notebooks/MELIDA_Evaluator_V2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Model LLM Evaluation Notebook\n",
        "\n",
        "This notebook evaluates **multiple Large Language Models (LLMs)** on a set of standardized test questions. We will start by comparing a few models (2‚Äì3) and set up the code to **scale up to 10+ models** easily using an external registry file for model configuration. The evaluation will use a consistent prompt strategy and track performance metrics like accuracy, tokens used, and response time for each model. We‚Äôll also log detailed results per question and export the outcomes to CSV files for visualization (e.g., in Tableau).\n",
        "\n",
        "**Key features of this notebook:**\n",
        "- Uses an **external JSON/YAML file** (‚Äúmodel registry‚Äù) to define which models to evaluate and how to access them, so you can easily add/remove models without changing code.\n",
        "- Evaluates each model on a set of **standardized test questions** with a chosen prompt format (e.g., multiple-choice questions) ‚Äì currently using a zero-shot prompt asking for the best answer.\n",
        "- Records **metrics per model**: accuracy (percentage of questions answered correctly), total score (number of correct answers), number of tokens used (if available), and average response time.\n",
        "- Stores **detailed logs per question** and model, including question ID, model name, full input prompt, model‚Äôs output, whether it was correct, and latency.\n",
        "- Exports results to **CSV files** (summary and detailed) for external analysis. We‚Äôll include a guide on how to use these in Tableau to filter by model/prompt/question and create visualizations of accuracy and identify the hardest questions.\n",
        "- Modular code structure with clear comments and section headings, so you can identify and modify specific parts (e.g., to change the prompt strategy or add new evaluation features like chain-of-thought or hallucination detection in the future).\n",
        "\n"
      ],
      "metadata": {
        "id": "RFDg3bqrp5Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Cell:\n",
        "# Check Runtime & GPU Availability\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def check_runtime():\n",
        "    \"\"\"Check whether a GPU or TPU is available.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"‚úÖ GPU is enabled! Using: {gpu_name}\")\n",
        "    elif \"COLAB_TPU_ADDR\" in os.environ:\n",
        "        print(\"‚úÖ TPU is enabled!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: No GPU or TPU detected. Running on CPU.\")\n",
        "        print(\"üëâ Go to Runtime > Change runtime type > Select GPU/TPU\")\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check GPU details using nvidia-smi if available.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è `nvidia-smi` not found. No GPU detected.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è No GPU found.\")\n",
        "\n",
        "# Run the checks\n",
        "check_runtime()\n",
        "check_gpu()\n",
        "\n",
        "#  Clone repository and change working directory\n",
        "!rm -rf MELIDA  # Remove any existing copy (optional)\n",
        "!git clone https://github.com/armelida/MELIDA.git\n",
        "%cd MELIDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyrEgEQ-xVTk",
        "outputId": "ce0f3671-3b01-411f-94a5-e15197065168"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU is enabled! Using: Tesla T4\n",
            "Thu Apr  3 10:34:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "Cloning into 'MELIDA'...\n",
            "remote: Enumerating objects: 582, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 582 (delta 56), reused 18 (delta 5), pack-reused 480 (from 1)\u001b[K\n",
            "Receiving objects: 100% (582/582), 1.48 MiB | 21.59 MiB/s, done.\n",
            "Resolving deltas: 100% (327/327), done.\n",
            "/content/MELIDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0A: Load API Keys & Save API Configuration\n",
        "\n",
        "# Install necessary libraries if not already present\n",
        "!pip install -q python-dotenv google-generativeai # Added google-generativeai\n",
        "# Keep openai pinned if needed, but google-generativeai is separate\n",
        "!pip install openai==0.28\n",
        "\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Initialize API keys dictionary - Added 'google'\n",
        "api_keys = {\"openai\": None, \"anthropic\": None, \"together\": None, \"google\": None}\n",
        "\n",
        "# Try to load from Colab secrets using userdata - Added 'google'\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_keys[\"openai\"] = userdata.get('OPENAI_API_KEY')\n",
        "    api_keys[\"anthropic\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "    api_keys[\"together\"] = userdata.get('TOGETHER_API_KEY')\n",
        "    # --- Add Google API Key ---\n",
        "    api_keys[\"google\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    # ------------------------\n",
        "    # Update check condition\n",
        "    if all(v is not None for k, v in api_keys.items() if k != 'google'): # Check others first\n",
        "         print(\"‚úì API keys (OpenAI, Anthropic, Together) loaded from Colab secrets\")\n",
        "    if api_keys[\"google\"]:\n",
        "         print(\"‚úì Google API key loaded from Colab secrets\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Note: Couldn't load some keys from Colab secrets - {e}\")\n",
        "\n",
        "# Fallback: load from environment variables if not loaded yet - Added 'google'\n",
        "if not all(api_keys.values()): # Check if *any* are missing\n",
        "    api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "    # --- Add Google API Key ---\n",
        "    api_keys[\"google\"] = api_keys[\"google\"] or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    # ------------------------\n",
        "    # Update log message if needed\n",
        "    loaded_from_env = [k for k, v in api_keys.items() if v and not userdata.get(k.upper()+'_API_KEY')] # Rough check\n",
        "    if loaded_from_env:\n",
        "        print(f\"‚úì API keys loaded from environment variables for: {', '.join(loaded_from_env)}\")\n",
        "\n",
        "\n",
        "# Fallback: load from a .env file if still missing - Added 'google'\n",
        "if not all(api_keys.values()):\n",
        "    try:\n",
        "        load_dotenv() # This will load variables from a .env file\n",
        "        api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "        api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "        # --- Add Google API Key ---\n",
        "        api_keys[\"google\"] = api_keys[\"google\"] or os.environ.get(\"GOOGLE_API_KEY\")\n",
        "        # ------------------------\n",
        "        # Update log message if needed\n",
        "        loaded_from_dotenv = [k for k, v in api_keys.items() if v and not userdata.get(k.upper()+'_API_KEY') and k not in loaded_from_env] # Rough check\n",
        "        if loaded_from_dotenv:\n",
        "             print(f\"‚úì API keys loaded from .env file for: {', '.join(loaded_from_dotenv)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Couldn't load from .env file - {e}\")\n",
        "\n",
        "# Propagate keys to os.environ so subsequent cells can access them - Added 'google'\n",
        "if api_keys[\"openai\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_keys[\"openai\"]\n",
        "if api_keys[\"anthropic\"]:\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_keys[\"anthropic\"]\n",
        "if api_keys[\"together\"]:\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = api_keys[\"together\"]\n",
        "# --- Add Google API Key ---\n",
        "if api_keys[\"google\"]:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_keys[\"google\"]\n",
        "# ------------------------\n",
        "\n",
        "# Save API configuration to a JSON file for future reference\n",
        "os.makedirs('config', exist_ok=True)\n",
        "api_config = {\n",
        "    \"openai\": {\"api_key\": api_keys[\"openai\"] or \"YOUR_OPENAI_API_KEY_HERE\"},\n",
        "    \"anthropic\": {\"api_key\": api_keys[\"anthropic\"] or \"YOUR_ANTHROPIC_API_KEY_HERE\"},\n",
        "    \"together\": {\"api_key\": api_keys[\"together\"] or \"YOUR_TOGETHER_API_KEY_HERE\"},\n",
        "    \"google\": {\"api_key\": api_keys[\"google\"] or \"YOUR_GOOGLE_API_KEY_HERE\"}\n",
        "}\n",
        "with open('config/api_config.json', 'w') as f:\n",
        "    json.dump(api_config, f, indent=2)\n",
        "\n",
        "# Report missing keys, if any\n",
        "missing = []\n",
        "if not api_keys[\"openai\"]: missing.append(\"OpenAI\")\n",
        "if not api_keys[\"anthropic\"]: missing.append(\"Anthropic\")\n",
        "if not api_keys[\"together\"]: missing.append(\"Together\")\n",
        "if not api_keys[\"google\"]: missing.append(\"Google\")\n",
        "if missing:\n",
        "    print(f\"‚ö†Ô∏è Missing API keys: {', '.join(missing)}\")\n",
        "    print(\"üëâ Please set the necessary API keys using Colab secrets (recommended), environment variables, or a .env file.\")\n",
        "else:\n",
        "    print(\"‚úì All required API configurations saved/loaded.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5_dSkv97Efr",
        "outputId": "bd2fe4e7-62df-42a7-8976-7c98f74dfc68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation (Cell 1)\n",
        "\n",
        "First, we install and import necessary libraries. This includes:\n",
        "- **Hugging Face Transformers** for local model inference (if using HuggingFace-hosted models).\n",
        "- **OpenAI/Anthropic API SDKs** (if using direct APIs like OpenAI‚Äôs GPT or Anthropic‚Äôs Claude).\n",
        "- **Together AI** client (if using the Together API for hosted models).\n",
        "- **PyYAML** (for reading YAML config) and **pandas** (for data manipulation and CSV export).\n",
        "\n",
        "We will also ensure any required API keys are set (for OpenAI, Anthropic, Together, etc.) via environment variables for security. Replace or set these environment variables before running the evaluation.\n"
      ],
      "metadata": {
        "id": "xiu37WIKrI_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMnoEmHYo81v"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Setup environment and install required packages\n",
        "\n",
        "# Install required packages - Added google-generativeai\n",
        "!pip install -q pandas PyYAML openai anthropic together transformers google-generativeai\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# Import model API clients\n",
        "import openai\n",
        "# Ensure your OpenAI API key is available (loaded in Cell 0A)\n",
        "\n",
        "import anthropic\n",
        "# Anthropic client might be initialized later in call_model or globally here if preferred\n",
        "\n",
        "import together\n",
        "# Together client might be initialized later in call_model or globally here if preferred\n",
        "\n",
        "# --- Add Google Generative AI Import ---\n",
        "import google.generativeai as genai\n",
        "# --------------------------------------\n",
        "\n",
        "# (Optional) If using Hugging Face transformers for local models:\n",
        "try:\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "except ImportError:\n",
        "    !pip install -q transformers # Ensure transformers is installed if needed\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Setup complete. Libraries loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configure Model Registry (Cell 2)\n",
        "We use an external model registry file (JSON or YAML) to list all models to evaluate and their access details. This file allows easy scaling to more models ‚Äì just add or remove entries without changing the notebook code. Each model entry can include:\n",
        "name: A human-readable name for the model (used in results and plots).\n",
        "provider: The method to access the model (huggingface, together, openai, anthropic, etc.).\n",
        "model_id: Identifier for the model:\n",
        "For huggingface, this is the model‚Äôs name on HuggingFace Hub (e.g., \"google/flan-t5-small\").\n",
        "For together, it might be a model ID known to the Together API (e.g., \"meta-llama/Llama-2-7b-chat-hf\").\n",
        "For openai, it could be the API model name (e.g., \"gpt-4\" or \"gpt-3.5-turbo\").\n",
        "For other providers, use the appropriate identifier.\n",
        "api_key_env (if needed): The environment variable name for the API key (e.g., \"OPENAI_API_KEY\"). This can be omitted for HuggingFace (if using local models or if no auth needed).\n",
        "Additional settings like max_tokens, temperature, etc., which define generation parameters for that model.\n",
        "Example model registry (YAML format):\n",
        "\n",
        "models:\n",
        "  - name: FlanT5 Small\n",
        "    provider: huggingface\n",
        "    model_id: google/flan-t5-small\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: GPT-4 (Jan 2025)\n",
        "    provider: openai\n",
        "    model_id: gpt-4\n",
        "    api_key_env: OPENAI_API_KEY\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: Llama2 7B Chat\n",
        "    provider: together\n",
        "    model_id: meta-llama/Llama-2-7b-chat-hf\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "\n",
        "In the code below, we load the model list from the registry file. Update model_config_path to point to your JSON/YAML file. The code will automatically detect JSON vs YAML based on file extension and parse accordingly. After loading, it prints out the model configurations to confirm."
      ],
      "metadata": {
        "id": "1_unmmk1qxdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load model registry from external JSON/YAML file\n",
        "\n",
        "# Set the path to your model registry file in the notebooks folder\n",
        "model_config_path = \"notebooks/models.yaml\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(model_config_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Model config file not found at {model_config_path}. \"\n",
        "        \"Please create it as per the example and update the path.\"\n",
        "    )\n",
        "\n",
        "# Parse the config file (supports YAML and JSON)\n",
        "if model_config_path.endswith((\".yaml\", \".yml\")):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = yaml.safe_load(f)\n",
        "elif model_config_path.endswith(\".json\"):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported config file format. Use .json or .yaml\")\n",
        "\n",
        "# The config should either be a dict with a top-level 'models' key or a list itself.\n",
        "if isinstance(config_data, dict) and \"models\" in config_data:\n",
        "    models_config = config_data[\"models\"]\n",
        "elif isinstance(config_data, list):\n",
        "    models_config = config_data\n",
        "else:\n",
        "    raise ValueError(\"Config file format error: expected a list of models or a 'models' key.\")\n",
        "\n",
        "print(f\"Loaded {len(models_config)} models from registry:\")\n",
        "for m in models_config:\n",
        "    print(f\" - {m.get('name', 'Unnamed')} ({m.get('provider', 'Unknown')}, id={m.get('model_id', 'N/A')})\")\n"
      ],
      "metadata": {
        "id": "z8Zg9195pKkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load Standardized Test Questions (Cell 3)\n",
        "Next, we prepare the standardized test questions for evaluation. These can be hard-coded, loaded from a file, or generated. In this notebook, we‚Äôll define a list of questions in a structured format (each with an ID, question text, multiple-choice options, and the correct answer). You can replace these with any set of questions relevant to your use case. For demonstration, we‚Äôll use a few simple sample questions. In a real scenario, you might load dozens of questions from a JSON/CSV file or an existing dataset. Ensure each question has a known correct answer to compute accuracy."
      ],
      "metadata": {
        "id": "qa5t7ftJrBM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load extracted exam questions for evaluation\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Define the path to the exported questions file.\n",
        "# Adjust this path if your exported file name or location is different.\n",
        "questions_file = \"data/questions/MIR-2024-v01-t01.json\"\n",
        "\n",
        "if not os.path.exists(questions_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Questions file not found at {questions_file}. \"\n",
        "        \"Please run the extraction process to generate the questions file.\"\n",
        "    )\n",
        "\n",
        "with open(questions_file, 'r', encoding='utf-8') as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(questions)} questions for evaluation.\")\n",
        "\n",
        "# Optionally, preview the first three questions\n",
        "for q in questions[:3]:\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print(f\"ID: {q['id']}\")\n",
        "    print(f\"Question: {q['question_text']}\")\n",
        "    print(\"Options:\")\n",
        "    for key, value in q['options'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"---------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "1ljgjq5bpKqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We defined a list of dictionaries, where each dictionary represents a question. Each question has:\n",
        "id: a unique identifier,\n",
        "question: the text of the question,\n",
        "choices: a list of answer choices (as strings, each prefixed with a letter),\n",
        "answer: the correct choice (here represented by the letter of the correct option).\n",
        "Feel free to extend or replace this list. For example, you could load questions from a file or generate them. Just ensure each question has a known correct answer for scoring."
      ],
      "metadata": {
        "id": "ycbGse8qrO6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define Prompt Strategy (Cell 4)\n",
        "The prompt strategy determines how we present each question to the model. For standardized test questions, a common strategy is to provide the question and multiple-choice options and ask the model to pick the best answer. We might instruct the model to output just the option letter (to make it easier to check correctness). In future, you could experiment with different strategies (e.g., asking for an explanation, chain-of-thought prompting, etc.). For now, we‚Äôll use a straightforward prompt: the question, the options, and a final instruction like ‚ÄúAnswer with the letter of the correct option.‚Äù We implement this as a function format_prompt(question) that takes a question entry and returns the full prompt text (or structured prompt) to send to the model. This function can be easily modified if you want to change how prompts are constructed."
      ],
      "metadata": {
        "id": "oWi8tcY7rVM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define the prompt formatting strategy\n",
        "\n",
        "def format_prompt(q):\n",
        "    \"\"\"\n",
        "    Format a question dictionary into a prompt string for the MIR exam in Spanish.\n",
        "\n",
        "    The prompt instructs the model to answer with a single letter (A, B, C, D) if it knows the answer,\n",
        "    or with N if unsure.\n",
        "    \"\"\"\n",
        "    # Get the main question text\n",
        "    question_text = q.get(\"question_text\", \"\")\n",
        "\n",
        "    # Get each answer option from the options dictionary (default to empty string if missing)\n",
        "    options = q.get(\"options\", {})\n",
        "    option_a = options.get(\"A\", \"\")\n",
        "    option_b = options.get(\"B\", \"\")\n",
        "    option_c = options.get(\"C\", \"\")\n",
        "    option_d = options.get(\"D\", \"\")\n",
        "\n",
        "    # Build the prompt using the provided format\n",
        "    prompt = (\n",
        "        \"Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. \"\n",
        "        \"Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. \"\n",
        "        \"Si no est√°s seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\n\"\n",
        "        f\"{question_text}\\n\\n\"\n",
        "        f\"A) {option_a}\\n\"\n",
        "        f\"B) {option_b}\\n\"\n",
        "        f\"C) {option_c}\\n\"\n",
        "        f\"D) {option_d}\\n\\n\"\n",
        "        \"Tu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# Test the prompt formatting on the first loaded question\n",
        "example_prompt = format_prompt(questions[0])\n",
        "print(\"Example formatted prompt:\\n\", example_prompt)\n"
      ],
      "metadata": {
        "id": "toS2gKl3pKtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: The format_prompt function takes a question from our list and builds a prompt. We put the question text, list all the choices (joined in one line for simplicity), and then give an explicit instruction. By asking for the letter only, we aim to have consistent outputs that are easy to check (the model hopefully will just respond with ‚ÄúB‚Äù, etc.). After defining the function, we preview an example prompt for the first question to verify the format. You can adjust this format as needed (for instance, if a model tends to do better with a different phrasing or if you want the model to explain its answer, etc.)."
      ],
      "metadata": {
        "id": "5-XAXQV_rgfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import openai\n",
        "import anthropic\n",
        "import together\n",
        "import google.generativeai as genai\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import requests  # Required for Together API HTTP calls\n",
        "\n",
        "def call_model(model_cfg, prompt):\n",
        "    \"\"\"\n",
        "    Call a model with the given prompt and return its response and metadata.\n",
        "    Returns: output_text, tokens_used, latency\n",
        "    \"\"\"\n",
        "    provider = model_cfg.get(\"provider\", \"\").lower()\n",
        "    model_id = model_cfg.get(\"model_id\")\n",
        "    api_key_env = model_cfg.get(\"api_key_env\")  # Environment variable for API key\n",
        "    max_tokens = model_cfg.get(\"max_tokens\", 2000)  # Default: 2000 tokens\n",
        "    temperature = model_cfg.get(\"temperature\", 0.7)   # Default temperature\n",
        "    tokens_used = None\n",
        "    output_text = \"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # HuggingFace branch\n",
        "    if provider == \"huggingface\":\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "            # Decide pipeline type based on model name\n",
        "            if \"google/flan\" in model_id.lower() or \"t5\" in model_id.lower():\n",
        "                pipe = pipeline(\"text2text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "                result = pipe(prompt, max_length=max_tokens, temperature=temperature if temperature > 0 else None, do_sample=temperature > 0)\n",
        "                output_text = result[0]['generated_text']\n",
        "            else:\n",
        "                pipe = pipeline(\"text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "                result = pipe(prompt, max_new_tokens=max_tokens, temperature=temperature if temperature > 0 else None, do_sample=temperature > 0)\n",
        "                output_text = result[0]['generated_text']\n",
        "                if output_text.startswith(prompt):\n",
        "                    output_text = output_text[len(prompt):]\n",
        "            # Optional token counting\n",
        "            try:\n",
        "                input_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "                output_tokens = tokenizer(output_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "                tokens_used = int(len(input_tokens[0]) + len(output_tokens[0]))\n",
        "            except Exception:\n",
        "                tokens_used = None\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR calling HuggingFace model {model_id}: {e}\")\n",
        "            output_text = f\"ERROR: {e}\"\n",
        "\n",
        "    # OpenAI branch\n",
        "    elif provider == \"openai\":\n",
        "        api_key = os.getenv(api_key_env or \"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            output_text = f\"ERROR: OpenAI API key ({api_key_env or 'OPENAI_API_KEY'}) not set.\"\n",
        "        else:\n",
        "            openai.api_key = api_key\n",
        "            try:\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                output_text = response['choices'][0]['message']['content'].strip()\n",
        "                if 'usage' in response:\n",
        "                    tokens_used = response['usage'].get('total_tokens')\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR calling OpenAI model {model_id}: {e}\")\n",
        "                output_text = f\"ERROR: {e}\"\n",
        "\n",
        "    # Anthropic branch\n",
        "    elif provider == \"anthropic\":\n",
        "        api_key = os.getenv(api_key_env or \"ANTHROPIC_API_KEY\")\n",
        "        if not api_key:\n",
        "            output_text = f\"ERROR: Anthropic API key ({api_key_env or 'ANTHROPIC_API_KEY'}) not set.\"\n",
        "        else:\n",
        "            try:\n",
        "                anthropic_client = anthropic.Client(api_key=api_key)\n",
        "                response = anthropic_client.messages.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                if response.content and isinstance(response.content, list):\n",
        "                    output_text = response.content[0].text.strip()\n",
        "                else:\n",
        "                    output_text = str(response.completion).strip()\n",
        "                if hasattr(response, 'usage'):\n",
        "                    tokens_used = response.usage.input_tokens + response.usage.output_tokens\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR calling Anthropic model {model_id}: {e}\")\n",
        "                output_text = f\"ERROR: {e}\"\n",
        "\n",
        "    # Together branch (updated to use HTTP requests)\n",
        "    elif provider == \"together\":\n",
        "        api_key = os.getenv(api_key_env or \"TOGETHER_API_KEY\")\n",
        "        if not api_key:\n",
        "            output_text = f\"ERROR: Together API key ({api_key_env or 'TOGETHER_API_KEY'}) not set.\"\n",
        "        else:\n",
        "            try:\n",
        "                endpoint = \"https://api.together.xyz/v1/chat/completions\"\n",
        "                headers = {\n",
        "                    \"Authorization\": f\"Bearer {api_key}\",\n",
        "                    \"Content-Type\": \"application/json\"\n",
        "                }\n",
        "                # Construct messages with a system prompt to enforce a one-letter answer\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a precise question-answering system. Respond with only one letter: A, B, C, or D. No extra text is allowed.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "                payload = {\n",
        "                    \"model\": model_id,  # e.g., \"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\"\n",
        "                    \"messages\": messages,\n",
        "                    \"max_tokens\": max_tokens,\n",
        "                    \"temperature\": temperature,\n",
        "                    \"top_p\": 0.7,\n",
        "                    \"top_k\": 50,\n",
        "                    \"repetition_penalty\": 1,\n",
        "                    \"stop\": [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
        "                    \"stream\": False\n",
        "                }\n",
        "                response = requests.post(endpoint, headers=headers, json=payload, timeout=30)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                if \"error\" in data:\n",
        "                    raise ValueError(f\"API Error: {data['error']}\")\n",
        "                if \"choices\" in data and len(data[\"choices\"]) > 0:\n",
        "                    output_text = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "                else:\n",
        "                    output_text = \"ERROR: Unexpected response structure from Together API\"\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR calling Together model {model_id}: {e}\")\n",
        "                output_text = f\"ERROR: {e}\"\n",
        "\n",
        "    # Google branch\n",
        "    elif provider == \"google\":\n",
        "        api_key = os.getenv(api_key_env or \"GOOGLE_API_KEY\")\n",
        "        if not api_key:\n",
        "            output_text = f\"ERROR: Google API key ({api_key_env or 'GOOGLE_API_KEY'}) not set.\"\n",
        "        else:\n",
        "            try:\n",
        "                genai.configure(api_key=api_key)\n",
        "                generation_config = genai.types.GenerationConfig(\n",
        "                    max_output_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                safety_settings = [\n",
        "                    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                ]\n",
        "                model = genai.GenerativeModel(\n",
        "                    model_name=model_id,\n",
        "                    generation_config=generation_config,\n",
        "                    safety_settings=safety_settings\n",
        "                )\n",
        "                response = model.generate_content(prompt)\n",
        "                if response.parts:\n",
        "                    output_text = response.text\n",
        "                elif response.prompt_feedback.block_reason:\n",
        "                    output_text = f\"ERROR: Blocked by safety filter - Reason: {response.prompt_feedback.block_reason}\"\n",
        "                    print(f\"WARNING: Call to {model_id} blocked. Reason: {response.prompt_feedback.block_reason}\")\n",
        "                else:\n",
        "                    output_text = \"ERROR: No content generated (unknown reason)\"\n",
        "                if hasattr(response, 'usage_metadata'):\n",
        "                    tokens_used = response.usage_metadata.prompt_token_count + response.usage_metadata.candidates_token_count\n",
        "                else:\n",
        "                    tokens_used = None\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR calling Google model {model_id}: {e}\")\n",
        "                output_text = f\"ERROR: {e}\"\n",
        "    else:\n",
        "        output_text = f\"ERROR: Unknown provider '{provider}' for model {model_cfg.get('name', model_id)}\"\n",
        "\n",
        "    latency = time.time() - start_time\n",
        "    if not isinstance(output_text, str):\n",
        "        output_text = str(output_text)\n",
        "    return output_text.strip(), tokens_used, latency\n",
        "\n",
        "# --------------------------\n",
        "# Example usage of the call_model function:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: testing the Together branch\n",
        "    model_cfg = {\n",
        "        \"provider\": \"together\",\n",
        "        \"model_id\": \"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",  # Replace with your model ID if needed\n",
        "        \"api_key_env\": \"TOGETHER_API_KEY\",\n",
        "        \"max_tokens\": 2000,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    prompt = (\"What is the capital of France? \"\n",
        "              \"A) Paris \"\n",
        "              \"B) London \"\n",
        "              \"C) Berlin \"\n",
        "              \"D) Madrid\")\n",
        "    output, tokens_used, latency = call_model(model_cfg, prompt)\n",
        "    print(\"Output:\", output)\n",
        "    print(\"Tokens used:\", tokens_used)\n",
        "    print(\"Latency:\", latency)\n"
      ],
      "metadata": {
        "id": "59_SJq6jrgDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Interface and Evaluation Functions (Cells 5‚Äì6)\n",
        "In this section, we set up functions to handle model inference and evaluation:\n",
        "call_model(model_config, prompt_text) ‚Äì Invokes a single model (based on its provider and config) with the given prompt, and returns the model‚Äôs answer, along with metadata like token usage and latency.\n",
        "evaluate_model(model_config, questions) ‚Äì Uses call_model to get answers for each question from one model, checks correctness, and collects detailed results.\n",
        "We will also prepare a loop or another function to evaluate all models and aggregate the results for comparison.\n",
        "Structuring this logic into functions makes the notebook modular and easier to update. For example, if in the future we want to add a step for chain-of-thought (CoT) prompting or filter the model output for hallucinations, we could modify or wrap call_model accordingly. 5.1 call_model Implementation: This function will branch based on the provider:\n",
        "HuggingFace: use transformers pipeline or model generate. We‚Äôll initialize a pipeline for text generation or use the model‚Äôs generate method. We also tokenize the input to count input tokens. The output tokens can be counted by the tokenizer as well.\n",
        "OpenAI: use openai.Completion or openai.ChatCompletion depending on model type. For chat models (e.g., GPT-4), we pass the prompt as a user message. We retrieve the output text and usage info (token counts).\n",
        "Anthropic: (Claude models) use anthropic‚Äôs client. Typically you provide a prompt with a special format (like \"\\n\\nHuman: <question>\\n\\nAssistant:\"). We skip detailed implementation here but it can be added.\n",
        "Together: use Together API client. For example, together_client.complete or the chat completion as needed, based on their documentation. (Ensure TOGETHER_API_KEY is set.)\n",
        "Additional providers (e.g., Cohere, AI21) can be integrated similarly by adding new branches.\n",
        "We also measure the time taken for each call (latency). If token counts are not readily available from the API, we will set them to None (or you could estimate via a tokenizer). Let‚Äôs implement call_model below:"
      ],
      "metadata": {
        "id": "626XG9mwrmbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In call_model:\n",
        "We take the model‚Äôs config and a prompt string.\n",
        "Based on provider, we handle the call differently.\n",
        "HuggingFace: We load the model and tokenizer (from local or HuggingFace Hub). We use pipeline for simplicity (it will handle the model loading and generation). We choose the pipeline task based on model type (a quick check for ‚Äút5‚Äù in the model name to decide between text2text-generation and text-generation). After generation, we count tokens by encoding the prompt and output with the tokenizer.\n",
        "OpenAI: We use the OpenAI API. If the model is chat-based (we guess by name containing ‚Äúgpt-3.5‚Äù or ‚Äúgpt-4‚Äù), we use the ChatCompletion endpoint with a single user message. Otherwise, we use the older Completion endpoint. We fetch the text from the response and also get token usage if provided. (Make sure your OpenAI API key is set in the environment.)\n",
        "Anthropic: We format the prompt in the required way for Claude and call the client‚Äôs completion method. (This assumes the anthropic package is installed and imported.) Token count isn‚Äôt directly captured here.\n",
        "Together: We initialize the Together client and call the chat.completions.create method with the prompt as a user message. (This assumes the model supports chat format; for pure text-generation models on Together, you might use a different method like client.completion.create.) We extract the content from the response. (Token usage may be available via Together‚Äôs response, but for simplicity, we set it to None in this example.)\n",
        "We measure the time just before and after the call to compute latency.\n",
        "Finally, we return output_text (the model‚Äôs answer), tokens_used, and latency.\n",
        "This function abstracts away the differences in model access, giving us a unified interface for the evaluation loop.\n",
        "\n",
        "5.2 evaluate_model Implementation: This function will loop through all questions for a single model, use call_model to get the answer, check correctness, and record results. It will return a list of result records (one per question for that model) and also compute summary metrics (like number correct). We‚Äôll implement evaluate_model next:"
      ],
      "metadata": {
        "id": "lzix9vk7rwpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  6\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Define file paths\n",
        "original_questions_file = 'data/questions/MIR-2024-v01-t01.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "\n",
        "# Check if files exist before proceeding\n",
        "if not os.path.exists(original_questions_file):\n",
        "     raise FileNotFoundError(f\"Original questions file not found: {original_questions_file}\")\n",
        "if not os.path.exists(answer_key_file):\n",
        "     raise FileNotFoundError(f\"Answer key file not found: {answer_key_file}\")\n",
        "\n",
        "# Load answer key (assumed to be a dictionary with question IDs as keys)\n",
        "with open(answer_key_file, 'r', encoding='utf-8') as f:\n",
        "     answer_key = json.load(f)\n",
        "\n",
        "# Load original questions (assumed to be a list)\n",
        "with open(original_questions_file, 'r', encoding='utf-8') as f:\n",
        "     questions = json.load(f)\n",
        "\n",
        "# Filter out questions that are not in the answer key or whose answer is empty\n",
        "filtered_questions = [\n",
        "     q for q in questions\n",
        "     if q.get('id') in answer_key and answer_key[q.get('id')].strip() != \"\"\n",
        "]\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(filtered_questions_file), exist_ok=True)\n",
        "\n",
        "# Save filtered questions to a new file\n",
        "with open(filtered_questions_file, 'w', encoding='utf-8') as f:\n",
        "     json.dump(filtered_questions, f, indent=2)\n",
        "\n",
        "print(f\"Filtered questions saved to {filtered_questions_file} with {len(filtered_questions)} questions out of {len(questions)} original questions.\")"
      ],
      "metadata": {
        "id": "rkL2jAQjrdH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6.1: Patch OpenAI to support legacy calls in evaluator.py and reload evaluator module\n",
        "\n",
        "!pip install openai==0.28\n",
        "\n",
        "import openai\n",
        "\n",
        "# Define a wrapper class so that openai.chat.completions.create(...) works.\n",
        "class ChatCompletionsWrapper:\n",
        "    @staticmethod\n",
        "    def create(*args, **kwargs):\n",
        "        return openai.ChatCompletion.create(*args, **kwargs)\n",
        "\n",
        "class OpenAIChatWrapper:\n",
        "    completions = ChatCompletionsWrapper\n",
        "\n",
        "# Assign our wrapper to openai.chat\n",
        "openai.chat = OpenAIChatWrapper\n",
        "\n",
        "# Verify the patch:\n",
        "print(\"openai.chat.completions.create:\", openai.chat.completions.create)\n",
        "\n",
        "# Reload evaluator so that it picks up our patched openai\n",
        "import importlib\n",
        "import src.evaluator as evaluator_module\n",
        "importlib.reload(evaluator_module)\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "print(\"OpenAI version:\", openai.__version__)\n"
      ],
      "metadata": {
        "id": "C3Uqr1dYpKxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.2\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "def extract_answer_letter(output):\n",
        "    \"\"\"\n",
        "    Extracts the first valid answer letter (A, B, C, D, or N) from the model output.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\b([ABCDN])\\b', output.upper())\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "# Parameters for test evaluation\n",
        "models_to_test = [\n",
        "    'o3-mini-2025-01-31',\n",
        "    'deepseek-ai/DeepSeek-V3',\n",
        "    'claude-3-7-sonnet-20250219',\n",
        "    'deepseek-ai/DeepSeek-R1',\n",
        "    'Google-Gemini-gemini-1.5-pro',\n",
        "    'meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo',\n",
        "    'meta-llama/Llama-3.3-70B-Instruct-Turbo',\n",
        "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
        "    'mistralai/Mixtral-8x22B-Instruct-v0.1',\n",
        "    'Qwen/Qwen2-VL-72B-Instruct',\n",
        "    'grok-2-latest'\n",
        "]\n",
        "prompt_strategy = \"Prompt-001\"\n",
        "sample_size = 5  # Evaluate 5 questions per model\n",
        "\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "\n",
        "# Define a separate folder for test evaluation results\n",
        "test_results_folder = \"data/test_results\"\n",
        "os.makedirs(test_results_folder, exist_ok=True)\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "results_summary = {}\n",
        "\n",
        "print(\"\\n--- RUNNING SAMPLE TEST EVALUATION (Results saved in a separate folder) ---\")\n",
        "for model in models_to_test:\n",
        "    print(f\"\\nEvaluating Model: {model} using Prompt Strategy: {prompt_strategy}\")\n",
        "    try:\n",
        "        # Run evaluation; if evaluator.run_evaluation supports an output_folder parameter, use it.\n",
        "        # Otherwise, move the result file to the test folder after creation.\n",
        "        result_file = evaluator.run_evaluation(\n",
        "            questions_file=filtered_questions_file,\n",
        "            answer_key_file=answer_key_file,\n",
        "            prompt_strategy=prompt_strategy,\n",
        "            model=model,\n",
        "            sample_size=sample_size\n",
        "            # , output_folder=test_results_folder  # Uncomment if supported by your evaluator\n",
        "        )\n",
        "        # If output_folder parameter is not supported, move the file manually:\n",
        "        new_result_file = os.path.join(test_results_folder, os.path.basename(result_file))\n",
        "        os.rename(result_file, new_result_file)\n",
        "        result_file = new_result_file\n",
        "\n",
        "        print(f\"‚úì Sample evaluation complete for {model}. Results saved to: {result_file}\")\n",
        "\n",
        "        with open(result_file, 'r') as f:\n",
        "            results = json.load(f)\n",
        "\n",
        "        details = results.get(\"details\", [])\n",
        "        if details:\n",
        "            print(\"\\n--- DETAILS FOR EACH QUESTION ---\")\n",
        "            for entry in details:\n",
        "                question_id = entry.get(\"question_id\", \"N/A\")\n",
        "                question_prompt = entry.get(\"prompt\", \"No prompt available\")\n",
        "                raw_output = entry.get(\"model_output\", \"No output\")\n",
        "                extracted = extract_answer_letter(raw_output)\n",
        "                entry[\"extracted_answer\"] = extracted\n",
        "                print(f\"Question ID: {question_id}\")\n",
        "                print(\"Prompt:\")\n",
        "                print(question_prompt)\n",
        "                print(\"Raw Output:\")\n",
        "                print(raw_output)\n",
        "                print(\"Extracted Answer:\", extracted)\n",
        "                print(\"-\" * 40)\n",
        "        else:\n",
        "            print(\"\\nNo per-question details found in the evaluation results.\")\n",
        "\n",
        "        summary = results.get(\"summary\", {})\n",
        "        results_summary[model] = summary\n",
        "\n",
        "        print(\"\\n--- SAMPLE EVALUATION SUMMARY ---\")\n",
        "        print(f\"Model: {summary.get('model', 'N/A')}\")\n",
        "        print(f\"Prompt Strategy: {summary.get('prompt_strategy', 'N/A')}\")\n",
        "        print(f\"Total Questions: {summary.get('total_questions', 'N/A')}\")\n",
        "        print(f\"Correct Answers: {summary.get('correct_count', 'N/A')} ({summary.get('accuracy', 0)*100:.2f}%)\")\n",
        "        print(f\"Incorrect Answers: {summary.get('incorrect_count', 'N/A')}\")\n",
        "        print(f\"Skipped Questions: {summary.get('skipped_count', 'N/A')}\")\n",
        "        print(f\"Invalid Count: {summary.get('invalid_count', 'N/A')}\")\n",
        "        print(f\"Total Score: {summary.get('total_score', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error during sample evaluation for {model}: {e}\")\n",
        "\n",
        "print(\"\\nIf the sample evaluation looks good, proceed to full evaluation in the next cell.\")\n"
      ],
      "metadata": {
        "id": "Jg_k5kIJ3RKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In evaluate_model:\n",
        "We iterate over each question, format the prompt, and call the model via call_model.\n",
        "We wrap the model call in a try-except to catch any errors (for instance, if an API call fails or a model isn‚Äôt available). If there‚Äôs an error, we log it and move on, leaving output empty for that question.\n",
        "We then parse the model‚Äôs output to extract the answer. We assume the model should reply with a letter. The code checks the first character of the output: if it‚Äôs one of ‚ÄúA, B, C, D‚Äù, we treat that as the chosen option. (If the output is something else, you could include additional parsing logic ‚Äì for example, sometimes the model might output the full option text or a sentence. Here, we simplify by taking the first letter when possible. If the output is empty or doesn‚Äôt start with a letter, we mark the answer as incorrect by default.)\n",
        "We compare the model‚Äôs answer letter (uppercased) to the true answer letter from the question. If they match, it‚Äôs correct and we increment correct_count.\n",
        "We append a dictionary to results containing all relevant info: question ID, model name, the exact prompt used, the model‚Äôs raw output, a boolean for correctness, latency (in seconds), and tokens used.\n",
        "We also print a one-line progress update for each question, indicating what the model answered and whether it was correct. This helps to monitor the evaluation as it happens, especially if many questions are being tested.\n",
        "Finally, the function returns the list of results and the count of correct answers.\n",
        "With these functions in place, we can now evaluate all models and compile the metrics.\n",
        "6. Run Evaluation for All Models (Cell 7)\n",
        "Now we‚Äôll loop through each model in our models_config, evaluate it on all questions using evaluate_model, and collect the outcomes. We will calculate summary metrics for each model:\n",
        "Accuracy (% correct)\n",
        "Total score (number of correct answers out of total questions)\n",
        "Total tokens used (if available; this could be sum of tokens across all questions for that model)\n",
        "Average response time per question (latency)\n",
        "We‚Äôll store summary results in a list of dictionaries (which we can later convert to a DataFrame for display or CSV export). We‚Äôll also accumulate all per-question results into a single list for detailed logging."
      ],
      "metadata": {
        "id": "v-QBGGZIr927"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7 FULL EVALUATION\n",
        "\n",
        "import re\n",
        "import json\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "def extract_answer_letter(output):\n",
        "    match = re.search(r'\\b([ABCDN])\\b', output.upper())\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "# Parameters for full evaluation (you might want to increase sample_size or use all questions)\n",
        "models_to_evaluate = ['o3-mini-2025-01-31', 'claude-3-7-sonnet-20250219']\n",
        "prompt_strategy = \"Prompt-001\"\n",
        "# For full evaluation, you might set sample_size to None or the full count.\n",
        "sample_size = None\n",
        "\n",
        "filtered_questions_file = 'data/questions/MIR-2024-v01-t01_filtered.json'\n",
        "answer_key_file = 'data/answers/MIR-2024-v01-t01-answers.json'\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "print(\"\\n--- RUNNING FULL EVALUATION ---\")\n",
        "for model in models_to_evaluate:\n",
        "    try:\n",
        "        result_file = evaluator.run_evaluation(\n",
        "            questions_file=filtered_questions_file,\n",
        "            answer_key_file=answer_key_file,\n",
        "            prompt_strategy=prompt_strategy,\n",
        "            model=model,\n",
        "            sample_size=sample_size  # Full evaluation: use all questions\n",
        "        )\n",
        "        print(f\"‚úì Full evaluation complete for {model}. Results saved to: {result_file}\")\n",
        "        # The full evaluation results will later be merged and exported to CSV.\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error during full evaluation for {model}: {e}\")\n",
        "\n",
        "print(\"\\nProceed to the cell that merges and exports full evaluation results.\")\n"
      ],
      "metadata": {
        "id": "k0Pbpuf6r9dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In Cell 7:\n",
        "We initialize all_details to gather every question‚Äôs result and summary_records for each model.\n",
        "We loop over each model configuration:\n",
        "* Call evaluate_model for that model, which returns the detailed results and count of correct answers.\n",
        "* We extend the all_details list with the results (so in the end, this list contains an entry for each model-question pair).\n",
        "* Compute accuracy as (num_correct / total_questions) * 100. We round it to two decimal places later for neatness.\n",
        "* Compute total tokens used by summing the tokens_used for each question result, if available. If none of the results have token info (i.e., the list is empty because maybe the API didn‚Äôt provide it), we leave total_tokens as None.\n",
        "* Compute average latency by summing all latencies and dividing by number of questions (we exclude any None latencies just in case).\n",
        "* Append a dictionary to summary_records with the model‚Äôs name and metrics. We include total questions for reference, and round the accuracy and average latency for readability.\n",
        "* Print a summary line for each model (e.g., ‚ÄúFinished ModelX: 8/10 correct, Accuracy 80.0%.‚Äù).\n",
        "After this loop, we have:\n",
        "* summary_records: a list of summary info for each model.\n",
        "* all_details: a list of per-question info, which we can turn into a detailed log.\n",
        "Next, we‚Äôll convert these to pandas DataFrames for easy viewing and export."
      ],
      "metadata": {
        "id": "_-mQLra8sJBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# FINAL EXPORT CELL: Full Evaluation Results to CSV, GitHub, & Download\n",
        "# =========================\n",
        "\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- Step 1: Gather Full Evaluation Result Files ---\n",
        "# Adjust the glob pattern if necessary to include only full evaluation files.\n",
        "# (This example assumes your full evaluation result files contain \"filtered\" in the filename.)\n",
        "result_files = glob.glob(\"data/results/EVAL-MIR-2024-v01-t01_filtered-*.json\")\n",
        "print(f\"Found {len(result_files)} full evaluation result file(s).\")\n",
        "\n",
        "# --- Step 2: Merge Detailed Evaluation Results ---\n",
        "# We assume each JSON file contains detailed results under either the \"details\" or \"results\" key.\n",
        "all_details = []\n",
        "for file in result_files:\n",
        "    try:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        details = data.get('details') or data.get('results')\n",
        "        if details:\n",
        "            all_details.extend(details)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "if not all_details:\n",
        "    print(\"No detailed evaluation results were found in the JSON files.\")\n",
        "else:\n",
        "    # Convert the merged results to a DataFrame.\n",
        "    df_full = pd.DataFrame(all_details)\n",
        "    print(\"Merged full evaluation results shape:\", df_full.shape)\n",
        "    print(\"Columns in full evaluation results:\", df_full.columns.tolist())\n",
        "\n",
        "    # --- Step 3: Export Merged Results to a Single CSV File ---\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    export_filename = f\"full_evaluation_results_{timestamp}.csv\"\n",
        "    export_dir = os.path.join(\"data\", \"exports\")\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "    export_path = os.path.join(export_dir, export_filename)\n",
        "    df_full.to_csv(export_path, index=False)\n",
        "    print(f\"‚úì Full evaluation results exported to CSV: {export_path}\")\n",
        "\n",
        "    # --- Step 4: Download the CSV File Locally (Colab) ---\n",
        "    files.download(export_path)\n",
        "\n",
        "    # --- Step 5: Push the CSV File to GitHub ---\n",
        "    try:\n",
        "        # Retrieve GitHub token from Colab secrets\n",
        "        github_token = userdata.get('GITHUB_TOKEN')\n",
        "        if not github_token:\n",
        "            raise ValueError(\"GITHUB_TOKEN is not set in Colab secrets.\")\n",
        "\n",
        "        # Configure the remote URL with your GitHub token\n",
        "        repo_name = \"armelida/MELIDA\"\n",
        "        token_url = f\"https://{github_token}@github.com/{repo_name}.git\"\n",
        "\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", \"armelida@gmail.com\"], check=True)\n",
        "        subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", \"Armelida\"], check=True)\n",
        "        subprocess.run([\"git\", \"remote\", \"set-url\", \"origin\", token_url], check=True)\n",
        "\n",
        "        # Stage the new CSV file for commit.\n",
        "        subprocess.run([\"git\", \"add\", export_path], check=True)\n",
        "        commit_message = f\"Export full evaluation results {timestamp}\"\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], check=True)\n",
        "\n",
        "        # Pull the latest changes and then push your commit.\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", \"main\", \"--rebase\"], check=True)\n",
        "        subprocess.run([\"git\", \"push\", \"origin\", \"main\"], check=True)\n",
        "        print(\"‚úì CSV file successfully pushed to GitHub.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GitHub push: {e}\")\n"
      ],
      "metadata": {
        "id": "hfno6LLELaoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CELL: Export Most Failed Questions CSV\n",
        "# =========================\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# (Optional) Load the most recent full evaluation CSV from your exports folder:\n",
        "export_files = sorted(glob.glob(\"data/exports/full_evaluation_results_*.csv\"))\n",
        "if export_files:\n",
        "    latest_export = export_files[-1]\n",
        "    df_full = pd.read_csv(latest_export)\n",
        "    print(f\"Loaded merged full evaluation results from: {latest_export}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No full evaluation CSV file found in data/exports/.\")\n",
        "\n",
        "# Standardize answer columns and compute correctness.\n",
        "df_full['model_answer'] = df_full['model_answer'].astype(str).str.strip().str.upper()\n",
        "df_full['correct_answer'] = df_full['correct_answer'].astype(str).str.strip().str.upper()\n",
        "df_full['raw_response'] = df_full['raw_response'].astype(str).str.strip()\n",
        "df_full['correct'] = df_full['model_answer'] == df_full['correct_answer']\n",
        "\n",
        "# Determine which column to use for question text.\n",
        "if 'prompt' in df_full.columns:\n",
        "    question_text_col = 'prompt'\n",
        "elif 'question_text' in df_full.columns:\n",
        "    question_text_col = 'question_text'\n",
        "else:\n",
        "    # If no column exists, create one with a default value.\n",
        "    df_full['question_text'] = \"Not available\"\n",
        "    question_text_col = 'question_text'\n",
        "\n",
        "# Filter out only the failed evaluations.\n",
        "df_failures = df_full[~df_full['correct']]\n",
        "\n",
        "# Group by question_id to aggregate failure information.\n",
        "df_failures_summary = df_failures.groupby(\"question_id\").agg(\n",
        "    failure_count=(\"model\", \"count\"),\n",
        "    models_failed=(\"model\", lambda x: \", \".join(sorted(x.unique()))),\n",
        "    correct_answer=(\"correct_answer\", \"first\"),\n",
        "    raw_responses=(\"raw_response\", lambda x: \" || \".join(x.astype(str).unique())),\n",
        "    question_text=(question_text_col, \"first\")\n",
        ").reset_index()\n",
        "\n",
        "# Sort by failure_count descending (most failed questions at the top).\n",
        "df_failures_summary = df_failures_summary.sort_values(\"failure_count\", ascending=False)\n",
        "\n",
        "# Save the summary to a CSV file.\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "failed_csv = f\"most_failed_questions_{timestamp}.csv\"\n",
        "export_dir = os.path.join(\"data\", \"exports\")\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "export_path = os.path.join(export_dir, failed_csv)\n",
        "df_failures_summary.to_csv(export_path, index=False)\n",
        "print(f\"‚úì Most failed questions exported to CSV: {export_path}\")\n",
        "\n",
        "# Optionally, display the DataFrame.\n",
        "print(\"=== Most Failed Questions ===\")\n",
        "print(df_failures_summary.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "vphnlZLZOrGl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}