{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNQrzULZpttW0HkrHDYihfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelida/MELIDA/blob/main/notebooks/MELIDA_Evaluator_V2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Model LLM Evaluation Notebook\n",
        "\n",
        "This notebook evaluates **multiple Large Language Models (LLMs)** on a set of standardized test questions. We will start by comparing a few models (2‚Äì3) and set up the code to **scale up to 10+ models** easily using an external registry file for model configuration. The evaluation will use a consistent prompt strategy and track performance metrics like accuracy, tokens used, and response time for each model. We‚Äôll also log detailed results per question and export the outcomes to CSV files for visualization (e.g., in Tableau).\n",
        "\n",
        "**Key features of this notebook:**\n",
        "- Uses an **external JSON/YAML file** (‚Äúmodel registry‚Äù) to define which models to evaluate and how to access them, so you can easily add/remove models without changing code.\n",
        "- Evaluates each model on a set of **standardized test questions** with a chosen prompt format (e.g., multiple-choice questions) ‚Äì currently using a zero-shot prompt asking for the best answer.\n",
        "- Records **metrics per model**: accuracy (percentage of questions answered correctly), total score (number of correct answers), number of tokens used (if available), and average response time.\n",
        "- Stores **detailed logs per question** and model, including question ID, model name, full input prompt, model‚Äôs output, whether it was correct, and latency.\n",
        "- Exports results to **CSV files** (summary and detailed) for external analysis. We‚Äôll include a guide on how to use these in Tableau to filter by model/prompt/question and create visualizations of accuracy and identify the hardest questions.\n",
        "- Modular code structure with clear comments and section headings, so you can identify and modify specific parts (e.g., to change the prompt strategy or add new evaluation features like chain-of-thought or hallucination detection in the future).\n",
        "\n"
      ],
      "metadata": {
        "id": "RFDg3bqrp5Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Cell:\n",
        "# Check Runtime & GPU Availability\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def check_runtime():\n",
        "    \"\"\"Check whether a GPU or TPU is available.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"‚úÖ GPU is enabled! Using: {gpu_name}\")\n",
        "    elif \"COLAB_TPU_ADDR\" in os.environ:\n",
        "        print(\"‚úÖ TPU is enabled!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: No GPU or TPU detected. Running on CPU.\")\n",
        "        print(\"üëâ Go to Runtime > Change runtime type > Select GPU/TPU\")\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check GPU details using nvidia-smi if available.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è `nvidia-smi` not found. No GPU detected.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è No GPU found.\")\n",
        "\n",
        "# Run the checks\n",
        "check_runtime()\n",
        "check_gpu()\n",
        "\n",
        "#  Clone repository and change working directory\n",
        "!rm -rf MELIDA  # Remove any existing copy (optional)\n",
        "!git clone https://github.com/armelida/MELIDA.git\n",
        "%cd MELIDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyrEgEQ-xVTk",
        "outputId": "31893f50-1588-4036-c7fb-27010fb12063"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è WARNING: No GPU or TPU detected. Running on CPU.\n",
            "üëâ Go to Runtime > Change runtime type > Select GPU/TPU\n",
            "‚ö†Ô∏è No GPU found.\n",
            "Cloning into 'MELIDA'...\n",
            "remote: Enumerating objects: 377, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 377 (delta 90), reused 52 (delta 25), pack-reused 205 (from 1)\u001b[K\n",
            "Receiving objects: 100% (377/377), 758.71 KiB | 2.62 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n",
            "/content/MELIDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0A: Load API Keys & Save API Configuration\n",
        "\n",
        "!pip install -q python-dotenv\n",
        "\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Initialize API keys dictionary\n",
        "api_keys = {\"openai\": None, \"anthropic\": None, \"together\": None}\n",
        "\n",
        "# Try to load from Colab secrets using userdata\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_keys[\"openai\"] = userdata.get('OPENAI_API_KEY')\n",
        "    api_keys[\"anthropic\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "    api_keys[\"together\"] = userdata.get('TOGETHER_API_KEY')\n",
        "    if api_keys[\"openai\"] and api_keys[\"anthropic\"] and api_keys[\"together\"]:\n",
        "        print(\"‚úì API keys loaded from Colab secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Couldn't load from Colab secrets - {e}\")\n",
        "\n",
        "# Fallback: load from environment variables if not loaded yet\n",
        "if not all(api_keys.values()):\n",
        "    api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "    if api_keys[\"openai\"] or api_keys[\"anthropic\"] or api_keys[\"together\"]:\n",
        "        print(\"‚úì API keys loaded from environment variables\")\n",
        "\n",
        "# Fallback: load from a .env file if still missing\n",
        "if not all(api_keys.values()):\n",
        "    try:\n",
        "        load_dotenv()  # This will load variables from a .env file in the current directory\n",
        "        api_keys[\"openai\"] = api_keys[\"openai\"] or os.environ.get(\"OPENAI_API_KEY\")\n",
        "        api_keys[\"anthropic\"] = api_keys[\"anthropic\"] or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        api_keys[\"together\"] = api_keys[\"together\"] or os.environ.get(\"TOGETHER_API_KEY\")\n",
        "        if api_keys[\"openai\"] or api_keys[\"anthropic\"] or api_keys[\"together\"]:\n",
        "            print(\"‚úì API keys loaded from .env file\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Couldn't load from .env file - {e}\")\n",
        "\n",
        "# Propagate keys to os.environ so subsequent cells can access them\n",
        "if api_keys[\"openai\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_keys[\"openai\"]\n",
        "if api_keys[\"anthropic\"]:\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = api_keys[\"anthropic\"]\n",
        "if api_keys[\"together\"]:\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = api_keys[\"together\"]\n",
        "\n",
        "# Save API configuration to a JSON file for future reference\n",
        "os.makedirs('config', exist_ok=True)\n",
        "api_config = {\n",
        "    \"openai\": {\"api_key\": api_keys[\"openai\"] or \"YOUR_OPENAI_API_KEY_HERE\"},\n",
        "    \"anthropic\": {\"api_key\": api_keys[\"anthropic\"] or \"YOUR_ANTHROPIC_API_KEY_HERE\"},\n",
        "    \"together\": {\"api_key\": api_keys[\"together\"] or \"YOUR_TOGETHER_API_KEY_HERE\"}\n",
        "}\n",
        "with open('config/api_config.json', 'w') as f:\n",
        "    json.dump(api_config, f, indent=2)\n",
        "\n",
        "# Report missing keys, if any\n",
        "missing = []\n",
        "if not api_keys[\"openai\"]:\n",
        "    missing.append(\"OpenAI\")\n",
        "if not api_keys[\"anthropic\"]:\n",
        "    missing.append(\"Anthropic\")\n",
        "if not api_keys[\"together\"]:\n",
        "    missing.append(\"Together\")\n",
        "if missing:\n",
        "    print(f\"‚ö† Missing API keys: {', '.join(missing)}\")\n",
        "    print(\"Please set the API keys using Colab secrets, environment variables, or a .env file.\")\n",
        "else:\n",
        "    print(\"‚úì Complete API configuration saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5_dSkv97Efr",
        "outputId": "f3a07147-e5e2-4f74-832c-3cb59e60cce8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì API keys loaded from Colab secrets\n",
            "‚úì Complete API configuration saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation (Cell 1)\n",
        "\n",
        "First, we install and import necessary libraries. This includes:\n",
        "- **Hugging Face Transformers** for local model inference (if using HuggingFace-hosted models).\n",
        "- **OpenAI/Anthropic API SDKs** (if using direct APIs like OpenAI‚Äôs GPT or Anthropic‚Äôs Claude).\n",
        "- **Together AI** client (if using the Together API for hosted models).\n",
        "- **PyYAML** (for reading YAML config) and **pandas** (for data manipulation and CSV export).\n",
        "\n",
        "We will also ensure any required API keys are set (for OpenAI, Anthropic, Together, etc.) via environment variables for security. Replace or set these environment variables before running the evaluation.\n"
      ],
      "metadata": {
        "id": "xiu37WIKrI_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fMnoEmHYo81v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b8b0a8-4106-4d46-ca1e-35919ed8c2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Libraries loaded.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup environment and install required packages\n",
        "\n",
        "\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q pandas PyYAML openai anthropic together transformers\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import yaml  # For YAML parsing (PyYAML)\n",
        "\n",
        "# Import model API clients\n",
        "import openai\n",
        "# Uncomment and ensure your OpenAI API key is available via environment variables\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "import anthropic\n",
        "# Uncomment if you plan to use Anthropic API:\n",
        "# anthropic_client = anthropic.Client(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
        "\n",
        "import together\n",
        "# Uncomment if you plan to use Together API:\n",
        "# together_client = together.Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n",
        "\n",
        "# (Optional) If using Hugging Face transformers for local models:\n",
        "try:\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "except ImportError:\n",
        "    !pip install -q transformers\n",
        "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Setup complete. Libraries loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configure Model Registry (Cell 2)\n",
        "We use an external model registry file (JSON or YAML) to list all models to evaluate and their access details. This file allows easy scaling to more models ‚Äì just add or remove entries without changing the notebook code. Each model entry can include:\n",
        "name: A human-readable name for the model (used in results and plots).\n",
        "provider: The method to access the model (huggingface, together, openai, anthropic, etc.).\n",
        "model_id: Identifier for the model:\n",
        "For huggingface, this is the model‚Äôs name on HuggingFace Hub (e.g., \"google/flan-t5-small\").\n",
        "For together, it might be a model ID known to the Together API (e.g., \"meta-llama/Llama-2-7b-chat-hf\").\n",
        "For openai, it could be the API model name (e.g., \"gpt-4\" or \"gpt-3.5-turbo\").\n",
        "For other providers, use the appropriate identifier.\n",
        "api_key_env (if needed): The environment variable name for the API key (e.g., \"OPENAI_API_KEY\"). This can be omitted for HuggingFace (if using local models or if no auth needed).\n",
        "Additional settings like max_tokens, temperature, etc., which define generation parameters for that model.\n",
        "Example model registry (YAML format):\n",
        "\n",
        "models:\n",
        "  - name: FlanT5 Small\n",
        "    provider: huggingface\n",
        "    model_id: google/flan-t5-small\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: GPT-4 (Jan 2025)\n",
        "    provider: openai\n",
        "    model_id: gpt-4\n",
        "    api_key_env: OPENAI_API_KEY\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "  - name: Llama2 7B Chat\n",
        "    provider: together\n",
        "    model_id: meta-llama/Llama-2-7b-chat-hf\n",
        "    max_tokens: 100\n",
        "    temperature: 0.0\n",
        "\n",
        "In the code below, we load the model list from the registry file. Update model_config_path to point to your JSON/YAML file. The code will automatically detect JSON vs YAML based on file extension and parse accordingly. After loading, it prints out the model configurations to confirm."
      ],
      "metadata": {
        "id": "1_unmmk1qxdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load model registry from external JSON/YAML file\n",
        "\n",
        "# Set the path to your model registry file in the notebooks folder\n",
        "model_config_path = \"notebooks/models.yaml\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(model_config_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Model config file not found at {model_config_path}. \"\n",
        "        \"Please create it as per the example and update the path.\"\n",
        "    )\n",
        "\n",
        "# Parse the config file (supports YAML and JSON)\n",
        "if model_config_path.endswith((\".yaml\", \".yml\")):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = yaml.safe_load(f)\n",
        "elif model_config_path.endswith(\".json\"):\n",
        "    with open(model_config_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported config file format. Use .json or .yaml\")\n",
        "\n",
        "# The config should either be a dict with a top-level 'models' key or a list itself.\n",
        "if isinstance(config_data, dict) and \"models\" in config_data:\n",
        "    models_config = config_data[\"models\"]\n",
        "elif isinstance(config_data, list):\n",
        "    models_config = config_data\n",
        "else:\n",
        "    raise ValueError(\"Config file format error: expected a list of models or a 'models' key.\")\n",
        "\n",
        "print(f\"Loaded {len(models_config)} models from registry:\")\n",
        "for m in models_config:\n",
        "    print(f\" - {m.get('name', 'Unnamed')} ({m.get('provider', 'Unknown')}, id={m.get('model_id', 'N/A')})\")\n"
      ],
      "metadata": {
        "id": "z8Zg9195pKkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79fe2f00-7b7b-441d-9814-27f33cf54e06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3 models from registry:\n",
            " - o3-mini-2025-01-31 (OpenAI, id=o3-mini-2025-01-31)\n",
            " - Claude (Anthropic, id=claude-3-7-sonnet-20250219)\n",
            " - Together (Together, id=deepseek-ai/DeepSeek-R1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load Standardized Test Questions (Cell 3)\n",
        "Next, we prepare the standardized test questions for evaluation. These can be hard-coded, loaded from a file, or generated. In this notebook, we‚Äôll define a list of questions in a structured format (each with an ID, question text, multiple-choice options, and the correct answer). You can replace these with any set of questions relevant to your use case. For demonstration, we‚Äôll use a few simple sample questions. In a real scenario, you might load dozens of questions from a JSON/CSV file or an existing dataset. Ensure each question has a known correct answer to compute accuracy."
      ],
      "metadata": {
        "id": "qa5t7ftJrBM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load extracted exam questions for evaluation\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Define the path to the exported questions file.\n",
        "# Adjust this path if your exported file name or location is different.\n",
        "questions_file = \"data/questions/MIR-2024-v01-t01.json\"\n",
        "\n",
        "if not os.path.exists(questions_file):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Questions file not found at {questions_file}. \"\n",
        "        \"Please run the extraction process to generate the questions file.\"\n",
        "    )\n",
        "\n",
        "with open(questions_file, 'r', encoding='utf-8') as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(questions)} questions for evaluation.\")\n",
        "\n",
        "# Optionally, preview the first three questions\n",
        "for q in questions[:3]:\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print(f\"ID: {q['id']}\")\n",
        "    print(f\"Question: {q['question_text']}\")\n",
        "    print(\"Options:\")\n",
        "    for key, value in q['options'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"---------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "1ljgjq5bpKqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c9ce1e-5d67-479d-9358-f75326baf83a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 185 questions for evaluation.\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q026\n",
            "Question: Entre los cambios metab√≥licos que se observan en un paciente con resistencia a insulina existe:\n",
            "Options:\n",
            "  A: Incremento de la expresi√≥n hep√°tica de genes gluconeog√©nicos mediado por FOXO1 (forkhead box other) fosforilado.\n",
            "  B: Descenso en los niveles intracelulares de hexoquinasa 2 dependiente de insulina.\n",
            "  C: Aumento de la glucogen√≥lisis muscular, contribuyendo al incremento de la glucemia.\n",
            "  D: Aumento en los niveles s√©ricos de amino√°cidos como leucina e isoleucina.\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q027\n",
            "Question: La deficiencia de acil-CoA-deshidrogenasa provoca una de las siguientes alteraciones bioqu√≠micas:\n",
            "Options:\n",
            "  A: Disminuci√≥n de √°cidos dicarbox√≠licos.\n",
            "  B: Aumento de la gluconeog√©nesis.\n",
            "  C: Disminuci√≥n de la ureag√©nesis.\n",
            "  D: Aumento de carnitina libre.\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "ID: MIR-2024-v01-t01-Q028\n",
            "Question: Respecto a la t√©cnica de exploraci√≥n de la motilidad pupilar y sus reflejos indique la respuesta INCORRECTA:\n",
            "Options:\n",
            "  A: El reflejo de la visi√≥n pr√≥xima se explora con una linterna que se enfoca sobre las pupilas de forma alternativa.\n",
            "  B: El signo pupilar de Marcus Gunn o defecto pupilar aferente relativo est√° presente en enfermedades del nervio √≥ptico y enfermedades retinianas no extensas.\n",
            "  C: El reflejo fotomotor consensuado est√° afectado cuando el da√±o implica la v√≠a eferente vehiculada por el nervio motor ocular com√∫n.\n",
            "  D: La anisocoria que aumenta en condiciones de oscuridad mostrando una pupila m√°s mi√≥tica se debe a que hay un trastorno en el sistema simp√°tico.\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We defined a list of dictionaries, where each dictionary represents a question. Each question has:\n",
        "id: a unique identifier,\n",
        "question: the text of the question,\n",
        "choices: a list of answer choices (as strings, each prefixed with a letter),\n",
        "answer: the correct choice (here represented by the letter of the correct option).\n",
        "Feel free to extend or replace this list. For example, you could load questions from a file or generate them. Just ensure each question has a known correct answer for scoring."
      ],
      "metadata": {
        "id": "ycbGse8qrO6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define Prompt Strategy (Cell 4)\n",
        "The prompt strategy determines how we present each question to the model. For standardized test questions, a common strategy is to provide the question and multiple-choice options and ask the model to pick the best answer. We might instruct the model to output just the option letter (to make it easier to check correctness). In future, you could experiment with different strategies (e.g., asking for an explanation, chain-of-thought prompting, etc.). For now, we‚Äôll use a straightforward prompt: the question, the options, and a final instruction like ‚ÄúAnswer with the letter of the correct option.‚Äù We implement this as a function format_prompt(question) that takes a question entry and returns the full prompt text (or structured prompt) to send to the model. This function can be easily modified if you want to change how prompts are constructed."
      ],
      "metadata": {
        "id": "oWi8tcY7rVM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define the prompt formatting strategy\n",
        "\n",
        "def format_prompt(q):\n",
        "    \"\"\"\n",
        "    Format a question dictionary into a prompt string for the MIR exam in Spanish.\n",
        "\n",
        "    The prompt instructs the model to answer with a single letter (A, B, C, D) if it knows the answer,\n",
        "    or with N if unsure.\n",
        "    \"\"\"\n",
        "    # Get the main question text\n",
        "    question_text = q.get(\"question_text\", \"\")\n",
        "\n",
        "    # Get each answer option from the options dictionary (default to empty string if missing)\n",
        "    options = q.get(\"options\", {})\n",
        "    option_a = options.get(\"A\", \"\")\n",
        "    option_b = options.get(\"B\", \"\")\n",
        "    option_c = options.get(\"C\", \"\")\n",
        "    option_d = options.get(\"D\", \"\")\n",
        "\n",
        "    # Build the prompt using the provided format\n",
        "    prompt = (\n",
        "        \"Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. \"\n",
        "        \"Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. \"\n",
        "        \"Si no est√°s seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\\n\\n\"\n",
        "        f\"{question_text}\\n\\n\"\n",
        "        f\"A) {option_a}\\n\"\n",
        "        f\"B) {option_b}\\n\"\n",
        "        f\"C) {option_c}\\n\"\n",
        "        f\"D) {option_d}\\n\\n\"\n",
        "        \"Tu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# Test the prompt formatting on the first loaded question\n",
        "example_prompt = format_prompt(questions[0])\n",
        "print(\"Example formatted prompt:\\n\", example_prompt)\n"
      ],
      "metadata": {
        "id": "toS2gKl3pKtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a33a788-0a62-4762-c41b-9d18c2515b83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example formatted prompt:\n",
            " Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°s seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\n",
            "\n",
            "Entre los cambios metab√≥licos que se observan en un paciente con resistencia a insulina existe:\n",
            "\n",
            "A) Incremento de la expresi√≥n hep√°tica de genes gluconeog√©nicos mediado por FOXO1 (forkhead box other) fosforilado.\n",
            "B) Descenso en los niveles intracelulares de hexoquinasa 2 dependiente de insulina.\n",
            "C) Aumento de la glucogen√≥lisis muscular, contribuyendo al incremento de la glucemia.\n",
            "D) Aumento en los niveles s√©ricos de amino√°cidos como leucina e isoleucina.\n",
            "\n",
            "Tu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: The format_prompt function takes a question from our list and builds a prompt. We put the question text, list all the choices (joined in one line for simplicity), and then give an explicit instruction. By asking for the letter only, we aim to have consistent outputs that are easy to check (the model hopefully will just respond with ‚ÄúB‚Äù, etc.). After defining the function, we preview an example prompt for the first question to verify the format. You can adjust this format as needed (for instance, if a model tends to do better with a different phrasing or if you want the model to explain its answer, etc.)."
      ],
      "metadata": {
        "id": "5-XAXQV_rgfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model Interface and Evaluation Functions (Cells 5‚Äì6)\n",
        "In this section, we set up functions to handle model inference and evaluation:\n",
        "call_model(model_config, prompt_text) ‚Äì Invokes a single model (based on its provider and config) with the given prompt, and returns the model‚Äôs answer, along with metadata like token usage and latency.\n",
        "evaluate_model(model_config, questions) ‚Äì Uses call_model to get answers for each question from one model, checks correctness, and collects detailed results.\n",
        "We will also prepare a loop or another function to evaluate all models and aggregate the results for comparison.\n",
        "Structuring this logic into functions makes the notebook modular and easier to update. For example, if in the future we want to add a step for chain-of-thought (CoT) prompting or filter the model output for hallucinations, we could modify or wrap call_model accordingly. 5.1 call_model Implementation: This function will branch based on the provider:\n",
        "HuggingFace: use transformers pipeline or model generate. We‚Äôll initialize a pipeline for text generation or use the model‚Äôs generate method. We also tokenize the input to count input tokens. The output tokens can be counted by the tokenizer as well.\n",
        "OpenAI: use openai.Completion or openai.ChatCompletion depending on model type. For chat models (e.g., GPT-4), we pass the prompt as a user message. We retrieve the output text and usage info (token counts).\n",
        "Anthropic: (Claude models) use anthropic‚Äôs client. Typically you provide a prompt with a special format (like \"\\n\\nHuman: <question>\\n\\nAssistant:\"). We skip detailed implementation here but it can be added.\n",
        "Together: use Together API client. For example, together_client.complete or the chat completion as needed, based on their documentation. (Ensure TOGETHER_API_KEY is set.)\n",
        "Additional providers (e.g., Cohere, AI21) can be integrated similarly by adding new branches.\n",
        "We also measure the time taken for each call (latency). If token counts are not readily available from the API, we will set them to None (or you could estimate via a tokenizer). Let‚Äôs implement call_model below:"
      ],
      "metadata": {
        "id": "626XG9mwrmbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define function to call a model and get its response (updated for Anthropic Messages API)\n",
        "\n",
        "import time\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "def call_model(model_cfg, prompt):\n",
        "    \"\"\"\n",
        "    Call a model with the given prompt and return its response and metadata.\n",
        "    Returns: output_text, tokens_used, latency\n",
        "    \"\"\"\n",
        "    provider = model_cfg.get(\"provider\", \"\").lower()\n",
        "    model_id = model_cfg.get(\"model_id\")\n",
        "    max_tokens = model_cfg.get(\"max_tokens\", 100)\n",
        "    temperature = model_cfg.get(\"temperature\", 0.0)\n",
        "    tokens_used = None\n",
        "    output_text = \"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    if provider == \"huggingface\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if \"google/flan\" in model_id.lower() or \"t5\" in model_id.lower():\n",
        "            pipe = pipeline(\"text2text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "            result = pipe(prompt, max_length=max_tokens, temperature=temperature)\n",
        "            output_text = result[0]['generated_text']\n",
        "        else:\n",
        "            pipe = pipeline(\"text-generation\", model=model_id, tokenizer=tokenizer)\n",
        "            result = pipe(prompt, max_length=max_tokens, temperature=temperature, do_sample=False)\n",
        "            output_text = result[0]['generated_text']\n",
        "        try:\n",
        "            input_tokens = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "            output_tokens = tokenizer(output_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "            tokens_used = int(len(input_tokens[0]) + len(output_tokens[0]))\n",
        "        except Exception:\n",
        "            tokens_used = None\n",
        "\n",
        "    elif provider == \"openai\":\n",
        "        # IMPORTANT: If using an \"o3-mini\" model, consider pinning openai to version 0.28.\n",
        "        if os.getenv(model_cfg.get(\"api_key_env\", \"OPENAI_API_KEY\")) is None:\n",
        "            raise RuntimeError(f\"OpenAI API key not set for model {model_cfg.get('name', model_id)}\")\n",
        "        try:\n",
        "            if \"o3-mini\" in model_id.lower():\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_completion_tokens=max_tokens\n",
        "                )\n",
        "            else:\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "            output_text = response['choices'][0]['message']['content'].strip()\n",
        "            if 'usage' in response:\n",
        "                tokens_used = response['usage'].get('total_tokens')\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error calling OpenAI model: {e}\")\n",
        "\n",
        "    elif provider == \"anthropic\":\n",
        "        # Use Anthropic's Messages API for models like Claude.\n",
        "        if os.getenv(model_cfg.get(\"api_key_env\", \"ANTHROPIC_API_KEY\")) is None:\n",
        "            raise RuntimeError(f\"Anthropic API key not set for model {model_cfg.get('name', model_id)}\")\n",
        "        try:\n",
        "            anthropic_client = anthropic.Client(api_key=os.getenv(model_cfg.get(\"api_key_env\", \"ANTHROPIC_API_KEY\")))\n",
        "            # Use the Messages API: supply model, messages, max_tokens, temperature, and stream flag.\n",
        "            response = anthropic_client.messages.create(\n",
        "                model=model_id,\n",
        "                messages=[{\"role\": \"human\", \"content\": prompt}],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                stream=False\n",
        "            )\n",
        "            output_text = response.get(\"completion\", \"\").strip()\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error calling Anthropic model: {e}\")\n",
        "        tokens_used = None\n",
        "\n",
        "    elif provider == \"together\":\n",
        "        if os.getenv(model_cfg.get(\"api_key_env\", \"TOGETHER_API_KEY\")) is None:\n",
        "            raise RuntimeError(f\"Together API key not set for model {model_cfg.get('name', model_id)}\")\n",
        "        try:\n",
        "            together_client = together.Together(api_key=os.getenv(model_cfg.get(\"api_key_env\", \"TOGETHER_API_KEY\")))\n",
        "            response = together_client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                stream=False\n",
        "            )\n",
        "            try:\n",
        "                output_text = response.choices[0].message.content.strip()\n",
        "            except Exception:\n",
        "                output_text = response['choices'][0]['message']['content'].strip() if isinstance(response, dict) else str(response)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error calling Together model: {e}\")\n",
        "        tokens_used = None\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider} for model {model_cfg.get('name', model_id)}\")\n",
        "\n",
        "    latency = time.time() - start_time\n",
        "    return output_text, tokens_used, latency\n",
        "\n",
        "# Testing (Cell 6) remains the same:\n",
        "# Cell 6: Test call_model with models from models.yaml\n",
        "#\n",
        "# sample_prompt = format_prompt(questions[0])\n",
        "# print(\"Sample Prompt:\\n\", sample_prompt)\n",
        "# print(\"=================================\")\n",
        "#\n",
        "# for model_cfg in models_config:\n",
        "#     print(f\"Testing model: {model_cfg.get('name', 'Unnamed')} (provider: {model_cfg.get('provider', 'Unknown')})\")\n",
        "#     try:\n",
        "#         output, tokens, latency = call_model(model_cfg, sample_prompt)\n",
        "#         print(\"Output:\", output)\n",
        "#         print(\"Tokens used:\", tokens)\n",
        "#         print(\"Latency (sec):\", latency)\n",
        "#     except Exception as e:\n",
        "#         print(\"Error calling model:\", e)\n",
        "#     print(\"=================================\")\n"
      ],
      "metadata": {
        "id": "59_SJq6jrgDH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In call_model:\n",
        "We take the model‚Äôs config and a prompt string.\n",
        "Based on provider, we handle the call differently.\n",
        "HuggingFace: We load the model and tokenizer (from local or HuggingFace Hub). We use pipeline for simplicity (it will handle the model loading and generation). We choose the pipeline task based on model type (a quick check for ‚Äút5‚Äù in the model name to decide between text2text-generation and text-generation). After generation, we count tokens by encoding the prompt and output with the tokenizer.\n",
        "OpenAI: We use the OpenAI API. If the model is chat-based (we guess by name containing ‚Äúgpt-3.5‚Äù or ‚Äúgpt-4‚Äù), we use the ChatCompletion endpoint with a single user message. Otherwise, we use the older Completion endpoint. We fetch the text from the response and also get token usage if provided. (Make sure your OpenAI API key is set in the environment.)\n",
        "Anthropic: We format the prompt in the required way for Claude and call the client‚Äôs completion method. (This assumes the anthropic package is installed and imported.) Token count isn‚Äôt directly captured here.\n",
        "Together: We initialize the Together client and call the chat.completions.create method with the prompt as a user message. (This assumes the model supports chat format; for pure text-generation models on Together, you might use a different method like client.completion.create.) We extract the content from the response. (Token usage may be available via Together‚Äôs response, but for simplicity, we set it to None in this example.)\n",
        "We measure the time just before and after the call to compute latency.\n",
        "Finally, we return output_text (the model‚Äôs answer), tokens_used, and latency.\n",
        "This function abstracts away the differences in model access, giving us a unified interface for the evaluation loop.\n",
        "\n",
        "5.2 evaluate_model Implementation: This function will loop through all questions for a single model, use call_model to get the answer, check correctness, and record results. It will return a list of result records (one per question for that model) and also compute summary metrics (like number correct). We‚Äôll implement evaluate_model next:"
      ],
      "metadata": {
        "id": "lzix9vk7rwpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Test call_model with models from models.yaml\n",
        "\n",
        "sample_prompt = format_prompt(questions[0])\n",
        "print(\"Sample Prompt:\\n\", sample_prompt)\n",
        "print(\"=================================\")\n",
        "\n",
        "for model_cfg in models_config:\n",
        "    print(f\"Testing model: {model_cfg.get('name', 'Unnamed')} (provider: {model_cfg.get('provider', 'Unknown')})\")\n",
        "    try:\n",
        "        output, tokens, latency = call_model(model_cfg, sample_prompt)\n",
        "        print(\"Output:\", output)\n",
        "        print(\"Tokens used:\", tokens)\n",
        "        print(\"Latency (sec):\", latency)\n",
        "    except Exception as e:\n",
        "        print(\"Error calling model:\", e)\n",
        "    print(\"=================================\")\n"
      ],
      "metadata": {
        "id": "C3Uqr1dYpKxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d702429-9e64-4620-895f-d33faf272b75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Prompt:\n",
            " Eres un M√©dico que est√° realizando el examen MIR, un test estandarizado en espa√±ol que determinar√° si obtienes tu residencia. Responde a la siguiente pregunta exactamente como se indica. Si conoces la respuesta, responde √öNICAMENTE con una de las letras A, B, C o D. Si no est√°s seguro, responde con N. Cualquier texto adicional invalidar√° tu respuesta y restar√° puntos.\n",
            "\n",
            "Entre los cambios metab√≥licos que se observan en un paciente con resistencia a insulina existe:\n",
            "\n",
            "A) Incremento de la expresi√≥n hep√°tica de genes gluconeog√©nicos mediado por FOXO1 (forkhead box other) fosforilado.\n",
            "B) Descenso en los niveles intracelulares de hexoquinasa 2 dependiente de insulina.\n",
            "C) Aumento de la glucogen√≥lisis muscular, contribuyendo al incremento de la glucemia.\n",
            "D) Aumento en los niveles s√©ricos de amino√°cidos como leucina e isoleucina.\n",
            "\n",
            "Tu respuesta (√öNICAMENTE una letra: A, B, C, D o N si no est√°s seguro):\n",
            "=================================\n",
            "Testing model: o3-mini-2025-01-31 (provider: OpenAI)\n",
            "Output: \n",
            "Tokens used: 330\n",
            "Latency (sec): 2.99891996383667\n",
            "=================================\n",
            "Testing model: Claude (provider: Anthropic)\n",
            "Error calling model: Error calling Anthropic model: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages: Unexpected role \"human\". Did you mean \"user\"?'}}\n",
            "=================================\n",
            "Testing model: Together (provider: Together)\n",
            "Output: <think>\n",
            "Okay, let's tackle this question about metabolic changes in insulin resistance. Hmm, the options are A to D. I need to remember what happens when someone is insulin resistant.\n",
            "\n",
            "First, insulin resistance means the body's cells don't respond well to insulin. So, insulin's normal actions are impaired. Let's go through each option.\n",
            "\n",
            "Option A: Incremento de la expresi√≥n hep√°tica de genes gluconeog√©nicos mediado por FOXO1 fosforilado\n",
            "Tokens used: None\n",
            "Latency (sec): 2.317519187927246\n",
            "=================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In evaluate_model:\n",
        "We iterate over each question, format the prompt, and call the model via call_model.\n",
        "We wrap the model call in a try-except to catch any errors (for instance, if an API call fails or a model isn‚Äôt available). If there‚Äôs an error, we log it and move on, leaving output empty for that question.\n",
        "We then parse the model‚Äôs output to extract the answer. We assume the model should reply with a letter. The code checks the first character of the output: if it‚Äôs one of ‚ÄúA, B, C, D‚Äù, we treat that as the chosen option. (If the output is something else, you could include additional parsing logic ‚Äì for example, sometimes the model might output the full option text or a sentence. Here, we simplify by taking the first letter when possible. If the output is empty or doesn‚Äôt start with a letter, we mark the answer as incorrect by default.)\n",
        "We compare the model‚Äôs answer letter (uppercased) to the true answer letter from the question. If they match, it‚Äôs correct and we increment correct_count.\n",
        "We append a dictionary to results containing all relevant info: question ID, model name, the exact prompt used, the model‚Äôs raw output, a boolean for correctness, latency (in seconds), and tokens used.\n",
        "We also print a one-line progress update for each question, indicating what the model answered and whether it was correct. This helps to monitor the evaluation as it happens, especially if many questions are being tested.\n",
        "Finally, the function returns the list of results and the count of correct answers.\n",
        "With these functions in place, we can now evaluate all models and compile the metrics.\n",
        "6. Run Evaluation for All Models (Cell 7)\n",
        "Now we‚Äôll loop through each model in our models_config, evaluate it on all questions using evaluate_model, and collect the outcomes. We will calculate summary metrics for each model:\n",
        "Accuracy (% correct)\n",
        "Total score (number of correct answers out of total questions)\n",
        "Total tokens used (if available; this could be sum of tokens across all questions for that model)\n",
        "Average response time per question (latency)\n",
        "We‚Äôll store summary results in a list of dictionaries (which we can later convert to a DataFrame for display or CSV export). We‚Äôll also accumulate all per-question results into a single list for detailed logging."
      ],
      "metadata": {
        "id": "v-QBGGZIr927"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Evaluate all models and gather results\n",
        "all_details = []   # list to collect detailed results for every model and question\n",
        "summary_records = []  # list to collect summary metrics for each model\n",
        "\n",
        "total_questions = len(questions)\n",
        "for model_cfg in models_config:\n",
        "    model_name = model_cfg[\"name\"]\n",
        "    results, num_correct = evaluate_model(model_cfg, questions)\n",
        "    all_details.extend(results)\n",
        "    # Calculate summary metrics\n",
        "    accuracy = (num_correct / total_questions) * 100  # percentage\n",
        "    # Sum tokens and average latency for this model\n",
        "    tokens_list = [r[\"tokens_used\"] for r in results if r[\"tokens_used\"] is not None]\n",
        "    total_tokens = sum(tokens_list) if tokens_list else None\n",
        "    latency_list = [r[\"latency\"] for r in results if r[\"latency\"] is not None]\n",
        "    avg_latency = (sum(latency_list) / len(latency_list)) if latency_list else None\n",
        "    summary_records.append({\n",
        "        \"model_name\": model_name,\n",
        "        \"accuracy (%)\": round(accuracy, 2),\n",
        "        \"total_score\": num_correct,\n",
        "        \"total_questions\": total_questions,\n",
        "        \"tokens_used_total\": total_tokens,\n",
        "        \"avg_latency_sec\": round(avg_latency, 2) if avg_latency is not None else None\n",
        "    })\n",
        "    print(f\"Finished {model_name}: {num_correct}/{total_questions} correct, Accuracy {accuracy:.1f}%.\")\n"
      ],
      "metadata": {
        "id": "k0Pbpuf6r9dF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "54802285-ae30-4e71-ce82-c67c07e4df1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2aaeec47fbfe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_cfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mall_details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Calculate summary metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: In Cell 7:\n",
        "We initialize all_details to gather every question‚Äôs result and summary_records for each model.\n",
        "We loop over each model configuration:\n",
        "* Call evaluate_model for that model, which returns the detailed results and count of correct answers.\n",
        "* We extend the all_details list with the results (so in the end, this list contains an entry for each model-question pair).\n",
        "* Compute accuracy as (num_correct / total_questions) * 100. We round it to two decimal places later for neatness.\n",
        "* Compute total tokens used by summing the tokens_used for each question result, if available. If none of the results have token info (i.e., the list is empty because maybe the API didn‚Äôt provide it), we leave total_tokens as None.\n",
        "* Compute average latency by summing all latencies and dividing by number of questions (we exclude any None latencies just in case).\n",
        "* Append a dictionary to summary_records with the model‚Äôs name and metrics. We include total questions for reference, and round the accuracy and average latency for readability.\n",
        "* Print a summary line for each model (e.g., ‚ÄúFinished ModelX: 8/10 correct, Accuracy 80.0%.‚Äù).\n",
        "After this loop, we have:\n",
        "* summary_records: a list of summary info for each model.\n",
        "* all_details: a list of per-question info, which we can turn into a detailed log.\n",
        "Next, we‚Äôll convert these to pandas DataFrames for easy viewing and export."
      ],
      "metadata": {
        "id": "_-mQLra8sJBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert summary and details to pandas DataFrames for display and export\n",
        "summary_df = pd.DataFrame(summary_records)\n",
        "details_df = pd.DataFrame(all_details)\n",
        "\n",
        "print(\"\\nSummary of results for each model:\")\n",
        "display(summary_df)\n",
        "\n",
        "print(\"\\nDetailed results (first few rows):\")\n",
        "display(details_df.head(10))\n"
      ],
      "metadata": {
        "id": "vfFmrWdbsHsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detailed comments: We create two DataFrames:\n",
        "summary_df with one row per model, containing accuracy, scores, etc.\n",
        "details_df with one row per question per model, containing everything from question ID to correctness.\n",
        "We then display the summary and the first few detailed results to verify the content. (In a real Jupyter environment, display(df) will show a nice table. In a text environment or script, you might use print(df.to_string()) or df.head().) Review the summary to ensure metrics make sense, and review the details to spot-check that outputs and correctness are recorded as expected."
      ],
      "metadata": {
        "id": "3JqybaU1tKR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Export Results to CSV (Cell 8)\n",
        "Now that we have the results in DataFrames, we‚Äôll export them to CSV files, which can be used in external analysis tools like Excel or Tableau. We will create two CSV files:\n",
        "llm_eval_summary.csv ‚Äì containing the summary metrics per model.\n",
        "llm_eval_details.csv ‚Äì containing the detailed per-question results.\n",
        "These files will include headers and can be imported directly into Tableau or other tools."
      ],
      "metadata": {
        "id": "rdDMq8WKsMLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Export the summary and detailed results to CSV files\n",
        "summary_csv_path = \"llm_eval_summary.csv\"\n",
        "details_csv_path = \"llm_eval_details.csv\"\n",
        "summary_df.to_csv(summary_csv_path, index=False)\n",
        "details_df.to_csv(details_csv_path, index=False)\n",
        "print(f\"Exported summary results to {summary_csv_path}\")\n",
        "print(f\"Exported detailed results to {details_csv_path}\")\n"
      ],
      "metadata": {
        "id": "djYouS4otOoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this, you should find two CSV files in your working directory:\n",
        "llm_eval_summary.csv ‚Äì with columns like model_name, accuracy (%), total_score, total_questions, tokens_used_total, avg_latency_sec.\n",
        "llm_eval_details.csv ‚Äì with columns like question_id, model_name, prompt, model_output, correct, latency, tokens_used.\n",
        "These can now be loaded into Tableau or any data analysis software for visualization."
      ],
      "metadata": {
        "id": "EW6I9m2ktTj7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDNmxetdtS3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "03450c01-c2b5-42ab-986d-62f3c05e5855"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '‚Äì' (U+2013) (<ipython-input-10-8963aea128aa>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-8963aea128aa>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    Identify questions with low scores across models ‚Äì these are the hardest questions. You can highlight them or filter to the hardest 5 questions.\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '‚Äì' (U+2013)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Using Tableau for Analysis of Results\n",
        "With the results exported, we can analyze and visualize the performance of the models. Below are step-by-step instructions to use Tableau (or a similar data visualization tool) to explore the data:\n",
        "Import the CSV files into Tableau: Open Tableau and connect to the llm_eval_summary.csv and llm_eval_details.csv files (you can import them separately or join/relate them on the model_name field if needed).\n",
        "Summary Dashboard (Model-Level): Using llm_eval_summary.csv, you can create a simple chart of model performance. For example:\n",
        "Create a bar chart with model_name on the x-axis and accuracy (%) on the y-axis to compare accuracy across models.\n",
        "Add labels to show the exact accuracy or score for each model bar.\n",
        "You could also include avg_latency_sec as a secondary metric (perhaps a separate chart or as a tooltip) to see the speed-accuracy tradeoff.\n",
        "Filter by Prompt or Question: Since all models used the same prompt strategy in this run, the summary is straightforward. If you had different prompt strategies or sets of questions, you could use filters. For instance, if prompt_strategy was a field, you could filter or color-code by it. Or using the detailed data, you could filter to a specific question to see all model answers for that one.\n",
        "Detailed Analysis (Question-Level): Using llm_eval_details.csv, you can analyze which questions were hardest:\n",
        "Create a view with question_id on one axis and perhaps the count of models that got it correct.\n",
        "For example, drag question_id to rows, and an aggregation of correct (treat correct as 0/1 values and take average or sum). Multiply by 100 to interpret as percentage of models correct. This will tell you the percent of models that answered each question correctly.\n",
        "Identify questions with low scores across models ‚Äì these are the hardest questions. You can highlight them or filter to the hardest 5 questions.\n",
        "You can also create a detail table showing each model‚Äôs answer (from model_output) for a given question by filtering question_id and listing model_name and model_output for context.\n",
        "Visualization Examples: You might create a dashboard with two charts ‚Äì one showing model accuracy comparison, and another showing a difficulty analysis of questions. Use color or annotations to highlight interesting findings (e.g., a particular model that outperforms others, or a question that stumped half the models).\n",
        "\n",
        "\n",
        "Example: A simple bar chart comparing model accuracy. In the figure above, each bar represents a model‚Äôs accuracy on the test set (e.g., GPT-4 achieved 100% on 3 questions, whereas a smaller FlanT5 model scored 66.7%). You can create similar charts in Tableau easily by dragging and dropping the accuracy (%) field for each model_name. Remember, you can use Tableau‚Äôs filters to focus on specific models or questions. For instance, a filter on model_name could let you compare any subset of models (e.g., comparing only GPT-4.5-Preview vs. Gemini-2.0), and a filter on question_id could let you inspect performance on individual questions. Note: Ensure that in Tableau, boolean fields like correct are treated appropriately (Tableau might import them as text \"TRUE\"/\"FALSE\"). You may want to create a calculated field like Correct (0/1) as IF [correct] THEN 1 ELSE 0 END for easier aggregation.\n",
        "9. Future Enhancements and Conclusion\n",
        "We designed this notebook to be modular and easy to extend. Here are a few ways you could build on this framework:\n",
        "Chain-of-Thought Prompting: Modify the format_prompt function or the evaluation loop to incorporate chain-of-thought (CoT) prompts (e.g., by asking the model to \"think step by step\" before answering, and then evaluating the final answer separately). You could then evaluate not just the final answer accuracy but also analyze the reasoning steps.\n",
        "Hallucination Detection: If the questions have definitive answers, any divergence in the model‚Äôs explanation could be flagged. You might extend the detailed logs with fields for whether the model‚Äôs explanation contains factual errors (this could be manual or via another automated checker).\n",
        "Additional Metrics: We tracked token usage and latency. You could also log prompt length or output length separately, or cost if using paid APIs (by multiplying token usage by cost per token).\n",
        "More Providers: You can easily add new model providers (Cohere, AI21, etc.) in the call_model function. Just include a new elif branch and use their SDK or HTTP calls.\n",
        "Finally, a note on the evaluator.py (if you have a separate script for evaluation):\n",
        "To support token and time tracking, ensure that evaluator.py captures the start and end time around model invocations (as we did with time.time() in call_model) and returns or logs the duration.\n",
        "Modify the evaluator to also return the model‚Äôs raw output and any usage stats if available. For example, if originally it only returned correctness, have it return a dict with keys like output, correct, tokens_used, latency.\n",
        "To make it compatible with multi-model comparison, you could refactor evaluator.py to accept a model config or identifier as a parameter, so it can be called in a loop for different models (similar to how we did with evaluate_model). It could also be extended to handle a list of models internally and produce a combined report.\n",
        "By implementing these modifications, the evaluation pipeline will be more robust and informative. The modular structure of this notebook should make such changes straightforward ‚Äì each component (prompt formatting, model calling, result aggregation) can be adjusted independently. Conclusion: You now have a complete pipeline to evaluate multiple LLMs side-by-side on standardized questions, with results ready for analysis. Feel free to experiment with different models (just update the registry file), add more questions, or tweak the prompt strategy. Happy evaluating!\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Un_97SjRKrww"
      }
    }
  ]
}