{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPolp8FFDjhBtTIr35yg7tu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelida/MELIDA/blob/main/src/questions-scraper/MELIDA_PDF_Scraper_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2YwGaUcetnwH"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pdfplumber pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown for Google Drive access\n",
        "!pip install gdown --quiet\n",
        "\n",
        "# Configure paths\n",
        "import os\n",
        "\n",
        "# Use a local directory structure in Colab instead of MyDrive\n",
        "BASE_PATH = \"/content/MELIDA\"  # Changed from MyDrive path\n",
        "PDF_DIR = os.path.join(BASE_PATH, \"data/raw/exams\")\n",
        "OUTPUT_DIR = os.path.join(BASE_PATH, \"data/questions\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(PDF_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"PDF Directory: {PDF_DIR}\")\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Download files from the shared Google Drive folder\n",
        "folder_id = \"1QXwB1AXaV8TlgqoN41PIdwn1VSDJI1zg\"\n",
        "print(f\"Downloading PDFs from Google Drive folder ID: {folder_id}\")\n",
        "\n",
        "# List and download files from the shared folder\n",
        "!gdown --folder --id {folder_id} -O {PDF_DIR}\n",
        "\n",
        "# List downloaded PDFs\n",
        "pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith('.pdf')]\n",
        "print(f\"\\nFound {len(pdf_files)} PDF files in {PDF_DIR}:\")\n",
        "for i, file in enumerate(pdf_files, 1):\n",
        "    print(f\"{i}. {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8ps_aBquEIu",
        "outputId": "f4fc947b-51b9-4445-86b4-0350c599b555"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF Directory: /content/MELIDA/data/raw/exams\n",
            "Output Directory: /content/MELIDA/data/questions\n",
            "Downloading PDFs from Google Drive folder ID: 1QXwB1AXaV8TlgqoN41PIdwn1VSDJI1zg\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Retrieving folder contents\n",
            "Processing file 12-eLFeor5K8RGn8jcIstT-ZvSBLG29SA Cuaderno_2024_MEDICINA_0_C.pdf\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12-eLFeor5K8RGn8jcIstT-ZvSBLG29SA\n",
            "To: /content/MELIDA/data/raw/exams/Cuaderno_2024_MEDICINA_0_C.pdf\n",
            "100% 797k/797k [00:00<00:00, 41.9MB/s]\n",
            "Download completed\n",
            "\n",
            "Found 1 PDF files in /content/MELIDA/data/raw/exams:\n",
            "1. Cuaderno_2024_MEDICINA_0_C.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "def normalize_spaces(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove double spaces and other whitespace issues from text\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Replace all whitespace sequences (including tabs, newlines) with a single space\n",
        "    normalized = re.sub(r'\\s+', ' ', text)\n",
        "    # Trim leading/trailing whitespace\n",
        "    normalized = normalized.strip()\n",
        "    return normalized\n",
        "\n",
        "def get_exam_metadata(filename: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Extracts exam type and year from filename\n",
        "    Example: 'Cuaderno_2024_MEDICINA_0_C.pdf' -> ('MIR', '2024')\n",
        "    Adjust patterns as needed for different exam types\n",
        "    \"\"\"\n",
        "    # Default values\n",
        "    exam_type = \"MIR\"\n",
        "    year = \"UNKNOWN\"\n",
        "\n",
        "    # Extract year\n",
        "    year_match = re.search(r'(\\d{4})', filename)\n",
        "    if year_match:\n",
        "        year = year_match.group(1)\n",
        "\n",
        "    # Extract exam type if present (customize based on your naming conventions)\n",
        "    if \"MEDICINA\" in filename.upper():\n",
        "        exam_type = \"MIR\"\n",
        "    elif \"ENFERMERIA\" in filename.upper():\n",
        "        exam_type = \"EIR\"\n",
        "    # Add more exam types as needed\n",
        "\n",
        "    return exam_type, year\n",
        "\n",
        "\n",
        "def format_question_id(exam_type: str, year: str, version: str = \"v01\",\n",
        "                       question_type: str = \"t01\", question_num: int = 0) -> str:\n",
        "    \"\"\"\n",
        "    Creates standardized question ID\n",
        "    Format: {exam_type}-{year}-{version}-{question_type}-Q{question_num:03d}\n",
        "    Example: MIR-2024-v01-t01-Q026\n",
        "    \"\"\"\n",
        "    return f\"{exam_type}-{year}-{version}-{question_type}-Q{question_num:03d}\"\n",
        "\n",
        "\n",
        "def create_formatted_question(exam_type: str, year: str, qnum: int,\n",
        "                             qtext: str, options: List[str],\n",
        "                             source_file: str, page_num: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Creates a question dict with the required format with improved cleaning\n",
        "    \"\"\"\n",
        "    # Clean the question text thoroughly\n",
        "    text = normalize_spaces(qtext)\n",
        "    text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text)  # Merge hyphen-split\n",
        "    text = re.sub(r'\\[(.*?)\\]', r'\\1', text)       # Remove bracket formatting\n",
        "\n",
        "    # Format options as required dictionary with A, B, C, D keys\n",
        "    option_dict = {}\n",
        "    option_keys = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "    for i, opt in enumerate(options):\n",
        "        if i < len(option_keys):\n",
        "            # Clean each option text\n",
        "            opt = normalize_spaces(opt)\n",
        "            opt = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', opt)  # Merge hyphen-split\n",
        "            option_dict[option_keys[i]] = opt\n",
        "\n",
        "    # Ensure all options exist even if empty\n",
        "    for key in option_keys:\n",
        "        if key not in option_dict:\n",
        "            option_dict[key] = \"\"\n",
        "\n",
        "    # Create the question object in the required format\n",
        "    return {\n",
        "        \"id\": format_question_id(exam_type, year, question_num=qnum),\n",
        "        \"question_text\": text,\n",
        "        \"options\": option_dict,\n",
        "        # Metadata fields (not in final output but useful for debugging)\n",
        "        \"_metadata\": {\n",
        "            \"source_file\": source_file,\n",
        "            \"page_number\": page_num,\n",
        "            \"original_number\": qnum\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def extract_lines_with_x0(page, bbox):\n",
        "    \"\"\"\n",
        "    Crops to bbox, extracts text lines as (text, x0).\n",
        "    Merges lines ending in '-'.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    hyphen_buffer = \"\"\n",
        "\n",
        "    cropped = page.crop(bbox)\n",
        "    raw_lines = cropped.extract_text_lines()\n",
        "\n",
        "    for ln in raw_lines:\n",
        "        text = ln[\"text\"].strip()\n",
        "        x0_val = ln[\"x0\"]\n",
        "\n",
        "        if hyphen_buffer:\n",
        "            text = hyphen_buffer + text\n",
        "            hyphen_buffer = \"\"\n",
        "\n",
        "        if re.search(r'-\\s*$', text):\n",
        "            hyphen_buffer = re.sub(r'-\\s*$', '', text)\n",
        "            continue\n",
        "\n",
        "        lines.append((text, x0_val))\n",
        "    return lines\n",
        "\n",
        "\n",
        "def split_markers(line_text):\n",
        "    \"\"\"\n",
        "    Splits on every '(\\\\d+)\\\\. ' pattern.\n",
        "    Returns [(num_str_or_None, snippet)].\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r'(\\d+)\\.\\s')\n",
        "    tokens = pattern.split(line_text)\n",
        "\n",
        "    results = []\n",
        "    leftover = tokens[0].strip()\n",
        "    if leftover:\n",
        "        results.append((None, leftover))\n",
        "\n",
        "    i = 1\n",
        "    while i < len(tokens):\n",
        "        num_str = tokens[i]\n",
        "        i += 1\n",
        "        snippet = tokens[i].strip() if i < len(tokens) else \"\"\n",
        "        i += 1\n",
        "        results.append((num_str, snippet))\n",
        "    return results\n",
        "\n",
        "\n",
        "def parse_lines(\n",
        "    lines, page_idx, exam_type, year, source_file,\n",
        "    questions,\n",
        "    current_qnum, current_qtext, current_opts,\n",
        "    expected_next_qnum=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Enhanced state machine logic to extract questions and options\n",
        "    With space normalization throughout\n",
        "    \"\"\"\n",
        "    MAX_QNUM = 210\n",
        "    parsing_warnings = []\n",
        "\n",
        "    # Initialize expected next question number if not provided\n",
        "    if expected_next_qnum is None and current_qnum is not None:\n",
        "        expected_next_qnum = current_qnum + 1\n",
        "    elif expected_next_qnum is None:\n",
        "        expected_next_qnum = 1\n",
        "\n",
        "    for (full_line, _x0) in lines:\n",
        "        if not full_line.strip():\n",
        "            continue\n",
        "\n",
        "        segments = split_markers(full_line)\n",
        "\n",
        "        for (num_str, snippet) in segments:\n",
        "            # Normalize spaces in the snippet\n",
        "            snippet = normalize_spaces(snippet)\n",
        "\n",
        "            if not snippet:  # Skip empty snippets\n",
        "                continue\n",
        "\n",
        "            if num_str is not None:\n",
        "                val = int(num_str)\n",
        "\n",
        "                # CASE A: Potential option if 1..4\n",
        "                if 1 <= val <= 4:\n",
        "                    # If we have a current question\n",
        "                    if current_qnum is not None:\n",
        "                        # Check if this is likely a valid next option\n",
        "                        exists_already = any(opt.startswith(f\"{val}. \") for opt in current_opts)\n",
        "                        is_next_option = (len(current_opts) == 0 and val == 1) or \\\n",
        "                                         (len(current_opts) > 0 and val <= len(current_opts) + 1)\n",
        "\n",
        "                        if len(current_opts) < 4 and not exists_already and is_next_option:\n",
        "                            # Before adding first option, ensure question ends properly\n",
        "                            if len(current_opts) == 0 and not (current_qtext.endswith(':') or current_qtext.endswith('?')):\n",
        "                                # Look for : or ? in the snippet\n",
        "                                colon_pos = snippet.find(':')\n",
        "                                qmark_pos = snippet.find('?')\n",
        "\n",
        "                                if colon_pos >= 0:\n",
        "                                    # Split at colon - move text before colon to question\n",
        "                                    current_qtext = normalize_spaces(current_qtext + \" \" + snippet[:colon_pos+1])\n",
        "                                    snippet = normalize_spaces(snippet[colon_pos+1:])\n",
        "                                elif qmark_pos >= 0:\n",
        "                                    # Split at question mark - move text before question mark to question\n",
        "                                    current_qtext = normalize_spaces(current_qtext + \" \" + snippet[:qmark_pos+1])\n",
        "                                    snippet = normalize_spaces(snippet[qmark_pos+1:])\n",
        "                                else:\n",
        "                                    # No ending found, add a colon\n",
        "                                    current_qtext = normalize_spaces(current_qtext)\n",
        "                                    if current_qtext and not (current_qtext.endswith(':') or current_qtext.endswith('?')):\n",
        "                                        current_qtext = current_qtext.rstrip() + \":\"\n",
        "                                    warning = f\"Question {current_qnum} has no proper ending before options, added colon\"\n",
        "                                    parsing_warnings.append(warning)\n",
        "                                    print(f\"WARNING: {warning} (page {page_idx+1})\")\n",
        "\n",
        "                            # Add the option with number prefix\n",
        "                            current_opts.append(f\"{val}. {snippet}\")\n",
        "                        # Check if this is likely a new question starting with option 1\n",
        "                        elif val == 1 and len(current_opts) > 0:\n",
        "                            # Finalize the current question\n",
        "\n",
        "                            # Process options for consistency\n",
        "                            normalized_opts = []\n",
        "                            collected_nums = set()\n",
        "\n",
        "                            for opt in current_opts:\n",
        "                                opt = normalize_spaces(opt)\n",
        "                                opt_match = re.match(r'(\\d+)\\.\\s+(.*)', opt)\n",
        "                                if opt_match:\n",
        "                                    opt_num = int(opt_match.group(1))\n",
        "                                    collected_nums.add(opt_num)\n",
        "                                    normalized_opts.append(f\"{opt_num}. {normalize_spaces(opt_match.group(2))}\")\n",
        "                                else:\n",
        "                                    # No number prefix, try to assign\n",
        "                                    for n in range(1, 5):\n",
        "                                        if n not in collected_nums:\n",
        "                                            collected_nums.add(n)\n",
        "                                            normalized_opts.append(f\"{n}. {normalize_spaces(opt)}\")\n",
        "                                            break\n",
        "\n",
        "                            # Fill in any missing options\n",
        "                            current_opts = normalized_opts\n",
        "                            for n in range(1, 5):\n",
        "                                if n not in collected_nums:\n",
        "                                    current_opts.append(f\"{n}. \")\n",
        "\n",
        "                            # Sort by option number\n",
        "                            current_opts.sort(key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else 0)\n",
        "\n",
        "                            # Check for missing options\n",
        "                            if len(current_opts) < 4:\n",
        "                                warning = f\"Incomplete options: Question {current_qnum} has only {len(current_opts)} options (expected 4), filled gaps\"\n",
        "                                parsing_warnings.append(warning)\n",
        "                                print(f\"WARNING: {warning} (page {page_idx+1})\")\n",
        "\n",
        "                            # Ensure question ends properly\n",
        "                            current_qtext = normalize_spaces(current_qtext)\n",
        "                            if not (current_qtext.endswith(':') or current_qtext.endswith('?')):\n",
        "                                current_qtext = current_qtext.rstrip() + \":\"\n",
        "                                warning = f\"Question {current_qnum} has no proper ending, added colon\"\n",
        "                                parsing_warnings.append(warning)\n",
        "                                print(f\"WARNING: {warning} (page {page_idx+1})\")\n",
        "\n",
        "                            # Extract final options without numbering\n",
        "                            final_opts = []\n",
        "                            for opt in current_opts[:4]:  # Ensure exactly 4 options\n",
        "                                opt_match = re.match(r'\\d+\\.\\s+(.*)', opt)\n",
        "                                if opt_match:\n",
        "                                    final_opts.append(normalize_spaces(opt_match.group(1)))\n",
        "                                else:\n",
        "                                    final_opts.append(normalize_spaces(opt))\n",
        "\n",
        "                            # Finalize the question\n",
        "                            questions.append(create_formatted_question(\n",
        "                                exam_type,\n",
        "                                year,\n",
        "                                current_qnum,\n",
        "                                current_qtext,\n",
        "                                final_opts,\n",
        "                                source_file,\n",
        "                                page_idx + 1\n",
        "                            ))\n",
        "\n",
        "                            # Start a new question - guess number based on expected sequence\n",
        "                            if expected_next_qnum > current_qnum + 1:\n",
        "                                # If we've seen a gap already, keep the expected number\n",
        "                                current_qnum = expected_next_qnum\n",
        "                            else:\n",
        "                                # Otherwise, assume it's the next in sequence\n",
        "                                current_qnum = current_qnum + 1\n",
        "\n",
        "                            current_qtext = normalize_spaces(snippet)\n",
        "                            current_opts = []\n",
        "                            expected_next_qnum = current_qnum + 1\n",
        "                        else:\n",
        "                            # This is likely not an option but part of the text\n",
        "                            if current_opts:\n",
        "                                current_opts[-1] += f\" {val}. {snippet}\"\n",
        "                            else:\n",
        "                                current_qtext = normalize_spaces(current_qtext + f\" {val}. {snippet}\")\n",
        "                    else:\n",
        "                        # No active question, but found option - treat as new question\n",
        "                        current_qnum = expected_next_qnum\n",
        "                        current_qtext = normalize_spaces(snippet)\n",
        "                        current_opts = []\n",
        "                        expected_next_qnum = current_qnum + 1\n",
        "\n",
        "                # CASE B: potential question number (5..210)\n",
        "                elif 5 <= val <= MAX_QNUM:\n",
        "                    # This is definitely a new question number\n",
        "\n",
        "                    # Finalize old question if any\n",
        "                    if current_qnum is not None:\n",
        "                        # Process options for consistency\n",
        "                        normalized_opts = []\n",
        "                        collected_nums = set()\n",
        "\n",
        "                        for opt in current_opts:\n",
        "                            opt = normalize_spaces(opt)\n",
        "                            opt_match = re.match(r'(\\d+)\\.\\s+(.*)', opt)\n",
        "                            if opt_match:\n",
        "                                opt_num = int(opt_match.group(1))\n",
        "                                collected_nums.add(opt_num)\n",
        "                                normalized_opts.append(f\"{opt_num}. {normalize_spaces(opt_match.group(2))}\")\n",
        "                            else:\n",
        "                                # No number prefix, try to assign\n",
        "                                for n in range(1, 5):\n",
        "                                    if n not in collected_nums:\n",
        "                                        collected_nums.add(n)\n",
        "                                        normalized_opts.append(f\"{n}. {normalize_spaces(opt)}\")\n",
        "                                        break\n",
        "\n",
        "                        # Fill in any missing options\n",
        "                        current_opts = normalized_opts\n",
        "                        for n in range(1, 5):\n",
        "                            if n not in collected_nums:\n",
        "                                current_opts.append(f\"{n}. \")\n",
        "\n",
        "                        # Sort by option number\n",
        "                        current_opts.sort(key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else 0)\n",
        "\n",
        "                        # Check for missing options\n",
        "                        if len(current_opts) < 4:\n",
        "                            warning = f\"Incomplete options: Question {current_qnum} has only {len(current_opts)} options (expected 4), filled gaps\"\n",
        "                            parsing_warnings.append(warning)\n",
        "                            print(f\"WARNING: {warning} (page {page_idx+1})\")\n",
        "\n",
        "                        # Ensure question ends properly\n",
        "                        current_qtext = normalize_spaces(current_qtext)\n",
        "                        if not (current_qtext.endswith(':') or current_qtext.endswith('?')):\n",
        "                            current_qtext = current_qtext.rstrip() + \":\"\n",
        "                            warning = f\"Question {current_qnum} has no proper ending, added colon\"\n",
        "                            parsing_warnings.append(warning)\n",
        "                            print(f\"WARNING: {warning} (page {page_idx+1})\")\n",
        "\n",
        "                        # Extract final options without numbering\n",
        "                        final_opts = []\n",
        "                        for opt in current_opts[:4]:  # Ensure exactly 4 options\n",
        "                            opt_match = re.match(r'\\d+\\.\\s+(.*)', opt)\n",
        "                            if opt_match:\n",
        "                                final_opts.append(normalize_spaces(opt_match.group(1)))\n",
        "                            else:\n",
        "                                final_opts.append(normalize_spaces(opt))\n",
        "\n",
        "                        # Finalize the question\n",
        "                        questions.append(create_formatted_question(\n",
        "                            exam_type,\n",
        "                            year,\n",
        "                            current_qnum,\n",
        "                            current_qtext,\n",
        "                            final_opts,\n",
        "                            source_file,\n",
        "                            page_idx + 1\n",
        "                        ))\n",
        "\n",
        "                    # Check for expected sequence\n",
        "                    if val != expected_next_qnum:\n",
        "                        # Log the gap or out-of-sequence number\n",
        "                        if val > expected_next_qnum:\n",
        "                            warning = f\"Question number gap: Expected {expected_next_qnum}, found {val} (gap of {val - expected_next_qnum})\"\n",
        "                        else:\n",
        "                            warning = f\"Out of order question: Expected {expected_next_qnum}, found {val} (backwards by {expected_next_qnum - val})\"\n",
        "\n",
        "                        parsing_warnings.append(warning)\n",
        "                        print(f\"WARNING: {warning} (page {page_idx+1})\")\n",
        "\n",
        "                    # Start new question\n",
        "                    current_qnum = val\n",
        "                    current_qtext = normalize_spaces(snippet)\n",
        "                    current_opts = []\n",
        "                    expected_next_qnum = val + 1  # Update expected next number\n",
        "                else:\n",
        "                    # Number out of range => just leftover text\n",
        "                    if current_opts:\n",
        "                        current_opts[-1] = normalize_spaces(current_opts[-1] + f\" {val}. {snippet}\")\n",
        "                    elif current_qnum is not None:\n",
        "                        current_qtext = normalize_spaces(current_qtext + f\" {val}. {snippet}\")\n",
        "\n",
        "            else:\n",
        "                # leftover text without a number prefix\n",
        "                if current_qnum is not None:\n",
        "                    # If we don't have a properly ended question yet and there are no options\n",
        "                    if not (current_qtext.endswith(':') or current_qtext.endswith('?')) and not current_opts:\n",
        "                        # Look for \":\" or \"?\" in the snippet\n",
        "                        colon_pos = snippet.find(':')\n",
        "                        qmark_pos = snippet.find('?')\n",
        "\n",
        "                        if colon_pos >= 0:\n",
        "                            # Split the snippet at the colon\n",
        "                            current_qtext = normalize_spaces(current_qtext + \" \" + snippet[:colon_pos+1])\n",
        "                            remaining = normalize_spaces(snippet[colon_pos+1:])\n",
        "                            if remaining:\n",
        "                                # Start first option with remaining text\n",
        "                                current_opts.append(f\"1. {remaining}\")\n",
        "                            continue\n",
        "                        elif qmark_pos >= 0:\n",
        "                            # Split the snippet at the question mark\n",
        "                            current_qtext = normalize_spaces(current_qtext + \" \" + snippet[:qmark_pos+1])\n",
        "                            remaining = normalize_spaces(snippet[qmark_pos+1:])\n",
        "                            if remaining:\n",
        "                                # Start first option with remaining text\n",
        "                                current_opts.append(f\"1. {remaining}\")\n",
        "                            continue\n",
        "\n",
        "                    # Normal case - append to appropriate place\n",
        "                    if current_opts:\n",
        "                        current_opts[-1] = normalize_spaces(current_opts[-1] + \" \" + snippet)\n",
        "                    else:\n",
        "                        current_qtext = normalize_spaces(current_qtext + \" \" + snippet)\n",
        "                # else we have no active question => ignore\n",
        "\n",
        "    return current_qnum, current_qtext, current_opts, expected_next_qnum, parsing_warnings\n",
        "\n",
        "\n",
        "def process_pdf(pdf_path: str, start_page: int = 2) -> tuple:\n",
        "    \"\"\"\n",
        "    Process PDF and extract questions with options\n",
        "    Returns both questions and warnings about extraction issues\n",
        "    \"\"\"\n",
        "    questions = []\n",
        "    warnings = []\n",
        "    current_qnum = None\n",
        "    current_qtext = \"\"\n",
        "    current_opts = []\n",
        "    expected_next_qnum = 1  # Start expecting question #1\n",
        "    MAX_QNUM = 210  # Define MAX_QNUM within this function\n",
        "\n",
        "    # Extract metadata from filename\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    exam_type, year = get_exam_metadata(filename)\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            source_file = os.path.basename(pdf_path)\n",
        "            num_pages = len(pdf.pages)\n",
        "\n",
        "            for i in range(start_page, num_pages):\n",
        "                page = pdf.pages[i]\n",
        "                w, h = page.width, page.height\n",
        "                mid_x = w / 2.0\n",
        "\n",
        "                # Split page into left and right columns\n",
        "                left_bbox = (0, 0, mid_x, h)\n",
        "                right_bbox = (mid_x, 0, w, h)\n",
        "\n",
        "                left_lines = extract_lines_with_x0(page, left_bbox)\n",
        "                right_lines = extract_lines_with_x0(page, right_bbox)\n",
        "\n",
        "                # Parse left column\n",
        "                current_qnum, current_qtext, current_opts, expected_next_qnum, page_warnings = parse_lines(\n",
        "                    left_lines, i, exam_type, year, source_file,\n",
        "                    questions,\n",
        "                    current_qnum, current_qtext, current_opts, expected_next_qnum\n",
        "                )\n",
        "                warnings.extend(page_warnings)\n",
        "\n",
        "                # Parse right column\n",
        "                current_qnum, current_qtext, current_opts, expected_next_qnum, page_warnings = parse_lines(\n",
        "                    right_lines, i, exam_type, year, source_file,\n",
        "                    questions,\n",
        "                    current_qnum, current_qtext, current_opts, expected_next_qnum\n",
        "                )\n",
        "                warnings.extend(page_warnings)\n",
        "\n",
        "        # Add the last question if not already added\n",
        "        if current_qnum is not None:\n",
        "            # Normalize the question text\n",
        "            current_qtext = normalize_spaces(current_qtext)\n",
        "\n",
        "            # Check for incomplete options\n",
        "            if len(current_opts) < 4:\n",
        "                warning = f\"Incomplete options: Final question {current_qnum} has only {len(current_opts)} options (expected 4)\"\n",
        "                warnings.append(warning)\n",
        "                print(f\"WARNING: {warning}\")\n",
        "\n",
        "                # Fill in missing options\n",
        "                normalized_opts = []\n",
        "                collected_nums = set()\n",
        "\n",
        "                for opt in current_opts:\n",
        "                    # Normalize the option text\n",
        "                    opt = normalize_spaces(opt)\n",
        "                    opt_match = re.match(r'(\\d+)\\.\\s+(.*)', opt)\n",
        "                    if opt_match:\n",
        "                        opt_num = int(opt_match.group(1))\n",
        "                        collected_nums.add(opt_num)\n",
        "                        normalized_opts.append(f\"{opt_num}. {normalize_spaces(opt_match.group(2))}\")\n",
        "                    else:\n",
        "                        # No number prefix, try to assign\n",
        "                        for n in range(1, 5):\n",
        "                            if n not in collected_nums:\n",
        "                                collected_nums.add(n)\n",
        "                                normalized_opts.append(f\"{n}. {opt}\")\n",
        "                                break\n",
        "\n",
        "                # Fill in any missing options\n",
        "                current_opts = normalized_opts\n",
        "                for n in range(1, 5):\n",
        "                    if n not in collected_nums:\n",
        "                        current_opts.append(f\"{n}. \")\n",
        "\n",
        "                # Sort by option number\n",
        "                current_opts.sort(key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else 0)\n",
        "\n",
        "            # Ensure question ends properly before finalizing\n",
        "            if not (current_qtext.endswith(':') or current_qtext.endswith('?')):\n",
        "                current_qtext = current_qtext.rstrip() + \":\"\n",
        "                warning = f\"Added missing ending to final question {current_qnum}\"\n",
        "                warnings.append(warning)\n",
        "\n",
        "            # Extract final options without numbering and normalize spaces\n",
        "            final_opts = []\n",
        "            for opt in current_opts[:4]:  # Ensure exactly 4 options\n",
        "                opt_match = re.match(r'\\d+\\.\\s+(.*)', opt)\n",
        "                if opt_match:\n",
        "                    final_opts.append(normalize_spaces(opt_match.group(1)))\n",
        "                else:\n",
        "                    final_opts.append(normalize_spaces(opt))\n",
        "\n",
        "            # Now finalize the question with processed options\n",
        "            questions.append(create_formatted_question(\n",
        "                exam_type,\n",
        "                year,\n",
        "                current_qnum,\n",
        "                current_qtext,\n",
        "                final_opts,\n",
        "                source_file,\n",
        "                i + 1\n",
        "            ))\n",
        "\n",
        "        # Post-processing to deduplicate questions\n",
        "        if questions:\n",
        "            # Sort questions by number\n",
        "            questions.sort(key=lambda q: q.get('_metadata', {}).get('original_number', 0))\n",
        "\n",
        "            # Deduplicate based on question number\n",
        "            deduped_questions = []\n",
        "            seen_numbers = set()\n",
        "\n",
        "            for q in questions:\n",
        "                qnum = q.get('_metadata', {}).get('original_number', 0)\n",
        "\n",
        "                if qnum not in seen_numbers:\n",
        "                    seen_numbers.add(qnum)\n",
        "                    deduped_questions.append(q)\n",
        "                else:\n",
        "                    # Duplicate found - check which one seems more complete\n",
        "                    existing_idx = next(i for i, existing_q in enumerate(deduped_questions)\n",
        "                                       if existing_q.get('_metadata', {}).get('original_number', 0) == qnum)\n",
        "\n",
        "                    # Check which has more complete options\n",
        "                    existing_q = deduped_questions[existing_idx]\n",
        "\n",
        "                    existing_options = existing_q['options']\n",
        "                    new_options = q['options']\n",
        "\n",
        "                    existing_emptiness = sum(1 for opt in existing_options.values() if not opt.strip())\n",
        "                    new_emptiness = sum(1 for opt in new_options.values() if not opt.strip())\n",
        "\n",
        "                    if new_emptiness < existing_emptiness:\n",
        "                        # Replace with more complete version\n",
        "                        deduped_questions[existing_idx] = q\n",
        "                        warning = f\"Replaced duplicate question {qnum} with more complete version\"\n",
        "                    else:\n",
        "                        warning = f\"Kept first occurrence of duplicate question {qnum}\"\n",
        "\n",
        "                    warnings.append(warning)\n",
        "\n",
        "            # Update the questions list\n",
        "            questions = deduped_questions\n",
        "            print(f\"After deduplication: {len(questions)} questions\")\n",
        "\n",
        "            # Final verification of sequential numbering\n",
        "            question_numbers = [q.get('_metadata', {}).get('original_number', 0) for q in questions]\n",
        "            question_numbers.sort()\n",
        "\n",
        "            if question_numbers:\n",
        "                print(f\"Extracted question numbers: {min(question_numbers)} to {max(question_numbers)}\")\n",
        "\n",
        "                # Check for gaps\n",
        "                expected_numbers = set(range(min(question_numbers), max(question_numbers) + 1))\n",
        "                found_numbers = set(question_numbers)\n",
        "                missing_numbers = expected_numbers - found_numbers\n",
        "\n",
        "                if missing_numbers:\n",
        "                    warning = f\"Missing {len(missing_numbers)} questions in the sequence\"\n",
        "                    warnings.append(warning)\n",
        "                    print(f\"WARNING: {warning}: {sorted(missing_numbers)[:10]}...\")\n",
        "\n",
        "        # Final summary\n",
        "        if warnings:\n",
        "            print(f\"\\nExtraction completed with {len(warnings)} warnings\")\n",
        "            print(f\"Successfully extracted {len(questions)} questions\")\n",
        "        else:\n",
        "            print(f\"\\nExtraction completed successfully with no warnings\")\n",
        "            print(f\"Successfully extracted {len(questions)} questions\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error processing {os.path.basename(pdf_path)}: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        warnings.append(error_msg)\n",
        "        # Make sure to initialize questions if an exception occurs\n",
        "        questions = []\n",
        "\n",
        "    # Return both the questions and the warnings\n",
        "    return questions, warnings\n",
        "\n",
        "\n",
        "def clean_output_for_export(questions: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Prepare questions for export by removing metadata fields\n",
        "    \"\"\"\n",
        "    cleaned = []\n",
        "    for q in questions:\n",
        "        # Create a copy without the metadata\n",
        "        cleaned_q = {k: v for k, v in q.items() if not k.startswith('_')}\n",
        "        cleaned.append(cleaned_q)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def export_questions(questions: List[Dict], output_dir: str,\n",
        "                    exam_type: str = \"MIR\", year: str = \"2024\",\n",
        "                    version: str = \"v01\", question_type: str = \"t01\") -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Export questions to CSV and JSON files with standardized naming\n",
        "    \"\"\"\n",
        "    if not questions:\n",
        "        return None, None\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Format the filename according to requirements\n",
        "    filename_base = f\"{exam_type}-{year}-{version}-{question_type}\"\n",
        "\n",
        "    # Clean the questions for export (remove metadata)\n",
        "    export_questions = clean_output_for_export(questions)\n",
        "\n",
        "    # Final space normalization pass before export\n",
        "    for q in export_questions:\n",
        "        q['question_text'] = normalize_spaces(q['question_text'])\n",
        "        for key, value in q['options'].items():\n",
        "            q['options'][key] = normalize_spaces(value)\n",
        "\n",
        "    # Export to JSON\n",
        "    json_path = os.path.join(output_dir, f\"{filename_base}.json\")\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(export_questions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Export to CSV (flattening the options dictionary)\n",
        "    df = pd.DataFrame(questions)\n",
        "\n",
        "    # Extract options from the nested dictionary for CSV format\n",
        "    if not df.empty and 'options' in df.columns:\n",
        "        for key in ['A', 'B', 'C', 'D']:\n",
        "            df[f'option_{key}'] = df['options'].apply(lambda x: x.get(key, ''))\n",
        "        df = df.drop(columns=['options'])\n",
        "\n",
        "    # Remove metadata columns\n",
        "    if '_metadata' in df.columns:\n",
        "        metadata_df = pd.json_normalize(df['_metadata'])\n",
        "        df = pd.concat([df.drop(columns=['_metadata']), metadata_df], axis=1)\n",
        "\n",
        "    # Export to CSV\n",
        "    csv_path = os.path.join(output_dir, f\"{filename_base}.csv\")\n",
        "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    return json_path, csv_path\n",
        "\n",
        "\n",
        "def process_directory(pdf_dir: str, output_dir: str, start_page: int = 2) -> None:\n",
        "    \"\"\"\n",
        "    Process all PDFs in a directory and export questions\n",
        "    Includes detailed warnings about extraction issues\n",
        "    \"\"\"\n",
        "    all_questions = []\n",
        "    all_warnings = []\n",
        "\n",
        "    # Process all PDFs\n",
        "    for filename in sorted(os.listdir(pdf_dir)):\n",
        "        if not filename.lower().endswith(\".pdf\"):\n",
        "            continue\n",
        "\n",
        "        pdf_path = os.path.join(pdf_dir, filename)\n",
        "        exam_type, year = get_exam_metadata(filename)\n",
        "\n",
        "        try:\n",
        "            questions, warnings = process_pdf(pdf_path, start_page=start_page)\n",
        "            all_questions.extend(questions)\n",
        "\n",
        "            # Record detailed warnings with file information\n",
        "            file_warnings = [{\"file\": filename, \"warning\": w} for w in warnings]\n",
        "            all_warnings.extend(file_warnings)\n",
        "\n",
        "            print(f\"Extracted {len(questions)} questions from {filename} ({exam_type} {year}) with {len(warnings)} warnings\")\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error processing {filename}: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            all_warnings.append({\"file\": filename, \"warning\": error_msg})\n",
        "\n",
        "    # Group questions by exam type and year\n",
        "    grouped_questions = {}\n",
        "    for q in all_questions:\n",
        "        # Extract exam type and year from question ID\n",
        "        id_parts = q['id'].split('-')\n",
        "        if len(id_parts) >= 2:\n",
        "            exam_type, year = id_parts[0], id_parts[1]\n",
        "            key = f\"{exam_type}-{year}\"\n",
        "\n",
        "            if key not in grouped_questions:\n",
        "                grouped_questions[key] = []\n",
        "\n",
        "            grouped_questions[key].append(q)\n",
        "\n",
        "    # Export each group separately\n",
        "    for key, questions in grouped_questions.items():\n",
        "        exam_type, year = key.split('-')\n",
        "\n",
        "        # Export with standard naming\n",
        "        json_path, csv_path = export_questions(\n",
        "            questions,\n",
        "            output_dir,\n",
        "            exam_type=exam_type,\n",
        "            year=year\n",
        "        )\n",
        "\n",
        "        if json_path:\n",
        "            print(f\"Exported {len(questions)} {exam_type} {year} questions to:\")\n",
        "            print(f\"  - JSON: {os.path.basename(json_path)}\")\n",
        "            print(f\"  - CSV: {os.path.basename(csv_path)}\")\n",
        "\n",
        "    # Export warnings to CSV\n",
        "    if all_warnings:\n",
        "        warnings_df = pd.DataFrame(all_warnings)\n",
        "        warnings_path = os.path.join(output_dir, \"extraction_warnings.csv\")\n",
        "        warnings_df.to_csv(warnings_path, index=False)\n",
        "        print(f\"\\nExported {len(all_warnings)} warnings to {warnings_path}\")"
      ],
      "metadata": {
        "id": "nqL0gp4SuK9c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option to upload PDFs directly to Colab (if not already in Drive)\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def upload_pdfs_to_drive():\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename, content in uploaded.items():\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            dest_path = os.path.join(PDF_DIR, filename)\n",
        "            with open(dest_path, 'wb') as f:\n",
        "                f.write(content)\n",
        "            print(f\"Saved {filename} to {dest_path}\")\n",
        "\n",
        "# Uncomment the line below to upload PDF files\n",
        "# upload_pdfs_to_drive()"
      ],
      "metadata": {
        "id": "3NzTN-k-uUD5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List PDF files available for processing\n",
        "pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith('.pdf')]\n",
        "print(f\"Found {len(pdf_files)} PDF files in {PDF_DIR}:\")\n",
        "for i, file in enumerate(pdf_files, 1):\n",
        "    print(f\"{i}. {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViGfv8IiucR9",
        "outputId": "39dac8a4-87d3-4c49-8810-8a60db808c26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 PDF files in /content/MELIDA/data/raw/exams:\n",
            "1. Cuaderno_2024_MEDICINA_0_C.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all PDFs in the directory\n",
        "process_directory(PDF_DIR, OUTPUT_DIR, start_page=2)"
      ],
      "metadata": {
        "id": "BZTBgVROugXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3415b61-1eee-41d7-e9bf-9d53d1e93260"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Question 3 has no proper ending, added colon (page 3)\n",
            "WARNING: Question number gap: Expected 4, found 5 (gap of 1) (page 3)\n",
            "WARNING: Question 16 has no proper ending, added colon (page 5)\n",
            "WARNING: Out of order question: Expected 17, found 16 (backwards by 1) (page 5)\n",
            "WARNING: Question number gap: Expected 26, found 32 (gap of 6) (page 7)\n",
            "WARNING: Out of order question: Expected 33, found 26 (backwards by 7) (page 7)\n",
            "WARNING: Question 45 has no proper ending, added colon (page 10)\n",
            "WARNING: Question number gap: Expected 46, found 47 (gap of 1) (page 10)\n",
            "WARNING: Out of order question: Expected 48, found 46 (backwards by 2) (page 10)\n",
            "WARNING: Question 90 has no proper ending, added colon (page 16)\n",
            "WARNING: Out of order question: Expected 91, found 5 (backwards by 86) (page 16)\n",
            "WARNING: Question number gap: Expected 6, found 91 (gap of 85) (page 17)\n",
            "WARNING: Question 113 has no proper ending before options, added colon (page 19)\n",
            "WARNING: Question 168 has no proper ending, added colon (page 28)\n",
            "WARNING: Out of order question: Expected 169, found 168 (backwards by 1) (page 28)\n",
            "WARNING: Question 169 has no proper ending, added colon (page 29)\n",
            "WARNING: Out of order question: Expected 170, found 8 (backwards by 162) (page 29)\n",
            "WARNING: Question 8 has no proper ending, added colon (page 29)\n",
            "WARNING: Question number gap: Expected 9, found 160 (gap of 151) (page 29)\n",
            "WARNING: Question number gap: Expected 161, found 170 (gap of 9) (page 29)\n",
            "WARNING: Out of order question: Expected 172, found 171 (backwards by 1) (page 29)\n",
            "WARNING: Out of order question: Expected 174, found 5 (backwards by 169) (page 29)\n",
            "WARNING: Question number gap: Expected 6, found 174 (gap of 168) (page 30)\n",
            "WARNING: Out of order question: Expected 195, found 194 (backwards by 1) (page 33)\n",
            "WARNING: Question 199 has no proper ending, added colon (page 34)\n",
            "WARNING: Out of order question: Expected 200, found 14 (backwards by 186) (page 34)\n",
            "WARNING: Question number gap: Expected 15, found 200 (gap of 185) (page 34)\n",
            "After deduplication: 209 questions\n",
            "Extracted question numbers: 1 to 210\n",
            "WARNING: Missing 1 questions in the sequence: [4]...\n",
            "\n",
            "Extraction completed with 39 warnings\n",
            "Successfully extracted 209 questions\n",
            "Extracted 209 questions from Cuaderno_2024_MEDICINA_0_C.pdf (MIR 2024) with 39 warnings\n",
            "Exported 209 MIR 2024 questions to:\n",
            "  - JSON: MIR-2024-v01-t01.json\n",
            "  - CSV: MIR-2024-v01-t01.csv\n",
            "\n",
            "Exported 39 warnings to /content/MELIDA/data/questions/extraction_warnings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION CELL: Check for questions without proper endings before export\n",
        "def verify_questions_in_json_files(output_dir):\n",
        "    all_issues = []\n",
        "    total_questions = 0\n",
        "\n",
        "    # Process each JSON file in the output directory\n",
        "    for json_file in [f for f in os.listdir(output_dir) if f.lower().endswith('.json')]:\n",
        "        file_path = os.path.join(output_dir, json_file)\n",
        "        file_issues = []\n",
        "\n",
        "        try:\n",
        "            # Load the questions from the JSON file\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                questions = json.load(f)\n",
        "\n",
        "            total_questions += len(questions)\n",
        "\n",
        "            # Check each question\n",
        "            for i, q in enumerate(questions):\n",
        "                question_text = q.get('question_text', '').strip()\n",
        "\n",
        "                if not (question_text.endswith(':') or question_text.endswith('?')):\n",
        "                    file_issues.append({\n",
        "                        'index': i,\n",
        "                        'id': q.get('id', 'unknown'),\n",
        "                        'text': question_text,\n",
        "                        'file': json_file,\n",
        "                        'source_file': q.get('_metadata', {}).get('source_file', 'unknown') if '_metadata' in q else 'unknown',\n",
        "                        'page_number': q.get('_metadata', {}).get('page_number', 'unknown') if '_metadata' in q else 'unknown'\n",
        "                    })\n",
        "\n",
        "            all_issues.extend(file_issues)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {json_file}: {str(e)}\")\n",
        "\n",
        "    # Report results\n",
        "    print(f\"Total questions checked: {total_questions}\")\n",
        "\n",
        "    if all_issues:\n",
        "        print(f\"WARNING: Found {len(all_issues)} questions without proper endings (: or ?)\")\n",
        "        print(\"First 5 issues:\")\n",
        "        for i, issue in enumerate(all_issues[:5]):\n",
        "            print(f\"{i+1}. ID: {issue['id']}\")\n",
        "            print(f\"   Text: {issue['text']}\")\n",
        "            print(f\"   File: {issue['file']}\")\n",
        "            print(f\"   Source: {issue['source_file']}, Page: {issue['page_number']}\")\n",
        "\n",
        "        # Export issues to CSV\n",
        "        issues_df = pd.DataFrame(all_issues)\n",
        "        issues_csv = os.path.join(output_dir, \"question_ending_issues.csv\")\n",
        "        issues_df.to_csv(issues_csv, index=False)\n",
        "        print(f\"Exported all {len(all_issues)} issues to {issues_csv}\")\n",
        "    else:\n",
        "        print(\"VERIFICATION PASSED: All questions end with ':' or '?'\")\n",
        "\n",
        "    return all_issues\n",
        "\n",
        "# Verify questions after processing by reading the output JSON files\n",
        "print(\"\\nVerifying question formatting...\")\n",
        "question_issues = verify_questions_in_json_files(OUTPUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5vvPEEq5JIz",
        "outputId": "979b49a2-6ca4-4bdf-df83-618ec935c8e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verifying question formatting...\n",
            "Total questions checked: 209\n",
            "VERIFICATION PASSED: All questions end with ':' or '?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List processed output files\n",
        "json_files = [f for f in os.listdir(OUTPUT_DIR) if f.lower().endswith('.json')]\n",
        "print(f\"Found {len(json_files)} processed JSON files in {OUTPUT_DIR}:\")\n",
        "for i, file in enumerate(json_files, 1):\n",
        "    print(f\"{i}. {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeSNroYhuiTH",
        "outputId": "a3340781-3ad9-4d99-c1b2-f4739015bd2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 processed JSON files in /content/MELIDA/data/questions:\n",
            "1. MIR-2024-v01-t01.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a sample from the first JSON file\n",
        "if json_files:\n",
        "    sample_file = os.path.join(OUTPUT_DIR, json_files[0])\n",
        "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"Sample file: {json_files[0]}\")\n",
        "    print(f\"Total questions: {len(data)}\")\n",
        "    print(\"\\nSample questions (first 2):\")\n",
        "\n",
        "    for i, q in enumerate(data[:2], 1):\n",
        "        print(f\"\\nQuestion {i}:\")\n",
        "        print(f\"ID: {q['id']}\")\n",
        "        print(f\"Text: {q['question_text']}\")\n",
        "        print(\"Options:\")\n",
        "        for key, value in q['options'].items():\n",
        "            print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "id": "FjcOt3R7ukJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d439de0a-b91b-498c-c17d-0b5cdbecbfc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample file: MIR-2024-v01-t01.json\n",
            "Total questions: 209\n",
            "\n",
            "Sample questions (first 2):\n",
            "\n",
            "Question 1:\n",
            "ID: MIR-2024-v01-t01-Q001\n",
            "Text: Pregunta asociada a la imagen 1. Mujer de 42 aos que acude a la consulta de gentica por un diagnstico reciente de cncer de endometrio. En base a los antecedentes familiares que constan en la imagen, cul de los siguientes sndromes es ms probable que presente?:\n",
            "Options:\n",
            "  A: Poliposis adenomatosa familiar.\n",
            "  B: Sndrome de Lynch.\n",
            "  C: Sndrome de cncer de mama y ovario hereditario.\n",
            "  D: Sndrome de Cowden. 2. Pregunta asociada a la imagen 2. Paciente de 65 aos que acude a urgencias por disminucin brusca de agudeza visual en ojo derecho. La retinografa de dicho ojo se muestra en la imagen. Uno de los siguientes tratamientos est indicado para una enfermedad que es un factor de riesgo para esta situacin. Indique cul:\n",
            "\n",
            "Question 2:\n",
            "ID: MIR-2024-v01-t01-Q002\n",
            "Text: Latanoprost y timolol. 2. Flecainida. 3. Hidroxicloroquina. 4. Complejos vitamnicos y antioxidantes. 3. Pregunta asociada a la imagen 3. Hombre de 70 aos exfumador de un paquete al da y bebedor de 2 copas de vino al da consulta por una molestia farngea de 1 mes de evolucin. Se observa exudado amigdalar por lo que se inicia tratamiento antibitico. El resultado del test de estreptococo es negativo. La clnica persiste y se aade odinofagia con otalgia refleja y leve cambio en el timbre de la voz. Nota un bulto duro de unos 2 cm a nivel II cervical ipsilateral no doloroso. La orofaringoscopia se refleja en la imagen asociada (imagen aumentada en el recuadro resaltado en rojo). Cul es la sospecha diagnstica y su manejo?:\n",
            "Options:\n",
            "  A: Resistencia bacteriana a antibioterapia recibida, precisa cultivo de exudado farngeo con antibiograma para indicacin adecuada.\n",
            "  B: Sospecha de absceso periamigdalino izquierdo, ser remitido a urgencias de Otorrinolaringologa para drenaje quirrgico.\n",
            "  C: Sospecha de neoplasia maligna de amgdala palatina, ser remitido preferente-urgente a Otorrinolaringologa.\n",
            "  D: Sospecha de angina de Plaut-Vincent, remitido a Otorrinolaringologa para realizacin de biopsia de ganglio para estudio de linfoma. Pgina: 4. Pregunta asociada a la imagen 4. Nia de 7 aos que acude por otalgia y otorrea purulenta derecha desde hace 3 semanas, tratada con antibiticos y antinflamatorios. Tras la resolucin de las manifestaciones clnicas, vuelve a aparecer la otorrea por lo que se pauta de nuevo tratamiento antibitico. A los 5 das se objetiva una tumefaccin retroauricular. En la otoscopia se pone de manifiesto una leve retraccin de la membrana timpnica, escasa secrecin blanquecina en el conducto auditivo externo y una tumefaccin retroauricular blanda que ocasiona un mnimo desplazamiento anterior del pabelln auricular. Indique el diagnstico que sugiere la imagen de resonancia magntica mostrada:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for questions that don't end with \":\" or \"?\"\n",
        "def check_question_endings(json_files_dir):\n",
        "    issues_found = []\n",
        "    total_questions = 0\n",
        "\n",
        "    # Process each JSON file\n",
        "    for json_file in [f for f in os.listdir(json_files_dir) if f.lower().endswith('.json')]:\n",
        "        file_path = os.path.join(json_files_dir, json_file)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                questions = json.load(f)\n",
        "\n",
        "            # Check each question in the file\n",
        "            for q in questions:\n",
        "                total_questions += 1\n",
        "                question_text = q.get('question_text', '').strip()\n",
        "\n",
        "                if not (question_text.endswith(':') or question_text.endswith('?')):\n",
        "                    issues_found.append({\n",
        "                        'id': q.get('id', 'unknown'),\n",
        "                        'file': json_file,\n",
        "                        'text': question_text,\n",
        "                        'last_char': question_text[-1] if question_text else 'empty'\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {json_file}: {str(e)}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Total questions analyzed: {total_questions}\")\n",
        "\n",
        "    # Calculate percentage with proper error handling\n",
        "    percentage = (len(issues_found)/total_questions)*100 if total_questions > 0 else 0\n",
        "    print(f\"Questions without proper ending (: or ?): {len(issues_found)} ({percentage:.2f}%)\")\n",
        "\n",
        "    # Print details of problematic questions\n",
        "    if issues_found:\n",
        "        print(\"\\nQuestions with improper endings:\")\n",
        "        for i, issue in enumerate(issues_found[:10], 1):  # Show first 10 issues\n",
        "            print(f\"{i}. ID: {issue['id']} (from {issue['file']})\")\n",
        "            print(f\"   Text: {issue['text'][:100]}...\")\n",
        "            print(f\"   Last character: '{issue['last_char']}'\")\n",
        "\n",
        "        if len(issues_found) > 10:\n",
        "            print(f\"\\n... and {len(issues_found) - 10} more issues.\")\n",
        "\n",
        "    return issues_found\n",
        "\n",
        "# Run the check on the output directory\n",
        "print(\"Checking question formatting...\")\n",
        "issues = check_question_endings(OUTPUT_DIR)\n",
        "\n",
        "# Optional: Export issues to CSV for further analysis\n",
        "if issues and len(issues) > 0:\n",
        "    issues_df = pd.DataFrame(issues)\n",
        "    issues_csv = os.path.join(OUTPUT_DIR, \"question_format_issues.csv\")\n",
        "    issues_df.to_csv(issues_csv, index=False)\n",
        "    print(f\"\\nExported {len(issues)} issues to {issues_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EumK2Zb30u0x",
        "outputId": "11ef80e2-99d6-40e0-99d3-60bbe0f0d300"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking question formatting...\n",
            "Total questions analyzed: 209\n",
            "Questions without proper ending (: or ?): 0 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a specific file\n",
        "def download_file(file_path):\n",
        "    try:\n",
        "        files.download(file_path)\n",
        "        print(f\"Started download of {os.path.basename(file_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {str(e)}\")\n",
        "\n",
        "# Example usage - uncomment to download a specific file\n",
        "# if json_files:\n",
        "#     download_file(os.path.join(OUTPUT_DIR, json_files[0]))\n",
        "\n",
        "# Create a zip file with all processed files for easier download\n",
        "import zipfile\n",
        "\n",
        "def create_and_download_zip():\n",
        "    zip_path = os.path.join(BASE_PATH, \"processed_questions.zip\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        for file in os.listdir(OUTPUT_DIR):\n",
        "            file_path = os.path.join(OUTPUT_DIR, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                zipf.write(file_path, arcname=file)\n",
        "\n",
        "    download_file(zip_path)\n",
        "    print(f\"All processed files zipped to {zip_path}\")\n",
        "\n",
        "# Uncomment to create and download a zip of all processed files\n",
        "# create_and_download_zip()"
      ],
      "metadata": {
        "id": "dGOTlaDfu7nT"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}